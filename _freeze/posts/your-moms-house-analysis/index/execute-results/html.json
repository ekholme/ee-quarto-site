{
  "hash": "0b02e04f92d98173dffee3220afacc1c",
  "result": {
    "markdown": "---\ntitle: \"Your Mom's House Analysis\"\ndescription: |\n  An exploratory analysis of transcripts from the YMH podcast.\ndate: \"2020-09-20\"\ncategories: [R, text analysis, Your Mom's House, EDA]\n---\n\n\n\nWhat's going on, mommies? I've been a big fan of [Your Mom's House](http://www.yourmomshousepodcast.com/) for a long time now, and find myself constantly refreshing my podcast feed on Wednesdays waiting for those sweet sweet updates. I've also been getting more and more into text analysis, and so I figured why not combine the two and analyze the transcripts from YMH.\n\nA few months ago, I wrote a [blog post](www.ericekholm.com/blog/youtube-transcripts) describing how to pull transcripts from each video in a Youtube playlist (*6/22/22 update: this link is broken now -- whoops!*). I'm not going to go over that part again, so if you're curious about how to get the data, check out that post.\n\nA couple notes before getting into the analysis:\n\n- The episode transcripts are automatically created by Youtube's speech-to-text model. It's usually really good, but it does make some mistakes & obviously has trouble with proper nouns (e.g. TikTok).\n- Another feature of the fact that the transcripts are generated automatically is that the speaker of each phrase isn't tagged. That is, there are no indications whether it's Tom, Christina, or someone else speaking any given line.\n- I'm only looking at the episodes available on Youtube, and the most recent episode I'm including is the one with Fortune Feimster.\n- I'm going to include my code, but I'm not going to explain step-by-step what each code chunk does here.\n\nWith that out of the way, let's move on and make sure to follow proto.\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(eemisc)\nlibrary(lubridate)\nlibrary(tidytext)\nlibrary(reactable)\nlibrary(tidylo)\nlibrary(igraph)\nlibrary(ggraph)\n\ntheme_set(theme_ee())\n\nherm <- harrypotter::hp(n = 1, option = \"HermioneGranger\")\n\nopts <- options(\n  ggplot2.discrete.fill = list(\n    harrypotter::hp(n = 3, option = \"HermioneGranger\"),\n    harrypotter::hp(n = 5, option = \"Always\")\n  )\n)\n\nymh <- read_csv(here::here(\"data/ymh_trans.csv\")) %>%\n  filter(ep_num > 370)\n\nymh_one_obs <- ymh %>%\n  group_by(title, ep_num) %>%\n  summarize(text = str_c(text, collapse = \" \") %>%\n           str_to_lower()) %>%\n  ungroup()\n```\n:::\n\n\n## General Exploration\n\nThere are a total of 169 episodes, beginning with 394 and ending with 567\n\nNext, let's take a look at the length of the episodes here.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nep_lengths <- ymh %>%\n  group_by(ep_num) %>%\n  summarize(mins = (max(start) + max(duration))/60) %>%\n  ungroup()\n\nep_lengths %>%\n  ggplot(aes(x = ep_num, y = mins)) +\n  geom_line(color = herm) +\n  geom_point(color = herm) +\n  labs(\n    title = \"Length of YMH Episodes\",\n    x = \"Episode Number\",\n    y = \"Length (mins)\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\nIt looks like episodes have tended to get longer up until about episode ~510 or so, and then the length dropped a bit. This might have been due to COVID and the fact that there was a stretch there where they didn't have guests (which made the episodes shorter).\n\nNext, let's take a look at the most common guests on the show.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nymh %>%\n  distinct(title) %>%\n  mutate(guest = str_remove_all(title, \"^.* w/ \") %>%\n           str_remove_all(\" - Ep. \\\\d+| - REUPLOADED\")) %>%\n  filter(str_detect(guest, \"Your Mom\", negate = TRUE)) %>%\n  mutate(guest = str_replace_all(guest, \"\\\\&\", \",\")) %>%\n  separate_rows(guest, sep = \",\") %>%\n  mutate(guest = str_trim(guest)) %>%\n  count(guest, name = \"num_appearances\", sort = TRUE) %>% \n  reactable()\n```\n\n::: {.cell-output-display}\n```{=html}\n<div id=\"htmlwidget-df681837ce7fe261b2bd\" class=\"reactable html-widget\" style=\"width:auto;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-df681837ce7fe261b2bd\">{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"guest\":[\"Bert Kreischer\",\"Dr. Drew Pinsky\",\"Brendan Schaub\",\"Johnny Pemberton\",\"Nikki Glaser\",\"Adam Ray\",\"Andrew Santino\",\"Ari Shaffir\",\"Brian Simpson\",\"Chris D'Elia\",\"H3H3\",\"Jo Koy\",\"Matt Fulchiron\",\"Moshe Kasher\",\"Natasha Leggero\",\"Ryan Sickler\",\"Sam Tripoli\",\"Tom Papa\",\"Tony Hinchcliffe\",\"Wheeler Walker Jr.\",\"Alison Rosen\",\"Alyssa Milano\",\"Amanda Cerny\",\"Andrew Collin\",\"Annie Lederman\",\"Anthony Jeselnik\",\"Big Daddy Kane\",\"Big Jay Oakerson\",\"Bill Burr\",\"Bobby Lee\",\"Brad Williams\",\"Brendon Urie\",\"Brent Weinbach\",\"Brittany Furlan\",\"Cesar Millan\",\"Chad Daniels\",\"Craig Ferguson\",\"Creators of HBO's McMillions\",\"Dane Cook\",\"Danny Brown\",\"Daymond John\",\"DMC\",\"Donnell Rawlings\",\"Doug Mellard\",\"Dr. Drew\",\"Earl Skakel\",\"Elizabeth Lail\",\"Fahim Anwar\",\"Felipe Esparza\",\"Finesse Mitchell\",\"Fortune Feimster\",\"Gabriel Iglesias\",\"Geoff Tate\",\"George Perez\",\"Grant Cardone\",\"Greg Fitzsimmons\",\"Greg Fitzsimmons and Kyle Kinane\",\"Heath Evans\",\"Ian Edwards\",\"J. Elvis Weinstein\",\"Jake Weisman and Matt Ingebretson\",\"Jamie-Lynn Sigler\",\"Jeff Ross\",\"Jessica Kirson\",\"Jim Gaffigan\",\"Joe Rogan\",\"Joey \\\"Coco\\\" Diaz\",\"Joey Diaz\",\"Josh Wolf\",\"Judd Apatow\",\"Kate Kennedy\",\"Kevin Blatt\",\"Kevin Christy\",\"Kevin Nealon\",\"Khalyla\",\"Kreayshawn\",\"Kyle Dunnigan\",\"Link\",\"Liza Treyger\",\"Marc Maron\",\"Martin Riese\",\"Matt Braunger\",\"Maz Jobrani\",\"Melissa Villase√±or\",\"Michael Rapaport\",\"Ms. Pat\",\"Nicole Byer\",\"Paul Gilmartin\",\"Pauly Shore\",\"Redban\",\"Rhett\",\"Robert Hines\",\"Robert Iler\",\"Ron Funches\",\"Russell Peters\",\"Ryan Sickler and Steven Randolph\",\"Ryan Stout\",\"Sarah Tiana\",\"Scott Thompson\",\"Sean Anders\",\"Sean Evans\",\"Sera Gamble\",\"Steve Byrne\",\"Steve Simeone\",\"Steve-O\",\"Taylor Tomlinson\",\"The Sklar Brothers\",\"Theo Von\",\"Tigerbelly\",\"Tim Dillon\",\"Tom Green\",\"Tom Rhodes\",\"Tommy Lee\",\"Too Short\",\"Try It Out Guy\",\"Water Sommelier\",\"Wheeler Walker Jr\",\"Whitney Cummings\",\"Yakov Smirnoff\",\"Yoshi\"],\"num_appearances\":[4,4,3,3,3,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]},\"columns\":[{\"accessor\":\"guest\",\"name\":\"guest\",\"type\":\"character\"},{\"accessor\":\"num_appearances\",\"name\":\"num_appearances\",\"type\":\"numeric\"}],\"defaultPageSize\":10,\"paginationType\":\"numbers\",\"showPageInfo\":true,\"minRows\":1,\"dataKey\":\"6e5fb6a5e4d085b6ea15ad633931eb34\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\n\nNote that this only counts guests named in the title of the episode and won't capture people like Josh Potter, Top Dog & Charo, etc. Dr. Drew is also probably under-counted here, since he makes a lot of cameos via phone call and whatnot (like when Tim and Christine called him up to ask about corn cob sanitation recently). The big takeaway here, though, is that there really aren't that many repeat guests.\n\n## Most Common Words\n\nCool. Let's shift gears here, now, and start to dig into the actual transcripts. Once again, these don't delineate who is speaking, but there is still a lot of data here. I'm going to start by looking at the most commonly-used words across all of the episodes. In doing this, I'm also going to filter out stop words -- things like \"the,\" \"at,\" \"I'm\", etc.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nymh_words <- ymh_one_obs %>%\n  unnest_tokens(word, text)\n\nymh_words <- ymh_words %>%\n  anti_join(stop_words) %>%\n  filter(!(word %in% c(\"yeah\", \"gonna\", \"uh\", \"hey\", \"cuz\")))\n\nymh_words %>%\n  count(word) %>%\n  slice_max(order_by = n, n = 20) %>%\n  ggplot(aes(x = n, y = fct_reorder(word, n))) +\n  geom_col(fill = herm) +\n  labs(\n    title = \"Most Used Words in YMH\",\n    y = NULL,\n    x = \"Count\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\nOk, so, there's a lot we could unpack here. But what stands out to me is that \"fucking\" is the 3rd most commonly used word in the podcast, right after \"people\" and \"guy.\" This isn't surprising to me, but I want to dig into it a little more on an episode-by-episode basis.\n\nI'm going to take a look at how many times the word \"fucking\" appears per episode. Actually, I'm going to look at \"fucking\"s per minute as a way to control for episode length.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nymh_words %>%\n  filter(word == \"fucking\") %>%\n  count(ep_num, word) %>%\n  left_join(ep_lengths, by = \"ep_num\") %>%\n  mutate(fpm = n/mins) %>%\n  ggplot(aes(x = ep_num, y = fpm)) +\n  geom_text(aes(size = fpm), label = \"fucking\", show.legend = FALSE, color = herm) +\n  annotate(\"text\", x =  510, y = 1.6, label = \"Ep 494 with Joey Diaz\", hjust = 0, size = 3.5) +\n  annotate(\"curve\", x = 494, xend = 508, y = 1.68, yend = 1.58, curvature = .4) +\n  labs(\n    x = \"Episode Number\",\n    y = \"'Fucking's Per Minute\",\n    title = \"Fucking's per Minute in YMH Episodes\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fucks_per_ep-1.png){width=672}\n:::\n:::\n\n\n\nIt's maybe no surprise that Episode 494 with Joey Diaz had far and away the highest rate of fucking's-per-minute at 1.76, which is about .76 higher than the next highest (1.008 in Ep 433 with Bill Burr). \n\n## YMH Vocabulary Over Time\n\nNext, I want to look at the evolution of some of the more popular mommy-isms over time -- the memes and vocabulary that define the show. I'm going to start by tracking the use of the word \"Julia\" -- as in \"good morning, julia!\" -- over time, since that's still my favorite clip from the show.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nymh_words %>%\n  filter(word == \"julia\") %>%\n  count(ep_num, word) %>%\n  left_join(x = tibble(ep_num = unique(ep_lengths$ep_num), word = \"julia\"),\n            y = ., by = c(\"ep_num\", \"word\")) %>%\n  mutate(n = replace_na(n, 0)) %>%\n  ggplot(aes(x = ep_num, y = n)) +\n  geom_line(color = herm, size = 1.25) +\n  labs(\n    y = \"Count\",\n    x = \"Episode Number\",\n    title = \"Good Morning, Julia!\",\n    subtitle = \"Counts of the word 'Julia' by episode\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\nSo, we see a pretty clear trend here -- the popularity of \"julia\" starts up right around episode ~470ish, stays high for a few episodes, and then peaks at episode ~477 (give or take), which I assume is the episode where Tony and Catherine actually interview Julia.\n\nThere are tons of other words and phrases I could make this same graph for -- \"four strokes,\" \"let me eat ya,\" etc. -- but I'm going to pick a handful and write a little function that'll extract those from the transcript and then make a faceted graph similar to the above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nisolate_phrases <- function(phrase) {\n  nwords <- str_count(phrase, \" \") + 1\n  \n  token <- if (nwords == 1) {\n    \"words\"\n  } else {\n      \"ngrams\"\n  }\n  \n  tmp <- if (token == \"ngrams\") {\n    ymh_one_obs %>%\n      unnest_tokens(output = words, input = text, token = token, n = nwords)\n  } else {\n    ymh_one_obs %>%\n      unnest_tokens(output = words, input = text, token = token)\n  }\n  \n  tmp %>%\n    filter(words == phrase)\n}\n\nphrases <- c(\"julia\", \"charles\", \"robert paul\", \"retarded\", \"garth\", \"cool guy\", \"fed smoker\", \"feathering\", \"scrum\", \"mystic rick\", \"don't be stingy\", \"piss on me\")\n\nymh_phrase_list <- map(phrases, isolate_phrases)\n\nphrase_grid <- expand_grid(unique(ep_lengths$ep_num), phrases) %>%\n  rename(ep_num = 1)\n\nymh_phrase_df <- ymh_phrase_list %>%\n  bind_rows() %>%\n  count(ep_num, words) %>%\n  left_join(x = phrase_grid, y = ., by = c(\"ep_num\", \"phrases\" = \"words\")) %>%\n  mutate(n = replace_na(n, 0))\n\nymh_phrase_df %>%\n  mutate(phrases = str_to_title(phrases)) %>%\n  ggplot(aes(x = ep_num, y = n)) +\n  geom_line(color = herm) +\n  facet_wrap(~phrases, scales = \"free_y\") +\n  labs(\n    y = \"Count\",\n    x = \"Episode Number\",\n    title = \"Now Here Are Some Cool Phrases\",\n    subtitle = \"Counts of Key YMH Words/Phrases by Episode\"\n  ) +\n  ggthemes::theme_tufte() +\n  theme(\n    plot.title.position = \"plot\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\nOne thing to note here is that the y-axis scale is different for each facet, so it's not really easy to make comparisons across plots. But if I keep them on the same scale, it makes it hard to see movement in anything other than the \"Julia\" plot. Overall, though, I think this does a pretty nice job at tracking the lifespan of some of the common memes/jokes that run through the show.\n\n## Popular Words Across Groups of Episodes\n\nOne limitation of the plots above is that I just chose 12 words/phrases that I think exemplify the show and tracked their usage over time. Like I said, there are tons of other phrases I could have chosen instead, and so the analysis is kinda limited by my personal choice.\n\nOne way to let the data drive the analysis is to look at \"definitive\" words over time. The way I'll do this is -- I'll classify every 5 episodes as a group (yes this is arbitrary, but it seems reasonable enough). Then, I'll calculate the weighted (log) odds of a specific word showing up in that group versus any other group. Then I'll choose the words with the highest odds per group. These will be the \"definitive\" words for that group of episodes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nymh_groups <- tibble(\n  ep_num = unique(ep_lengths$ep_num),\n  group = 1:length(unique(ep_lengths$ep_num)) %/% 5\n) %>%\n  group_by(group) %>%\n  mutate(max = max(ep_num),\n         min = min(ep_num),\n         group_name = glue::glue(\"Ep { min } - Ep { max }\")) %>%\n  ungroup() %>%\n  select(-c(group, min, max))\n\nset.seed(0408)\n\nymh_lo <- ymh_words %>%\n  left_join(x = .,\n            y = ymh_groups,\n            by = \"ep_num\") %>%\n  count(group_name, word) %>%\n  filter(n > 6 & (str_length(word) > 2) & !(word %in% c(\"xiy\", \"pql\", \"8ww\", \"hmh\", \"music\"))) %>%\n  bind_log_odds(set = group_name, feature = word, n = n) %>%\n  group_by(group_name) %>%\n  slice_max(order_by = log_odds_weighted, n = 3, with_ties = FALSE)\n\nymh_lo %>%\n  ungroup() %>%\n  mutate(header = rep(c(\"Most Distinctive\", \"2nd Most Distinctive\", \"3rd Most Distinctive\"),\n            length(unique(ymh_lo$group_name)))) %>%\n  select(-c(n, log_odds_weighted)) %>%\n  rename(`Ep Group` = group_name) %>%\n  pivot_wider(names_from = header, values_from = word) %>%\n  reactable()\n```\n\n::: {.cell-output-display}\n```{=html}\n<div id=\"htmlwidget-406d3250e740af6f7fbf\" class=\"reactable html-widget\" style=\"width:auto;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-406d3250e740af6f7fbf\">{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"Ep Group\":[\"Ep 394 - Ep 397\",\"Ep 398 - Ep 402\",\"Ep 404 - Ep 409\",\"Ep 410 - Ep 414\",\"Ep 415 - Ep 419\",\"Ep 420 - Ep 424\",\"Ep 426 - Ep 430\",\"Ep 431 - Ep 435\",\"Ep 436 - Ep 440\",\"Ep 441 - Ep 445\",\"Ep 446 - Ep 451\",\"Ep 452 - Ep 456\",\"Ep 457 - Ep 461\",\"Ep 462 - Ep 466\",\"Ep 467 - Ep 471\",\"Ep 472 - Ep 476\",\"Ep 477 - Ep 481\",\"Ep 482 - Ep 487\",\"Ep 488 - Ep 492\",\"Ep 493 - Ep 497\",\"Ep 498 - Ep 502\",\"Ep 503 - Ep 507\",\"Ep 508 - Ep 512\",\"Ep 513 - Ep 517\",\"Ep 518 - Ep 522\",\"Ep 523 - Ep 527\",\"Ep 528 - Ep 532\",\"Ep 533 - Ep 537\",\"Ep 538 - Ep 542\",\"Ep 543 - Ep 547\",\"Ep 548 - Ep 552\",\"Ep 553 - Ep 557\",\"Ep 558 - Ep 562\",\"Ep 563 - Ep 567\"],\"Most Distinctive\":[\"waters\",\"marry\",\"soup\",\"soup\",\"machines\",\"machines\",\"machines\",\"wrestling\",\"mcdonald's\",\"tree\",\"stroke\",\"burp\",\"queef\",\"forrest\",\"julia\",\"julia\",\"julia\",\"julia\",\"pimp\",\"homie\",\"persian\",\"panties\",\"bukkake\",\"september\",\"julia\",\"wreck\",\"wolf\",\"charles\",\"charles\",\"quarantine\",\"wrestling\",\"wrestling\",\"jamie\",\"stingy\"],\"2nd Most Distinctive\":[\"personality\",\"blonde\",\"queef\",\"rub\",\"egg\",\"claus\",\"mtv\",\"hog\",\"breast\",\"scat\",\"taco\",\"stroke\",\"marijuana\",\"mcdonald's\",\"throttle\",\"mtv\",\"studio\",\"mcdonald's\",\"bus\",\"prison\",\"500\",\"dmc\",\"intro\",\"jet\",\"wreck\",\"bus\",\"wreck\",\"robert\",\"fed\",\"elementary\",\"poutine\",\"quarantine\",\"jewish\",\"quarantine\"],\"3rd Most Distinctive\":[\"spring\",\"blind\",\"blind\",\"blind\",\"bus\",\"waters\",\"wolf\",\"march\",\"sneeze\",\"defecation\",\"meth\",\"massage\",\"rub\",\"chicago\",\"diaper\",\"canada\",\"machines\",\"terry\",\"tape\",\"aids\",\"coach\",\"intro\",\"tock\",\"intro\",\"jewish\",\"rec\",\"mcdonald's\",\"march\",\"sparkling\",\"virus\",\"quarantine\",\"poutine\",\"quarantine\",\"fed\"]},\"columns\":[{\"accessor\":\"Ep Group\",\"name\":\"Ep Group\",\"type\":[\"glue\",\"character\"]},{\"accessor\":\"Most Distinctive\",\"name\":\"Most Distinctive\",\"type\":\"character\"},{\"accessor\":\"2nd Most Distinctive\",\"name\":\"2nd Most Distinctive\",\"type\":\"character\"},{\"accessor\":\"3rd Most Distinctive\",\"name\":\"3rd Most Distinctive\",\"type\":\"character\"}],\"defaultPageSize\":10,\"paginationType\":\"numbers\",\"showPageInfo\":true,\"minRows\":1,\"dataKey\":\"efc171a4a2b00b240b05a9826f59cd87\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\n\nLet's unpack this a little bit. I'll start at the end. The three most definitive words of Eps 563-567 are \"stingy\" (as in \"don't be stingy, Mark\"), \"quarantine\" (as in the thing everyone is talking about), and \"fed\" (as in \"fed smoker\" -- presumably this is driven by the interview with fed smoker's nemesis). Before that, in Eps 558-562, we have \"jamie\" (as in Jamie-Lynn Sigler), \"jewish\" (the dude with the cool mouse pads), and, again, \"quarantine.\" This analysis gives us a loose idea about what some of the  key themes of these episodes are.\n\n## Connections between Words\n\nThe last thing I'm going to do is look at connections between words. To do this, I'm going to make a network diagram. The basic idea here is that this analysis will visualize words that are commonly used together by drawing links between them. I'm also going to change the size of the word to indicate how often the word gets used (with bigger words being used more).\n\nOne other note -- like in previous steps here, I'm filtering out stop words (e.g. \"the\", \"it\", etc). I also noticed when I first tried this that I got a lot of phrases from Saatva reads -- lots of pairings with the word \"mattress\" and \"delivery,\" so I'm filtering those out as well.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfilter_words <- c(\"music\", \"yeah\", \"uh\", \"huh\", \"hmm\", \"mattress\", \"delivery\")\n\nymh_bigrams <- ymh_one_obs %>%\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2) %>%\n  separate(bigram, c(\"word1\", \"word2\")) %>%\n  filter(!is.element(word1, stop_words$word) &\n           !is.element(word2, stop_words$word) &\n           !is.na(word1) &\n           !is.na(word2) &\n           !is.element(word1, filter_words) &\n           !is.element(word2, filter_words) &\n           word1 != word2) %>%\n  count(word1, word2, sort = TRUE)\n\nymh_bigrams_small <- ymh_bigrams %>%\n  slice_max(order_by = n, n = 100)\n\nymh_verts <- ymh_one_obs %>%\n  unnest_tokens(word, text) %>%\n  count(word) %>%\n  mutate(n = log(n)) %>%\n  filter(is.element(word, ymh_bigrams_small$word1) |\n           is.element(word, ymh_bigrams_small$word2))\n\nymh_net <- graph_from_data_frame(ymh_bigrams_small, vertices = ymh_verts)\n\nset.seed(0409)\n\nymh_net %>%\n  ggraph(layout = \"igraph\", algorithm = \"fr\") +\n  geom_edge_link(aes(size = n), color = herm) +\n  geom_node_point(color = herm) +\n  geom_node_text(aes(label = name, size = n), repel = TRUE, segment.color = herm, label.padding = .1, box.padding = .1) +\n  theme_void() +\n  labs(title = \"Most Commonly Connected Words in YMH\") +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = .5, size = 16))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nThe way to read this graph is to look for words that are connected (note that the direction doesn't mean anything here -- it's just random). So, the words \"tick\" and \"tock\" (TikTok) show up a lot together, as do \"toilet\" and \"paper\" (from Tushy reads). Over on the left side, we see more of a wheel-shaped pattern with \"gonna\" in the middle, which is linked to a bunch of other words -- \"gonna die,\" \"gonna throw\" (up), \"gonna call,\" etc. There's also a little network toward the bottom left where we see connections between guy, guys, black, white, cool, and nice (black guys, cool guys, white guy, etc). A few other standouts to me are \"water champ,\" \"mentall ill\", \"robert paul,\" and, of course \"morning julia.\"\n\nThat's it for now -- keep it high and tight, Jeans.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/core-js-2.5.3/shim.min.js\"></script>\n<script src=\"../../site_libs/react-17.0.0/react.min.js\"></script>\n<script src=\"../../site_libs/react-17.0.0/react-dom.min.js\"></script>\n<script src=\"../../site_libs/reactwidget-1.0.0/react-tools.js\"></script>\n<script src=\"../../site_libs/htmlwidgets-1.5.4/htmlwidgets.js\"></script>\n<script src=\"../../site_libs/reactable-binding-0.3.0/reactable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}