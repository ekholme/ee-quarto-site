{
  "hash": "52809218f364c9b12a0fb34084a6e68b",
  "result": {
    "markdown": "---\ntitle: 'MLE Learning Out Loud 2: Logistic Regression'\ndescription: >\n  Learning maximum likelihood estimation by fitting logistic regression 'by\n  hand' (sort of)\ndate: '2022-09-28'\nformat:\n  html:\n    code-fold: false\ncategories:\n  - Julia\n  - Learning Out Loud\n  - Maximum Likelihood\n  - Logistic Regression\n---\n\nIn a [previous post](https://www.ericekholm.com/posts/mle-learning-julia/), I did some \"learning out loud\" by practicing estimating a few models via maximum likelihood by hand. In this short blog, I figured I could extend this learning by applying what I learned previously to logistic regression.\n\nAs a reminder, the point of these \"learning out loud\" posts is to give myself a medium to work through concepts. Hopefully these metacognitive exercises will benefits others, too. The concepts I'm covering here are things that I'm either learning anew or brushing back up on after not using for a while. But either way, I'm not trying to portray myself as an expert. If you are an expert and you notice I'm doing something wrong, I'd love to hear from you!\n\n# Stating the Problem\n\nSo, what I want to do here is get point estimates for the coefficients in a logistic regression model \"by hand\" (or mostly by hand). I'm going to be doing this in Julia, because I'm also interested in getting better at Julia stuff, but obviously the concepts are the same across any programming language.\n\n# Setup\n\nFirst, we'll load the libraries we're using here and set a seed:\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nusing GLM #to check my work against\nusing Distributions #for the Bernoulli distribution\nusing Random #to set a seed\nusing Optim #to do the acutal optimizing\nusing Statistics #mean and std\nusing RDatasets #to get data\n\nRandom.seed!(0408)\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nTaskLocalRNG()\n```\n:::\n:::\n\n\n# Load and Preprocess Data\n\nNext, we'll load in some data and do some light preprocessing. We'll use the `Default` data from the `RDatasets` package, which presents features describing a given person as well as a binary indicator of whether they defaulted on a credit card payment.\n\nAfter loading the data, we'll pull out the default variable, dummy code it, and then assign it to a vector called `y`. We'll also select just the \"balance\" and \"income\" columns of the data and assign those to `X`. There are other columns we could use as predictors, but that's not really the point here.\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\ndata = RDatasets.dataset(\"ISLR\", \"Default\")\n\ny = [r.Default == \"Yes\" ? 1 : 0 for r in eachrow(data)]\n\nX = data[:, [:Balance, :Income]]\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div class=\"data-frame\"><p>10,000 rows × 2 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>Balance</th><th>Income</th></tr><tr><th></th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th></tr></thead><tbody><tr><th>1</th><td>729.526</td><td>44361.6</td></tr><tr><th>2</th><td>817.18</td><td>12106.1</td></tr><tr><th>3</th><td>1073.55</td><td>31767.1</td></tr><tr><th>4</th><td>529.251</td><td>35704.5</td></tr><tr><th>5</th><td>785.656</td><td>38463.5</td></tr><tr><th>6</th><td>919.589</td><td>7491.56</td></tr><tr><th>7</th><td>825.513</td><td>24905.2</td></tr><tr><th>8</th><td>808.668</td><td>17600.5</td></tr><tr><th>9</th><td>1161.06</td><td>37468.5</td></tr><tr><th>10</th><td>0.0</td><td>29275.3</td></tr><tr><th>11</th><td>0.0</td><td>21871.1</td></tr><tr><th>12</th><td>1220.58</td><td>13268.6</td></tr><tr><th>13</th><td>237.045</td><td>28251.7</td></tr><tr><th>14</th><td>606.742</td><td>44994.6</td></tr><tr><th>15</th><td>1112.97</td><td>23810.2</td></tr><tr><th>16</th><td>286.233</td><td>45042.4</td></tr><tr><th>17</th><td>0.0</td><td>50265.3</td></tr><tr><th>18</th><td>527.54</td><td>17636.5</td></tr><tr><th>19</th><td>485.937</td><td>61566.1</td></tr><tr><th>20</th><td>1095.07</td><td>26464.6</td></tr><tr><th>21</th><td>228.953</td><td>50500.2</td></tr><tr><th>22</th><td>954.262</td><td>32457.5</td></tr><tr><th>23</th><td>1055.96</td><td>51317.9</td></tr><tr><th>24</th><td>641.984</td><td>30466.1</td></tr><tr><th>25</th><td>773.212</td><td>34353.3</td></tr><tr><th>26</th><td>855.009</td><td>25211.3</td></tr><tr><th>27</th><td>643.0</td><td>41473.5</td></tr><tr><th>28</th><td>1454.86</td><td>32189.1</td></tr><tr><th>29</th><td>615.704</td><td>39376.4</td></tr><tr><th>30</th><td>1119.57</td><td>16556.1</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nNext, we'll z_score the predictor variables, convert them to a matrix, and append a column vector of ones to the matrix (so we can estimate the intercept). The `mapcols()` function from `DataFrames.jl` will apply the z_score function to all of the columns in X, which is actually only 2 in this case.\n\nFirst we'll define a z-score function\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\nfunction z_score(x)\n    u = mean(x)\n    s = std(x)\n\n    res = Float64[]\n    for i in 1:lastindex(x)\n        tmp = (x[i] - u) / s\n        push!(res, tmp)\n    end\n\n    return res\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nz_score (generic function with 1 method)\n```\n:::\n:::\n\n\nAnd then we'll actually apply it.\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\nXz = hcat(ones(length(y)), Matrix(mapcols(z_score, X)))\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n10000×3 Matrix{Float64}:\n 1.0  -0.218824    0.813147\n 1.0  -0.037614   -1.60542\n 1.0   0.492386   -0.131206\n 1.0  -0.632861    0.164023\n 1.0  -0.102786    0.370897\n 1.0   0.174098   -1.95142\n 1.0  -0.0203871  -0.645722\n 1.0  -0.0552131  -1.19344\n 1.0   0.673295    0.296293\n 1.0  -1.727      -0.31805\n 1.0  -1.727      -0.873227\n 1.0   0.796355   -1.51825\n 1.0  -1.23695    -0.394799\n ⋮                \n 1.0  -1.727       0.616625\n 1.0   0.338849   -1.01252\n 1.0  -0.957166   -0.610505\n 1.0  -0.36504     1.59599\n 1.0   0.571147    0.897805\n 1.0   0.213889    1.73331\n 1.0  -1.37056    -1.39173\n 1.0  -0.255977    1.46029\n 1.0  -0.160036   -1.03896\n 1.0   0.02075     1.88347\n 1.0   1.51667     0.236351\n 1.0  -1.31163    -1.24874\n```\n:::\n:::\n\n\n# Define a Logistic Function\n\nNext, we'll write a logistic function that will implement the logistic transformation. This is built into the `StatsFuns.jl` package, but I want to write it out by hand to reinforce what it is. We'll use this to predict y values with a given input (which will actually be X*$\\beta$)\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\nmy_logistic(x) = exp(x) / (1 + exp(x))\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nmy_logistic (generic function with 1 method)\n```\n:::\n:::\n\n\n# Define a Maximum Likelihood Estimator\n\nNow that we have some data, we can write a function that uses maximum likelihood estimation to give us the best $\\beta$ parameters for our given **X** and y. If you want to brush up on maximum likelihood, you can read [my previous \"learning out loud\" post](https://www.ericekholm.com/posts/mle-learning-julia/), or you can probably find materials written by someone who knows way more than I do. Either way, I'm not going to recap what MLE is here.\n\nLet's define our function that we'll use to estimate $\\beta$. The important thing to keep in mind is that the return value of this function isn't the $\\beta$ values, but rather the negative log likelihood, since this is what we we want to optimize.\n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\nfunction ml_logreg(x, y, b)\n\n    ŷ = my_logistic.(x*b)\n    res = Float64[]\n\n    for i in 1:lastindex(y)\n        push!(res, logpdf(Bernoulli(ŷ[i]), y[i]))\n    end\n\n    ret = -sum(res)\n\n    return ret\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\nml_logreg (generic function with 1 method)\n```\n:::\n:::\n\n\nSo what's going on in this code?\n\n1. We're getting $ŷ$ estimates for a given x and b by running them through the `my_logistic()` function. This will give us a 10000x1 vector\n1. We're instantiating an empty vector that will (eventually) contain Float64 values.\n1. For each index in $ŷ$ (i.e. 1 through 10000), we're getting the log-likelihood of the true outcome (y[i]) given a Bernoulli distribution parameterized by success rate $ŷ$[i].\n\nI think this is the trickiest part of the whole problem, so I want to put it into words to make sure I understand it. In our problem, our y values are either 0 or 1. And the output of the `my_logistic()` function is going to be, for each y, a predicted probability that $y = 1$, i.e. a predicted success rate. Since a Bernoulli distribution is parameterized by a given success rate and models the outcome of a single yes/no (1/0) trial, it makes sense to use this to generate the likelihoods we want to maximize.\n\nMore concretely, the likelihoods we get will be dependent on:\n\n1. the provided success rate *p*, and\n1. the actual outcome\n\nWhere values of *p* that are closer to the actual outcome will be larger:\n\n::: {.cell execution_count=7}\n``` {.julia .cell-code}\nlogpdf(Bernoulli(.5), 1)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n-0.6931471805599453\n```\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.julia .cell-code}\n#will be larger than the previous\nlogpdf(Bernoulli(.8), 1)\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n-0.2231435513142097\n```\n:::\n:::\n\n\n::: {.cell execution_count=9}\n``` {.julia .cell-code}\n#will be even larger\nlogpdf(Bernoulli(.99), 1)\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n-0.01005033585350145\n```\n:::\n:::\n\n\nAnd inversely, you can imagine that if the outcome were 0, we'd want our predicted success rate to be very low.\n\nReturning to our `ml_logreg()` function, what we're doing then is applying this logic to all of our $ŷ$ and corresponding *y* values (i.e. we're getting the likelihood of *y* for a given $ŷ$), and then we're creating a vector with all of these likelihoods -- that's what the `push!(...)` notation is doing -- pushing these likelihoods to the empty float vector we created.\n\nFinally, we're summing all of our likelihoods and then multiplying the result by negative one, since the optimizer we're using actually wants to *minimize* a loss function rather than *maximize* a loss function.\n\nWe can run this function by providing any X, y, and $\\beta$, and it'll give us back a negative loglikelihood -- the negative sum of all of the individual likelihoods.\n\n::: {.cell execution_count=10}\n``` {.julia .cell-code}\n#just some arbitrary numbers to test the function with\nstart_vals = [.1, .1, .1]\n\nml_logreg(Xz, y, start_vals)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n7372.506385031871\n```\n:::\n:::\n\n\n# Optimize $\\beta$\n\nSo the above gives us the likelihood for a starting value of $\\beta$, but we want to find the *best* values of $\\beta$. To do that, we can optimize the function. Like I said in my previous post, the optimizers are written by people much smarter than I am, so I'm just going to use that package rather than futz around with doing any, like, calculus by hand -- although maybe that's a topic for a later learning out loud post.\n\n::: {.cell execution_count=11}\n``` {.julia .cell-code}\nres = optimize(b -> ml_logreg(Xz, y, b), start_vals)\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n * Status: success\n\n * Candidate solution\n    Final objective value:     7.894831e+02\n\n * Found with\n    Algorithm:     Nelder-Mead\n\n * Convergence measures\n    √(Σ(yᵢ-ȳ)²)/n ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   0  (vs limit Inf)\n    Iterations:    103\n    f(x) calls:    190\n```\n:::\n:::\n\n\nAnd then we can get the $\\beta$ coefficients that minimize the loss function (i.e. that maximize the likelihood)\n\n::: {.cell execution_count=12}\n``` {.julia .cell-code}\nOptim.minimizer(res)\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n3-element Vector{Float64}:\n -6.125561839584853\n  2.731586594783831\n  0.2775242967112382\n```\n:::\n:::\n\n\nAnd just to confirm that we did this correctly, we can check our point estimates against what we'd get if we fit the model using the `GLM` package.\n\n::: {.cell execution_count=13}\n``` {.julia .cell-code}\nlogreg_res = glm(Xz, y, Binomial())\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\nGeneralizedLinearModel{GLM.GlmResp{Vector{Float64}, Binomial{Float64}, LogitLink}, GLM.DensePredChol{Float64, LinearAlgebra.Cholesky{Float64, Matrix{Float64}}}}:\n\nCoefficients:\n─────────────────────────────────────────────────────────────────\n        Coef.  Std. Error       z  Pr(>|z|)  Lower 95%  Upper 95%\n─────────────────────────────────────────────────────────────────\nx1  -6.12557    0.187562   -32.66    <1e-99  -6.49318   -5.75795\nx2   2.73159    0.109984    24.84    <1e-99   2.51602    2.94715\nx3   0.277522   0.0664854    4.17    <1e-04   0.147213   0.407831\n─────────────────────────────────────────────────────────────────\n```\n:::\n:::\n\n\nCool beans!\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}