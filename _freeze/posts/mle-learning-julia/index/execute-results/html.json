{
  "hash": "e10b4dcef6f90470729dd19f70510a7b",
  "result": {
    "markdown": "---\ntitle: MLE Learning Out Loud\ndescription: |\n  Learning maximum likelihood estimation by fitting models 'by hand' (in Julia!)\nauthor: EE\ndate: '2022-08-31'\nformat:\n  html:\n    code-fold: false\n---\n\n*Disclaimer! The whole point of these \"learning out loud\" blog posts is to give myself a venue in which to practice/learn various statistics and programming concepts. I'm deciding to post these on my website both to normalize this notion of learning in public and also to invite people who know more than me to provide feedback. If I get something wrong, I'd love for you to tell me!*\n\n# Maury Povich as a metaphor for maximum likelihood estimation\n\nSo this obviously isn't 100% mathematically rigorous, but based on my understanding of maximum likelihood estimation (MLE), I think it's kind of like the Maury Povich show...\n\nBack when I was in high school, some of my friends and I used to eat lunch in our track coach's classroom and watch the Maury Povich show. For those of you that haven't every watched *Maury*, it's an...interesting study of human behavior...and worth checking out. But basically it's like Jerry Springer or any of these other daytime drama-fests, covering everything from infidelity to obesity to insane behavior and everything in between. But Maury's specialty was paternity tests.\n\nAlthough the details of the paternity test episodes differed slightly, a common pattern was that a pregnant woman along with multiple men would come on the show, and each of the men would take a paternity test. Maury would ask the men and the women to describe how confident they were in the results of the test, and the men would usually offer up something like: \n\n*\"I am a thousand percent sure I am not the father.\"*\n\nWhich would then elicit the next man to say:\n\n*\"Well I am one million percent sure I'm not the father!\"*\n\nWhich would in turn elicit animated reactions from the audience, the mother, and the other potential father(s) on the stage.\n\n**So how's this like maximum likelihood estimation?**\n\nSo my understanding of the logic of maximum likelihood estimation (MLE) is that, given a set of data, we can estimate the likelihood of a distribution parameterized by a given set of parameters. Imagine we have a bunch of measures of adult heights, and we assume that height is normally distributed. We know that a normal distribution is defined by its mean and its standard deviation. And so using our set of data, we can estimate the likelihood of any combination of mean and standard deviation (i.e. any set of parameters) given this data. And the parameters with the maximum likelihood are the \"best\" given our set of data. We'll walk through this with examples later.\n\nWhat matters here though is that the actual number describing the likelihood (or the log-likelihood, more likely) doesn't really matter. It's not arbitrary, but it'll differ depending upon how many observations are in your dataset, the distribution you're using, etc. The values of the (log)likelihood relative to one another are what matters. And in this respect I'm reminded of Maury's paternity tests.\n\nIt doesn't matter if a guest on the show says he's 100% sure the baby isn't his. If the guy next to him says he's 110% sure the baby's not his, then he's more certain than the first guy. Likewise, if the first guy says he's one million percent sure the baby isn't his, he still \"loses\" if the guy next to him says he's 2 million percent sure. The actual number doesn't matter -- what matters is the estimate relative to the other estimates.\n\n# Some Examples\n\nI'm not 100% sure the Maury analogy actually holds up, but whatever, let's work through some examples\n\nFirst we'll load some necessary packages.\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nusing Distributions\nusing CairoMakie\nusing Random\nusing Optim\nusing GLM\nusing DataFrames\n```\n:::\n\n\n# Case 1: Fitting a Normal Distribution\n\nThis is the simplest case. First, we're going to generate some sample data, s, from a normal distribution with $\\mu = 0$ and $\\sigma = 1$\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nRandom.seed!(0408)\ns = rand(Normal(), 10000)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n10000-element Vector{Float64}:\n -1.4055556573814212\n  0.8813161144909877\n  0.4695240597638853\n  1.0596565592604608\n -1.1909245261358548\n -1.4819187811057175\n -0.40408041211016915\n -0.37805385034816524\n -1.5132047920081557\n  2.2528479354589197\n -1.6595728371412546\n  1.321172026499611\n -1.5741912720732054\n  ⋮\n -0.6706076665047674\n  1.313413766916552\n -0.5776340358208154\n  2.2968511578121857\n  0.6020915294889897\n  0.19216658269979192\n  0.8936776607551574\n -0.5898756308872724\n  0.2424739897566387\n  0.7926169568329148\n -0.46603730352631795\n -0.6572491362891565\n```\n:::\n:::\n\n\nThen we'll generate a bunch of normal distributions with various means and standard deviations\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\nμs = collect(-2.0:2.0)\nσs = [0.5:0.5:2;]\n\nds = []\n\nfor i = μs, j = σs\n    d = Normal(i, j)\n    push!(ds, d)\nend\n```\n:::\n\n\nSo our task now is going to be to determine the likelihood of each distribution (defined with a given set a parameters) given our data, *s*, that we've drawn from a normal distribution with $\\mu = 0$ and $\\sigma = 1$\n\nTo do this, we use the probability density function (pdf) of our normal distribution to determine the likelihood of the parameters for any given observation. Fortunately, Julia (and other languages) have tools that can help us do this without having to write out the entire equation by hand. That said, here's the equation -- even though I'm not going to call it directly, it's probably useful to see it.\n\n\n$$f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma}} \\exp[-\\frac{(x - \\mu)^2}{2\\sigma^2}]$$\n\n\n\nLet's take a look at the first observation and the first distribution we defined:\n\nThe first value in our sample is:\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\ns[1]\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n-1.4055556573814212\n```\n:::\n:::\n\n\nAnd the first distribution we'll look at is\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\nds[1]\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nNormal{Float64}(μ=-2.0, σ=0.5)\n```\n:::\n:::\n\n\nAnd if we look at the pdf of this, we get:\n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\npdf(ds[1], s[1])\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n0.39356088133821826\n```\n:::\n:::\n\n\nI'm not a statistician (hence these learning posts), but my understanding of this is that it generally represents the \"fit\" of the distribution (and its parameters) to the given sample/data point. These values will be bound between 0 and 1, since they're likelihoods, with higher values indicating better fit/higher likelihood.\n\nThe next step is to convert this to a log scale, since logging allows us to sum things rather than multiply them (which we're gonna do soon).\n\n::: {.cell execution_count=7}\n``` {.julia .cell-code}\nlogpdf(ds[1], s[1])\n#same as log(pdf(ds[1], s[]1))\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n-0.9325195055871961\n```\n:::\n:::\n\n\nSo this gives us the log likelihood of a given data point. But now we need to do this for all of the data points in our sample to determine the \"fit\"/likelihood of the distribution to our whole sample.\n\n::: {.cell execution_count=8}\n``` {.julia .cell-code}\nsum(logpdf.(ds[1], s))\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n-103363.07786213113\n```\n:::\n:::\n\n\nApparently `Distributions.jl` gives us a helper for this via `loglikelihood`, so the above is the same as:\n\n::: {.cell execution_count=9}\n``` {.julia .cell-code}\nloglikelihood(ds[1], s)\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n-103363.07786213112\n```\n:::\n:::\n\n\nSo this gives us the (log)likelihood of a distribution (normal, in this case, defined by parameters $\\mu$ and $\\sigma$) given our sample. That is, the relatively plausibility of the parameters given our data. The goal then is to pick the *best* distribution/parameters, which we can do by *maximizing the likelihood*. In Maury terms, we want to find guy who's most sure that the baby isn't his.\n\nOr, apparently, it's more common to minimize the negative loglikelihood, which is the same thing (and called logloss, I guess).\n\nSo let's do this for all of the distributions we specified earlier\n\n::: {.cell execution_count=10}\n``` {.julia .cell-code}\nlls = []\n\nfor i in ds\n    res = -loglikelihood(i, s)\n    push!(lls, res)\nend\n\nlls = Float64.(lls)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n20-element Vector{Float64}:\n 103363.07786213112\n  34465.6764159677\n  24477.94356153769\n  22439.92990862643\n  42816.83247490566\n  19329.115069161333\n  17750.58296295708\n  18655.789571924823\n  22270.587087680186\n  14192.553722354956\n  15467.666808820915\n  17371.649235223234\n  41724.3417004547\n  19055.992375548587\n  17629.195099129196\n  18587.50889852165\n 101178.09631322921\n  33919.43102874222\n  24235.16783388192\n  22303.36856182006\n```\n:::\n:::\n\n\nAnd then we can plot the loglikelihoods we get:\n\n::: {.cell execution_count=11}\n``` {.julia .cell-code}\nind = collect(1.0:length(ds))\n\nlines(ind, lls)\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n![](index_files/figure-html/cell-12-output-1.svg){}\n:::\n:::\n\n\nNotice that our negative log likelihood is minimized in the 10th distribution, so let's take a look at what that is\n\n::: {.cell execution_count=12}\n``` {.julia .cell-code}\nds[10]\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\nNormal{Float64}(μ=0.0, σ=1.0)\n```\n:::\n:::\n\n\nThis makes sense! This was the distribution that we drew our samples from!\n\nIf we want to do this without looking at a plot, we can apparently do this:\n\n::: {.cell execution_count=13}\n``` {.julia .cell-code}\n#get the index of the minimum value in lls\nmin_ll = findall(lls .== minimum(lls))\n\n#get the distribution at this index\nds[min_ll]\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n1-element Vector{Any}:\n Normal{Float64}(μ=0.0, σ=1.0)\n```\n:::\n:::\n\n\nSo this tells us that -- of the distributions we tested! -- the most likely distribution given our data is a normal distribution with mean of 0 and standard deviation of 1. This doesn't necessarily mean that this $\\mu = 0$ and $\\sigma = 1$ are the *optimal* parameters. There could be better parameters that we didn't test, and so in the future we'd want to probably use some sort of optimizing functions that can do all of the math for us.\n\n# Case 2: Simple Linear Regression\n\nSo now let's move on a bit and try a simple linear regression. First we'll just generate some fake data and a \"ground truth\" function\n\n::: {.cell execution_count=14}\n``` {.julia .cell-code}\n#generate some x values\nx = collect(0:.1:10)\n\n#generate error\nϵ = rand(Normal(0, 1), length(x))\n\n#define a function relating x to y\nf(x) = 0.5 + 2*x\n\n#generate y as f(x) plus error\ny = f.(x) .+ ϵ\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n101-element Vector{Float64}:\n  2.4461255238293758\n  1.2496723713219855\n  1.5014319678963934\n  1.8228982131749496\n -0.4320716243406362\n  0.9628100854937409\n  2.475384749019799\n  2.047025196204242\n  1.7487030877341891\n  1.4865883008076408\n  0.405749179591091\n  2.5585877608457355\n  2.956751811280712\n  ⋮\n 19.232878652117428\n 17.53450559237765\n 19.290332010598092\n 18.144129060042054\n 18.413568163778812\n 17.89449730367819\n 19.175282300038607\n 20.826640516579364\n 19.21519350783753\n 19.070233221768582\n 21.072296712369102\n 19.469128284822276\n```\n:::\n:::\n\n\nAnd then we can plot the x and y values we just created:\n\n::: {.cell execution_count=15}\n``` {.julia .cell-code}\nCairoMakie.scatter(x, y)\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n![](index_files/figure-html/cell-16-output-1.svg){}\n:::\n:::\n\n\nAnother way to think about the above is that we expect a linear relationship between x and y in the form of\n\n$y = \\alpha + \\beta x + \\epsilon$\n\nWe need to estimate alpha and beta in a way that optimally fits this line, and we can do this with maximum likelihood. We can take advantage of the fact that linear regression assumes that residuals are normally distributed with an expected value (mean) of 0, since this will provide as with a distribution we can try to parameterize optimally. Since the residuals are dependent upon the predicted values of y, and since the predicted values of y are dependent on the intercept ($\\alpha$) and the coefficient ($\\beta$), this will give us a way to estimate the terms in the regression line.\n\n$\\sigma$ is not super important to us, but we still need to estimate it. We can estimate the loglikelihood of a given set of parameters using the function below.\n\n::: {.cell execution_count=16}\n``` {.julia .cell-code}\nfunction max_ll_reg(x, y, params)\n\n    α = params[1]\n    β = params[2]\n    σ = params[3]\n\n    ŷ = α .+ x.*β\n\n    resids = y .- ŷ\n\n    d = Normal(0, σ)\n\n    ll = -loglikelihood(d, resids)\n    \n    return ll\n\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\nmax_ll_reg (generic function with 1 method)\n```\n:::\n:::\n\n\nAnd let's see how this works by passing in some generic values -- .5 as the intercept, 2 as the beta coefficient, and 1 as the error variance.\n\n::: {.cell execution_count=17}\n``` {.julia .cell-code}\nyy = max_ll_reg(x, y, [.5, 2, 1])\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\n137.00423917573127\n```\n:::\n:::\n\n\nThe next step then is to optimize this. We pass some starting values and our `max_ll_reg` function into an optimizer, tell it to find the optimal values for the parameters ($\\alpha$, $\\beta$, and $\\sigma$), and then the magical optimizing algorithm written by people much smarter than me will give us our coefficients.\n\n::: {.cell execution_count=18}\n``` {.julia .cell-code}\nres = optimize(params -> max_ll_reg(x, y, params), [0.0, 1.0, 1.0])\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\n * Status: success\n\n * Candidate solution\n    Final objective value:     1.361561e+02\n\n * Found with\n    Algorithm:     Nelder-Mead\n\n * Convergence measures\n    √(Σ(yᵢ-ȳ)²)/n ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   0  (vs limit Inf)\n    Iterations:    130\n    f(x) calls:    234\n```\n:::\n:::\n\n\nAnd then this will give us the maximum likelihood solution for our regression equation.\n\n::: {.cell execution_count=19}\n``` {.julia .cell-code}\nOptim.minimizer(res)\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\n3-element Vector{Float64}:\n 0.6262632240571052\n 1.9908770881500015\n 0.9315933450872507\n```\n:::\n:::\n\n\nWe can check this by fitting the model with the `GLM` package\n\n::: {.cell execution_count=20}\n``` {.julia .cell-code}\ndata = DataFrame(X = x, Y = y)\n\nols_res = lm(@formula(Y ~ X), data)\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}}}}, Matrix{Float64}}\n\nY ~ 1 + X\n\nCoefficients:\n────────────────────────────────────────────────────────────────────────\n                Coef.  Std. Error      t  Pr(>|t|)  Lower 95%  Upper 95%\n────────────────────────────────────────────────────────────────────────\n(Intercept)  0.626272   0.185875    3.37    0.0011   0.257455   0.995089\nX            1.99088    0.0321144  61.99    <1e-80   1.92715    2.0546\n────────────────────────────────────────────────────────────────────────\n```\n:::\n:::\n\n\net voila, we get the same $\\alpha$ and $\\beta$! The coefficients aren't exactly the same as the ones we specified when generating the data, but that's because of the error we introduced.\n\nIt's maybe also worth nothing that Julia lets us solve the equation via the `\\` operator, which apparently provides a shorthand for solving systems of linear equations:\n\n::: {.cell execution_count=21}\n``` {.julia .cell-code}\n#we have to include a column of 1s in the matrix to get the intercept\nxmat = hcat(ones(length(x)), x)\n\nxmat \\ y\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\n2-element Vector{Float64}:\n 0.6262717121103298\n 1.9908750493937837\n```\n:::\n:::\n\n\n# Case 3: Multiple Regression\n\nAnd I think we can extend the same logic above to multiple regression. The first step is to generate some data:\n\n::: {.cell execution_count=22}\n``` {.julia .cell-code}\n#make a 100x3 matrix of random numbers\ntmp = randn(100, 3)\n\n#append a leading column of 1s (so we can get the intercept)\n𝐗 = hcat(ones(100), tmp)\n\n#provide 'ground truth' coefficients\n𝚩 = [.5, 1, 2, 3]\n\n#define a function to multiply X by B\nf₂(X) = X*𝚩\n\n#create some error\nϵ = rand(Normal(0, .5), size(𝐗)[1])\n\n#make outcome values that comprise our generating function plus error\n𝐘 = f₂(𝐗) + ϵ\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\n100-element Vector{Float64}:\n  1.9539840955199055\n  2.2807060973619135\n  2.406665555383834\n -0.014731760693462048\n  0.6002032048684441\n -3.531148702629271\n  3.194699866301238\n -1.17858666703318\n -0.31117832513371646\n  1.5595030004201824\n  7.314823243199307\n -2.1182414214687673\n -3.502694667516171\n  ⋮\n  9.205296659574488\n  4.233455153011074\n  5.620823053237128\n -2.4088759447640156\n  5.127431971734125\n  1.0043279157869205\n  3.6343775497324184\n  2.611885689812401\n  0.11077494956658729\n  3.06142672043232\n  1.961141975667116\n -0.013605916161866072\n```\n:::\n:::\n\n\nThen we can define another function to return the maximum likelihood. This is the same as the simple regression function above, except it's generalized to allow for more than 1 slope coefficient. Julia provides some neat functionality via the `begin` and `end` keywords that let us access the first and last elements of a vector, and we can even do things like `end-1` to get the second-to-last element, which is pretty nifty.\n\n::: {.cell execution_count=23}\n``` {.julia .cell-code}\nfunction max_ll_mreg(x, y, params)\n    𝚩 = params[begin:end-1]\n    σ = params[end]\n\n    ŷ = x*𝚩\n\n    resids = y .- ŷ\n\n    d = Normal(0, σ)\n\n    ll = -loglikelihood(d, resids)\n\n    return ll\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\nmax_ll_mreg (generic function with 1 method)\n```\n:::\n:::\n\n\nThen we can do the same thing as before -- provide some starting parameters (coefficients), and tell our super-smart optimizer function to give us the parameters that maximize the likelihood.\n\n::: {.cell execution_count=24}\n``` {.julia .cell-code}\nstart_params = [.4, .5, 1.5, 4.0, 1.0]\n\nmreg_res = optimize(params -> max_ll_mreg(𝐗, 𝐘, params), start_params)\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\n * Status: success\n\n * Candidate solution\n    Final objective value:     6.860916e+01\n\n * Found with\n    Algorithm:     Nelder-Mead\n\n * Convergence measures\n    √(Σ(yᵢ-ȳ)²)/n ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   0  (vs limit Inf)\n    Iterations:    221\n    f(x) calls:    392\n```\n:::\n:::\n\n\nAnd then we can show the results:\n\n::: {.cell execution_count=25}\n``` {.julia .cell-code}\nOptim.minimizer(mreg_res)\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\n5-element Vector{Float64}:\n 0.42976727494283473\n 1.0367345683471323\n 1.8923643524058003\n 3.0304421915621127\n 0.4805357922701782\n```\n:::\n:::\n\n\nAnd we can check that these are returning the correct values\n\n::: {.cell execution_count=26}\n``` {.julia .cell-code}\n𝐗 \\ 𝐘\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\n4-element Vector{Float64}:\n 0.42976708591782187\n 1.0367343909692166\n 1.8923640529063954\n 3.0304412982361364\n```\n:::\n:::\n\n\nAlternatively, we could have written out the joint pdf for the normal distribution by hand, like below.\n\nFirst we can define this function: \n\n::: {.cell execution_count=27}\n``` {.julia .cell-code}\nfunction alt_mle_mlr(x, y, params)\n    𝚩 = params[begin:end-1]\n    σ = params[end]\n\n    ŷ = x*𝚩\n\n    n = length(ŷ)\n\n    ll = -n/2*log(2π) - n/2* log(σ^2) - (sum((y .- ŷ).^2) / (2σ^2))\n    \n    ll = -ll\n\n    return ll\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```\nalt_mle_mlr (generic function with 1 method)\n```\n:::\n:::\n\n\nThen see what the loglikelihood is given our starting parameters:\n\n::: {.cell execution_count=28}\n``` {.julia .cell-code}\nalt_mle_mlr(𝐗, 𝐘, start_params)\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\n174.11351826768353\n```\n:::\n:::\n\n\nThen optimize the function:\n\n::: {.cell execution_count=29}\n``` {.julia .cell-code}\nmreg_res2 = optimize(params -> alt_mle_mlr(𝐗, 𝐘, params), start_params)\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\n * Status: success\n\n * Candidate solution\n    Final objective value:     6.860916e+01\n\n * Found with\n    Algorithm:     Nelder-Mead\n\n * Convergence measures\n    √(Σ(yᵢ-ȳ)²)/n ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   0  (vs limit Inf)\n    Iterations:    221\n    f(x) calls:    392\n```\n:::\n:::\n\n\nAnd check the results:\n\n::: {.cell execution_count=30}\n``` {.julia .cell-code}\nOptim.minimizer(mreg_res2)\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```\n5-element Vector{Float64}:\n 0.42976727494283473\n 1.0367345683471323\n 1.8923643524058003\n 3.0304421915621127\n 0.4805357922701782\n```\n:::\n:::\n\n\nAnd there we go. Hopefully that was helpful for some others. I'll probably do some more of these \"learning out loud\" posts as I dig into some more math, Julia, or other topics.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}