{
  "hash": "9aec5babf99f040836c124b07d03098d",
  "result": {
    "markdown": "---\ntitle: \"Scratonicity - Part 1\"\ndescription: |\n  An initial exploration of dialogue from The Office\ndate: \"2020-03-14\"\ncategories: [R, The Office, EDA, Text Analysis]\n---\n\n\n\n## I Just Want to Lie on the Beach and Eat Hog Dogs\n\nWho doesn't love _The Office_? I went through high school and college following the on-again off-again romance of Jim and Pam, the Icarus-esque ascendancy and fall of Ryan the Temp, and the perpetual cringey-ness of Michael Scott. And aside from that handful of people who fled the room in a cold panic at even the mention of \"Scott's Tots,\" I think this was probably true for most of my generation. You'd be hard pressed to go to a Halloween party in the late aughts without seeing someone dressed in the tan-and-yellow palette of Dwight Schrute, and before the modern era of Netflix and Hulu, we regularly set aside Thursday nights to tune into NBC.\n\nAnd although I was a big _Office_ fan several years ago, I haven't thought too too much about it recently -- at least until I stumbled across the release of the `{schrute}` package recently. [`{schrute}`](https://CRAN.R-project.org/package=schrute) is an R package with one purpose -- presenting the entire transcripts of _The Office_ in tibble format, making the dialogue of the show much easier to analyze. I played around with the package and a [quick sentiment analysis](https://github.com/ekholme/TidyTuesday/blob/master/53%20-%20the%20office/jim%20pam%20script.Rmd) back in December when I looked at the sentiments expressed by Jim and Pam over the course of the series:\n\n[![](https://github.com/ekholme/TidyTuesday/blob/master/53%20-%20the%20office/jim_pam_sentiments.jpg?raw=true){width=100%}](https://github.com/ekholme/TidyTuesday/blob/master/53%20-%20the%20office/jim_pam_sentiments.jpg?raw=true)\n\nThere's a ton more we can do with the package, though, and with the transcripts available and in a clean format, plus all of the tools `R` has available for text analysis, I figured I'd do a mini-series of blog posts analyzing some of the data. The plan (as of now) is to start this first post with some exploratory analyses and visualizations, then move into some other types of modeling in later posts. I'll also include all of my code throughout.\n\n<br/>\n\n_**As a quick aside, a lot of the text analyses I'm going to work through in this first post come from the [Text Mining with R book by Julia Silge and David Robinson.](https://www.tidytextmining.com/) I'd strongly recommend this to anyone looking to dive into analyzing text data.**_\n\n## Setup\n\n\n\n\n\nFirst, let's read in the data. I'm also going to limit the data to the first seven seasons, which spans the \"Michael Scott\" era. Not only because these are the best seasons (which they undoubtedly are), but also because doing so eliminates a major confounding factor (i.e. Steve Carell leaving the show) from the analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noffice <- theoffice %>%\n  filter(as.numeric(season) <= 7)\n\nglimpse(office)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 41,348\nColumns: 12\n$ index            <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16…\n$ season           <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ episode          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ episode_name     <chr> \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\",…\n$ director         <chr> \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis…\n$ writer           <chr> \"Ricky Gervais;Stephen Merchant;Greg Daniels\", \"Ricky…\n$ character        <chr> \"Michael\", \"Jim\", \"Michael\", \"Jim\", \"Michael\", \"Micha…\n$ text             <chr> \"All right Jim. Your quarterlies look very good. How …\n$ text_w_direction <chr> \"All right Jim. Your quarterlies look very good. How …\n$ imdb_rating      <dbl> 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6…\n$ total_votes      <int> 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706,…\n$ air_date         <fct> 2005-03-24, 2005-03-24, 2005-03-24, 2005-03-24, 2005-…\n```\n:::\n:::\n\n\n\nJust to check that the data we have matches what we're expecting, let's take a look at which seasons we have, plus how many episodes we have per season.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noffice %>%\n  distinct(season, episode) %>%\n  count(season, name = \"num_episodes\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 7 × 2\n  season num_episodes\n   <int>        <int>\n1      1            6\n2      2           22\n3      3           23\n4      4           14\n5      5           26\n6      6           24\n7      7           24\n```\n:::\n:::\n\n\n\nThis generally matches what Wikipedia is telling me once we account for two-part episodes, and we can see that we only have the first seven seasons.\n\n### Me think, why waste time say lot word, when few word do trick\n\nA few questions we can ask here involve how much/how often different characters speak. Probably the most basic question is: who has the most lines?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntop_20_chars <- office %>%\n  count(character, sort = TRUE) %>%\n  top_n(20) %>%\n  pull(character)\noffice %>%\n  filter(is.element(character, top_20_chars)) %>%\n  count(character, sort = TRUE) %>%\n  ggplot(aes(x = fct_reorder(character, n), y = n)) +\n  geom_col(fill = purple) +\n  labs(\n    x = \"\",\n    y = \"Number of Lines\",\n    title = \"Who Has the Most Lines?\"\n  ) +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/most lines-1.png){width=672}\n:::\n:::\n\n\n\nIt's not surprising to me that Michael has the most lines, but the magnitude of the difference between him and Dwight is a bit surprising.\n\nWhat if we look at the number of lines per season?\n\n\n::: {.cell}\n\n```{.r .cell-code}\noffice %>%\n  filter(is.element(character, top_20_chars)) %>%\n  count(character, season, sort = TRUE) %>%\n  ggplot(aes(x = as.numeric(season), y = n, color = character)) +\n    geom_line() +\n    geom_point()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/character lines lineplot-1.png){width=672}\n:::\n:::\n\n\n\nThis isn't terribly informative -- let's go back to our bar graph.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noffice %>%\n  filter(is.element(character, top_20_chars)) %>%\n  count(character, season, sort = TRUE) %>%\n  group_by(season) %>%\n  top_n(n = 5) %>%\n  ungroup() %>%\n  ggplot(aes(x = fct_reorder(character, n), y = n)) +\n    geom_col(fill = purple) +\n    coord_flip() +\n    facet_wrap(~season, scales = \"free\") +\n    labs(\n      title = \"Number of Lines by Season\",\n      x = \"\",\n      y = \"\"\n    ) +\n    theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/lines by season-1.png){width=672}\n:::\n:::\n\n\n\nAgain, not surprising that Michael has the most lines across all seasons. Dwight, Jim, and Pam are always the next three, but the orders change a bit between seasons. The fifth spot is where we see some movement, with Oscar and Jan sneaking in before Andy joins the show in Season 3. And check out Ryan in S4!\n\n### Sometimes I'll start a sentence and I don't even know where it's going\n\nSo, above, we just looked at the number of _lines_ each character had. Another option is to do some analyses at the word level. For instance, we can look at patterns of word usage for individual characters, between characters, and over time.\n\nTo start with this, I'm going to restructure the data so we have one word per row in our tibble. I'm also going to remove \"stop words\" (e.g. \"a,\" \"the,\" \"at\"), since these will show up a lot but (for our purposes) aren't actually all that meaningful:\n\n\n::: {.cell}\n\n```{.r .cell-code}\noffice_words <- office %>%\n  unnest_tokens(word, text) %>%\n  anti_join(stop_words)\nglimpse(office_words)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 125,040\nColumns: 12\n$ index            <int> 1, 1, 1, 2, 2, 3, 3, 3, 4, 4, 6, 6, 6, 6, 6, 6, 6, 6,…\n$ season           <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ episode          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ episode_name     <chr> \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\",…\n$ director         <chr> \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis…\n$ writer           <chr> \"Ricky Gervais;Stephen Merchant;Greg Daniels\", \"Ricky…\n$ character        <chr> \"Michael\", \"Michael\", \"Michael\", \"Jim\", \"Jim\", \"Micha…\n$ text_w_direction <chr> \"All right Jim. Your quarterlies look very good. How …\n$ imdb_rating      <dbl> 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6…\n$ total_votes      <int> 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706,…\n$ air_date         <fct> 2005-03-24, 2005-03-24, 2005-03-24, 2005-03-24, 2005-…\n$ word             <chr> \"jim\", \"quarterlies\", \"library\", \"told\", \"close\", \"ma…\n```\n:::\n:::\n\n\n\nWe can see that we have a new column, `word`, with one word per row. We can also see that the only words in the first line of dialogue (All right Jim. Your quarterlies look very good. How are things at the library?) that make it through the stop words filter are `jim`, `quarterlies`, and `library`. We could fiddle with the stop words list if we wanted to keep words like \"good\" or \"things,\" but I'm not too concerned about that for now.\n\nAs a first pass, let's take a look at our 20 characters with the most lines of dialogue and see what each character's most commonly-used word is:\n\n\n::: {.cell}\n\n```{.r .cell-code}\noffice_words %>%\n  filter(is.element(character, top_20_chars)) %>%\n  count(character, word, sort = TRUE) %>%\n  group_by(character) %>%\n  top_n(n = 1) %>%\n  kable(format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"condensed\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-condensed table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> character </th>\n   <th style=\"text-align:left;\"> word </th>\n   <th style=\"text-align:right;\"> n </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Michael </td>\n   <td style=\"text-align:left;\"> yeah </td>\n   <td style=\"text-align:right;\"> 563 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Dwight </td>\n   <td style=\"text-align:left;\"> michael </td>\n   <td style=\"text-align:right;\"> 280 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Jim </td>\n   <td style=\"text-align:left;\"> yeah </td>\n   <td style=\"text-align:right;\"> 274 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Pam </td>\n   <td style=\"text-align:left;\"> michael </td>\n   <td style=\"text-align:right;\"> 257 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Jan </td>\n   <td style=\"text-align:left;\"> michael </td>\n   <td style=\"text-align:right;\"> 159 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Andy </td>\n   <td style=\"text-align:left;\"> yeah </td>\n   <td style=\"text-align:right;\"> 138 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Kevin </td>\n   <td style=\"text-align:left;\"> yeah </td>\n   <td style=\"text-align:right;\"> 79 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> David </td>\n   <td style=\"text-align:left;\"> michael </td>\n   <td style=\"text-align:right;\"> 67 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Ryan </td>\n   <td style=\"text-align:left;\"> yeah </td>\n   <td style=\"text-align:right;\"> 66 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Oscar </td>\n   <td style=\"text-align:left;\"> michael </td>\n   <td style=\"text-align:right;\"> 65 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Phyllis </td>\n   <td style=\"text-align:left;\"> michael </td>\n   <td style=\"text-align:right;\"> 59 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Toby </td>\n   <td style=\"text-align:left;\"> michael </td>\n   <td style=\"text-align:right;\"> 50 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Darryl </td>\n   <td style=\"text-align:left;\"> na </td>\n   <td style=\"text-align:right;\"> 48 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Kelly </td>\n   <td style=\"text-align:left;\"> god </td>\n   <td style=\"text-align:right;\"> 44 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Angela </td>\n   <td style=\"text-align:left;\"> dwight </td>\n   <td style=\"text-align:right;\"> 40 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Holly </td>\n   <td style=\"text-align:left;\"> yeah </td>\n   <td style=\"text-align:right;\"> 39 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Erin </td>\n   <td style=\"text-align:left;\"> michael </td>\n   <td style=\"text-align:right;\"> 37 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Karen </td>\n   <td style=\"text-align:left;\"> yeah </td>\n   <td style=\"text-align:right;\"> 28 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Stanley </td>\n   <td style=\"text-align:left;\"> michael </td>\n   <td style=\"text-align:right;\"> 27 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Meredith </td>\n   <td style=\"text-align:left;\"> wait </td>\n   <td style=\"text-align:right;\"> 22 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\nSo, that's not great. We can see that our stop words didn't pick up \"yeah.\" One way around this would be to filter out additional words like \"yeah,\" \"hey,\" etc. that aren't in our stop words list. But we'll probably still leave out some common words that we might not want to show up in our exploration. A better approach is probably to use the tf-idf statistics (term frequency-inverse document frequency), which adjusts the weight a term is given in the analysis for each character by how commonly the word is used by all characters, with more common words getting lower weights. Essentially, this lets us figure out which words are important/unique to each of our characters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noffice_words %>%\n  filter(is.element(character, top_20_chars)) %>%\n  count(character, word, sort = TRUE) %>%\n  bind_tf_idf(word, character, n) %>%\n  group_by(character) %>%\n  top_n(n = 5, wt = tf_idf) %>%\n  slice(1:5) %>%\n  ungroup() %>%\n  ggplot() +\n    geom_col(aes(x = reorder_within(word, tf_idf, within = character), y = tf_idf), fill = purple) +\n    facet_wrap(~character, scales = \"free\") +\n    coord_flip() +\n    scale_x_reordered() +\n    theme_minimal() +\n    labs(\n      x = \"\",\n      y = \"\",\n      title = \"Which Words are Important to Which Characters?\"\n    ) +\n    theme(\n      axis.text.x = element_blank()\n    )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/character common words-1.png){width=672}\n:::\n:::\n\n\n\nThis looks right -- we see that \"tuna\" and \"nard\" are important to Andy, which totally makes sense. Some other gems in here are \"wuphf\" for Ryan, \"wimowheh\" for Jim, and \"awesome\" for Kevin.\n\nNext, let's take a closer look at how Michael's speech compares to some of the other main characters -- Dwight, Jim, and Pam. We'll also leave Kelly in here because I think she'll be interesting to compare to Michael.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmain_char_words <-  office_words %>%\n  filter(character %in% c(\"Michael\", \"Dwight\", \"Jim\", \"Pam\", \"Kelly\"),\n         str_detect(word, \"\\\\d+\", negate = TRUE)) %>%\n  count(character, word) %>%\n  group_by(character) %>%\n  mutate(word_prop = n/sum(n)) %>%\n  ungroup() %>%\n  select(-n) %>%\n  pivot_wider(names_from = character,\n              values_from = word_prop)\nchar_plot <- function(df, char) {\n  df %>%\n  select(word, Michael, {{char}}) %>%\n  mutate(color = log(abs(Michael-{{char}}))) %>%\n  ggplot(aes(y = Michael, x = {{char}})) +\n    geom_text(aes(label = word, color = color), check_overlap = TRUE, vjust = 1) +\n    geom_abline(color = \"grey50\", lty = 2) +\n    scale_x_log10(labels = scales::percent_format()) +\n    scale_y_log10(labels = scales::percent_format()) +\n    scale_color_distiller(\n      type = \"seq\",\n      palette = \"Purples\",\n      direction = 1\n    ) +\n    theme_minimal() +\n    theme(\n      legend.position = \"none\"\n    )\n}\nmain_char_words %>%\n  char_plot(Dwight)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/dwight words-1.png){width=672}\n:::\n:::\n\n\n\nOk, so let's walk through how to read this. For a given word, the y-axis shows how frequently Michael uses that word, and the x-axis shows how frequently Dwight uses that word. The diagonal dotted line represents equal usage -- words that appear on or close to the line are words that Michael and Dwight use about as frequently as one another. Words _above_ the line are those that Michael uses more; words _below_ the line are those that Dwight uses more. Words closer to the line will appear lighter in the graph, whereas words farther way will have more color. So, looking at the graph, we can see that Dwight and Michael both say \"hey\" pretty often and use the word more or less equally. Dwight says \"Mose\" way more often than Michael does (because it's farther from the line), whereas Michael says \"Scott\" more often than Dwight.\n\nLet's take a look at what these graphs look like for Jim and Pam\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmain_char_words %>%\n  char_plot(Jim)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/jim graph-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmain_char_words %>%\n  char_plot(Pam)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/pam graph-1.png){width=672}\n:::\n:::\n\n\n\nAand let's throw Kelly in there too because it might be interesting.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmain_char_words %>%\n  char_plot(Kelly)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/kelly plot-1.png){width=672}\n:::\n:::\n\n\n\nWhat we see here is that, at least when compared to Michael, Kelly's speech is pretty idiosyncratic -- there are lots of words (\"blah\", \"bitch\", \"god\") that she uses waaaayy more frequently than Michael does.\n\nAnd finally (for this section), I would be remiss if I made it through an analysis of how characters from _The Office_ speak without giving a \"that's what she said\" tally...\n\n\n::: {.cell}\n\n```{.r .cell-code}\noffice %>%\n  filter(str_detect(text, \"what she said\")) %>%\n  count(character) %>%\n  ggplot(aes(x = fct_reorder(character, n), y = n)) +\n    geom_col(fill = purple) +\n    labs(\n      x = \"\",\n      y = \"Count\",\n      title = \"That's What She Said!\"\n    ) +\n    coord_flip()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/thats what she said-1.png){width=672}\n:::\n:::\n\n\n\nNot at all a surprise....\n\n### Identity theft is not a joke, Jim!\n\nFinally, I want to visualize who characters talk to. To do this, I'm going to put together a network plot showing links between characters based on how frequently they interact.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(0408)\noffice_links <- office %>%\n  filter(character %in% top_20_chars) %>%\n  group_by(episode) %>%\n  mutate(to = lead(character)) %>%\n  ungroup() %>%\n  rename(from = character) %>%\n  count(from, to) %>%\n  filter(from != to,\n         !is.na(to),\n         n > 25)\noffice_verts <- office_links %>%\n  group_by(from) %>%\n  summarize(size = log(sum(n), base = 2)) %>%\n  ungroup()\nnetwork_graph <- graph_from_data_frame(office_links, vertices = office_verts)\nnetwork_graph %>%\n  ggraph(layout = \"igraph\", algorithm = \"fr\") +\n  geom_edge_link(aes(edge_alpha = n^.5), color = purple, edge_width = 1) +\n  geom_node_point(aes(size = size, color = size)) +\n  geom_node_text(aes(label = name, size = size), repel = TRUE, family = \"Garamond\", fontface = \"bold\") +\n  scale_color_distiller(\n      type = \"seq\",\n      palette = \"Purples\",\n      direction = 1\n    ) +\n  labs(\n    title = \"Who Talks to Whom in The Office?\"\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(hjust = .5)\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/network-1.png){width=672}\n:::\n:::\n\n\n\nThe network graph shows links between characters. The size and color of the node (point) associated with a person corresponds to the the total number of interactions they have, with larger and purple-r nodes representing more interactions. The color of the link between characters also corresponds to the number of interactions between two characters, with darker purple links representing more interactions and lighter links representing fewer interactions. Also, characters with more total interactions are sorted toward the center of the network, which is where we see Michael, Jim, Pam, and Dwight. Finally, interactions are only shown if characters have more than 25 total interactions (this prevents the graph from showing a jillion lines).\n\nI'm going to wrap this one up here, but later on I'll probably play around a bit with doing some statistical modeling -- predicting who is speaking, who a character is speaking to, something like that.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}