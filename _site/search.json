[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am an educator, researcher, data scientist, and writer. I am currently an Education Data Specialist for Chesterfield County Public Schools, where I use data to help guide decisions and improve students’ educational experiences. In my free time, I enjoy reading, writing, data visualization/analysis, going to the gym, and spending time with friends & family (including my pup, Nala).\nI earned my PhD in educational psychology from the VCU School of Education in July 2019, where I worked in the Motivation in Context Lab and in the Discourse and Learning Lab to study students’ motivation, behaviors, and achievement. In my dissertation, I studied how writers’ daily emotional states (think: anxiety, enjoyment, boredom) influenced their productivity as well as the day-to-day inertia of these emotional states.\nDuring grad school, I discovered a love for working with data – exploring, visualizing, modeling, and communicating. I mostly work (and blog) in R, but I’m always learning new languages and technologies. Well, as much as I can with a toddler and an infant in the house."
  },
  {
    "objectID": "about.html#projects-and-publications",
    "href": "about.html#projects-and-publications",
    "title": "About Me",
    "section": "Projects and Publications",
    "text": "Projects and Publications\nI’ve published some stuff – both academic and not – which you can find on my publications page."
  },
  {
    "objectID": "about.html#consulting-and-contracted-work",
    "href": "about.html#consulting-and-contracted-work",
    "title": "About Me",
    "section": "Consulting and Contracted Work",
    "text": "Consulting and Contracted Work\nTime permitting, I occasionally do some consulting and contract work. Although I specialize in educational research, I’m generally proficient with a wide range of statistical modeling frameworks as well as data visualization and dashboard design (via {shiny}), among other things. If you’re interested in working with me, send me an email."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "R\n\n\nTutorial\n\n\nAPI\n\n\nD&D\n\n\nEDA\n\n\n\n\nExploring monster stats in D&D 5e\n\n\n\n\n\n\nDec 28, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nTutorial\n\n\nAPI\n\n\nD&D\n\n\n\n\nWrangling JSON data from an API\n\n\n\n\n\n\nDec 18, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\ntidymodels\n\n\nstats\n\n\nsplines\n\n\n\n\nUsing splines and iteration via map() to fit and interrogate models\n\n\n\n\n\n\nNov 18, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nTutorial\n\n\nglm\n\n\nPoisson Regression\n\n\nstats\n\n\n\n\nPracticing Poisson regression using Xmen data\n\n\n\n\n\n\nMay 6, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nTutorial\n\n\nFunction Writing\n\n\n\n\nExamples and tutorial for writing rolling aggregate/window functions\n\n\n\n\n\n\nMay 6, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nRVA\n\n\nGeography\n\n\nEDA\n\n\n\n\nAnalyzing pet ownership in RVA\n\n\n\n\n\n\nApr 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nThe Office\n\n\nText Analysis\n\n\nClustering\n\n\n\n\nK means clustering with The Office dialogue\n\n\n\n\n\n\nApr 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nPuzzles\n\n\nCoding Challenge\n\n\n\n\nSolving a math puzzle and exploring the accumulate() function\n\n\n\n\n\n\nMar 31, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nThe Office\n\n\nEDA\n\n\nText Analysis\n\n\n\n\nAn initial exploration of dialogue from The Office\n\n\n\n\n\n\nMar 14, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Eric Ekholm",
    "section": "",
    "text": "I am an educator, researcher, data scientist, and writer. I am currently an Education Data Specialist for Chesterfield County Public Schools, where I use data to help guide decisions and improve students’ educational experiences."
  },
  {
    "objectID": "posts/scrantonicity-part-1/index.html",
    "href": "posts/scrantonicity-part-1/index.html",
    "title": "Scratonicity - Part 1",
    "section": "",
    "text": "Who doesn’t love The Office? I went through high school and college following the on-again off-again romance of Jim and Pam, the Icarus-esque ascendancy and fall of Ryan the Temp, and the perpetual cringey-ness of Michael Scott. And aside from that handful of people who fled the room in a cold panic at even the mention of “Scott’s Tots,” I think this was probably true for most of my generation. You’d be hard pressed to go to a Halloween party in the late aughts without seeing someone dressed in the tan-and-yellow palette of Dwight Schrute, and before the modern era of Netflix and Hulu, we regularly set aside Thursday nights to tune into NBC.\nAnd although I was a big Office fan several years ago, I haven’t thought too too much about it recently – at least until I stumbled across the release of the {schrute} package recently. {schrute} is an R package with one purpose – presenting the entire transcripts of The Office in tibble format, making the dialogue of the show much easier to analyze. I played around with the package and a quick sentiment analysis back in December when I looked at the sentiments expressed by Jim and Pam over the course of the series:\n\n\n\n\n\nThere’s a ton more we can do with the package, though, and with the transcripts available and in a clean format, plus all of the tools R has available for text analysis, I figured I’d do a mini-series of blog posts analyzing some of the data. The plan (as of now) is to start this first post with some exploratory analyses and visualizations, then move into some other types of modeling in later posts. I’ll also include all of my code throughout.\n\nAs a quick aside, a lot of the text analyses I’m going to work through in this first post come from the Text Mining with R book by Julia Silge and David Robinson. I’d strongly recommend this to anyone looking to dive into analyzing text data."
  },
  {
    "objectID": "pubs.html",
    "href": "pubs.html",
    "title": "Publications",
    "section": "",
    "text": "Peer-Reviewed Publications\nBack when I was more actively involved in academia, I published some papers, mostly related to student motivation, how people write, or statistical methodology. Publishing peer-reviewed work has been less of a priority for me lately, but I still dabble here and there.\n\nMarrs, S., Zumbrunn, S., & Ekholm, E. (2022). Measuring students’ perceptions of writing feedback. Current Research in Psychology and Behavioral Science, 3(4).\nZumbrunn, S., Ekholm, E., Broda, M., & Koenka, A. C. (2022). Trajectories of students’ writing feedback attitudes. Journal of Experimental Education. https://doi.org/10.1080/00220973.2022.2064413\nChow, J., Ekholm, E., & Bae, C. (2021). Relative contribution of verbal working memory and attention to child language. Assessment for Effective Intervention, 47(1), 3-13.\nBroda, M., Ekholm, E., & Zumbrunn, S. (2020). Assessing the predictive nature of teacher and student writing self-regulation discrepancy. Frontline Learning Research, 8(4), 52.\nGellock, J., Ekholm, E., Greenhalgh, G., LeCrom, C., Reina, C., & Kudesia, R. (2019). Women’s lacrosse players’ perceptions of teammate leadership: Examining athlete leadership behaviors, attributes, and interactions. Journal of Athlete Development and Experience, 1(2), 2. https://scholarworks.bgsu.edu/jade/vol1/iss2/2.\nZumbrunn, S., Marrs, S., Broda, M., Ekholm, E., Jackson, L., & DeBusk-Lane, M. (2019). Toward a more complete understanding of writing enjoyment: A mixed methods study of elementary students. AERA Open, 5(2), https://doi.org/10.1177/2332858419850792.\nBroda, M., Tucker, S., Ekholm, E., Johnson, T., & Liang, Q. (2019). Small fingers, big data: Preschoolers’ subitizing speed and accuracy during interactions with multi-touch technology. The Journal of Educational Research, 112(2), 211-222. https://doi.org/10.1080/00220671.2018.1486281.\nEkholm, E. & Chow. J. C. (2018). Addressing publication bias in educational psychology. Translational Issues in Psychological Science, 4(4), 425-439. http://dx.doi.org/10.1037/tps0000181.\nBroda, M., Ekholm, E., Schneider, B., & Hutton, A. (2018). Teachers’ social networks, college-going practices, and the diffusion of a school-based reform initiative. SAGE Open. https://doi.org/10.1177/2158244018817397.\nChow, J.C., Ekholm, E., & Coleman, H. (2018). Does oral language underpin the development of later behavior problems? A longitudinal meta-analysis. School Psychology Quarterly, 33(3), 337-349. http://dx.doi.org/10.1037/spq0000255.\nChow, J.C., & Ekholm, E. (2018). Do published studies yield larger effect sizes than unpublished studies in education and special education? A meta-review. Educational Psychology Review, 30(3), 727-744. https://doi.org/10.1007/s10648-018-9437-7.\nChow, J. C., & Ekholm, E. (2018). Language domains differentially predict mathematics performance in young children. Early Childhood Research Quarterly, 46, 179-186. https://doi.org/10.1016/j.ecresq.2018.02.011\nEkholm, E., Zumbrunn, S. & DeBusk-Lane, M. (2018). Clarifying an elusive construct: A systematic review of writing attitudes. Educational Psychology Review, 30(3), 827-856. https://doi.org/10.1007/s10648-017-9423-5.\nEkholm, E. (2017). Ethical concerns of using texts involving death in the English classroom. English Journal, 107(2), 25-30.\nZumbrunn, S., Ekholm, E., Stringer, J. K., McKnight, K., & DeBusk-Lane, M. (2017). Student experiences with writing: Taking the temperature of the classroom. The Reading Teacher, 70(6), 667-677. https://doi.org/10.1002/trtr.1574.\nEkholm, E., Zumbrunn, S., & Conklin, S. (2015). The relation of college student self-efficacy toward writing and writing self-regulation aptitude: writing feedback perceptions as a mediating variable. Teaching in Higher Education, 20(2), 197-207. https://doi.org/10.1080/13562517.2014.974026.\n\n\n\n\nOther Publications\n\nFiction and Humor\n\nEkholm, E. “If You Take My Survey, I’ll Let You Watch Me Cut My Finger Off.” McSweeney’s Internet Tendency (June 7, 2016).\nEkholm, E. “Call for Submissions for APA’s Annual Conference, Sponsored by Shark Week.” McSweeney’s Internet Tendency (August 11, 2014).\n\n\n\nBook Reviews\n\nEkholm, E. (September, 2016). A review of Kelly Gallagher’s In the Best Interests of Students.\n\n\n\nOther\n\nGuest blog post, AERA Motivation SIG Blog, Ekholm, E. (2018). “What Motivation Researchers Can Learn from Professional Sports Teams.” November 6, 2018.\n\n\n\n\n\nConference Presentations & Invited Talks\nOnly conference presentations since 2018 included\n\nEkholm, E. (May, 2022). Data Viz Tips & Tricks. Invited talk presented to the Educational Research & Evaluation Network (EREN).\nEkholme, E., & Fox, P. (October, 2021). Using machine learning to support student attendance practices. Session presented at the annual Metropolitan Education Research Consortium (MERC) conference, Richmond, VA. Video available here (~27:30 timestamp).\nEkholm, E., & Zumbrun, S. (April, 2020). Investigating relations between writers’ emotional experiences and attention regulation: A daily diary study. Paper presented at the American Educational Research Association (AERA) Annual Meeting, San Francisco, CA.\nEkholm, E., Zumbrunn, S., Broda, M., & Luther, T. C. (April, 2019). The development of student writing feedback attitudes in grades 3-7: A latent growth analysis. Poster presented at the American Educational Research Association (AERA) Annual Meeting, Toronto, Canada.\nZumbrunn, S., Broda, M., Marrs, S., & Ekholm, E. (April, 2019). The complexities and challenges of understanding individual differences and the development of multiple dimensions of writing self-efficacy. Poster presented at the American Educational Research Association (AERA) Annual Meeting, Toronto, Canada.\nBroda, M., Ekholm, E., Hutton, C., & Schneider, B. (April, 2019). Teachers’ social networks, college-going practices, and the diffusion of a school-based reform initiative. Paper presented at the American Educational Research Association (AERA) Annual Meeting, Toronto, Canada.\nZumbrunn, S., Broda, M., Ekholm, E., & Luther, T. C. (April, 2019). RoboCogger: Using mobile technology to assess and increase student writing metacognition, motivation, and performance. Paper presented at the American Educational Research Association (AERA) Annual Meeting, Toronto, Canada.\nBroda, M., Ekholm, E., & Zumbrunn, S. (October, 2018). Are we grading the “write” stuff? The relationship between teachers’ expectations, students’ self-regulation, and writing achievement. Paper presented at the CREATE Conference, Williamsburg, VA.\nZumbrunn, S., Marrs, S., Malmberg, L., Ekholm, E., Broda, M., & DeBusk-Lane, M. (August, 2018). Individual differences and the development of multiple dimensions of writing self-efficacy. Paper presented at the International Conference of the EARLI Special Interest Group on Writing, Antwerp, Belgium.\nEkholm, E., Zumbrunn, S., Hope, S., & Stim, H. (April, 2018). Teachers’ writing beliefs and instructional practices: A mixed-methods study. Poster presented at the American Educational Research Association (AERA) Annual Meeting, New York City, NY.\nMarrs, S., Ekholm, E., & Zumbrunn, S. (April, 2018). Exploring profiles of student perceptions of writing feedback. Roundtable presented at the American Educational Research Association (AERA) Annual Meeting, New York City, NY.\nTucker, S., Broda, M., Ekholm, E., Johnson, T., & Liang, Q. (April, 2018). Preschoolers’ subitizing speed and accuracy during interactions with multi-touch technology. Roundtable presented at the American Educational Research Association (AERA) Annual Meeting, New York City, NY.\nChow, J.C. & Ekholm, E. (February, 2018). Estimating the magnitude of the difference between published and unpublished studies in special education: A metareview. Poster presented at the Council for Exceptional Children Annual Convention, Tampa, FL."
  },
  {
    "objectID": "posts/scrantonicity-part-1/index.html#setup",
    "href": "posts/scrantonicity-part-1/index.html#setup",
    "title": "Scratonicity - Part 1",
    "section": "Setup",
    "text": "Setup\nFirst, let’s read in the data. I’m also going to limit the data to the first seven seasons, which spans the “Michael Scott” era. Not only because these are the best seasons (which they undoubtedly are), but also because doing so eliminates a major confounding factor (i.e. Steve Carell leaving the show) from the analysis.\n\noffice <- theoffice %>%\n  filter(as.numeric(season) <= 7)\n\nglimpse(office)\n\nRows: 41,348\nColumns: 12\n$ index            <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16…\n$ season           <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ episode          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ episode_name     <chr> \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\",…\n$ director         <chr> \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis…\n$ writer           <chr> \"Ricky Gervais;Stephen Merchant;Greg Daniels\", \"Ricky…\n$ character        <chr> \"Michael\", \"Jim\", \"Michael\", \"Jim\", \"Michael\", \"Micha…\n$ text             <chr> \"All right Jim. Your quarterlies look very good. How …\n$ text_w_direction <chr> \"All right Jim. Your quarterlies look very good. How …\n$ imdb_rating      <dbl> 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6…\n$ total_votes      <int> 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706,…\n$ air_date         <fct> 2005-03-24, 2005-03-24, 2005-03-24, 2005-03-24, 2005-…\n\n\nJust to check that the data we have matches what we’re expecting, let’s take a look at which seasons we have, plus how many episodes we have per season.\n\noffice %>%\n  distinct(season, episode) %>%\n  count(season, name = \"num_episodes\")\n\n# A tibble: 7 × 2\n  season num_episodes\n   <int>        <int>\n1      1            6\n2      2           22\n3      3           23\n4      4           14\n5      5           26\n6      6           24\n7      7           24\n\n\nThis generally matches what Wikipedia is telling me once we account for two-part episodes, and we can see that we only have the first seven seasons.\n\nMe think, why waste time say lot word, when few word do trick\nA few questions we can ask here involve how much/how often different characters speak. Probably the most basic question is: who has the most lines?\n\ntop_20_chars <- office %>%\n  count(character, sort = TRUE) %>%\n  top_n(20) %>%\n  pull(character)\noffice %>%\n  filter(is.element(character, top_20_chars)) %>%\n  count(character, sort = TRUE) %>%\n  ggplot(aes(x = fct_reorder(character, n), y = n)) +\n  geom_col(fill = purple) +\n  labs(\n    x = \"\",\n    y = \"Number of Lines\",\n    title = \"Who Has the Most Lines?\"\n  ) +\n  coord_flip()\n\n\n\n\nIt’s not surprising to me that Michael has the most lines, but the magnitude of the difference between him and Dwight is a bit surprising.\nWhat if we look at the number of lines per season?\n\noffice %>%\n  filter(is.element(character, top_20_chars)) %>%\n  count(character, season, sort = TRUE) %>%\n  ggplot(aes(x = as.numeric(season), y = n, color = character)) +\n    geom_line() +\n    geom_point()\n\n\n\n\nThis isn’t terribly informative – let’s go back to our bar graph.\n\noffice %>%\n  filter(is.element(character, top_20_chars)) %>%\n  count(character, season, sort = TRUE) %>%\n  group_by(season) %>%\n  top_n(n = 5) %>%\n  ungroup() %>%\n  ggplot(aes(x = fct_reorder(character, n), y = n)) +\n    geom_col(fill = purple) +\n    coord_flip() +\n    facet_wrap(~season, scales = \"free\") +\n    labs(\n      title = \"Number of Lines by Season\",\n      x = \"\",\n      y = \"\"\n    ) +\n    theme_minimal()\n\n\n\n\nAgain, not surprising that Michael has the most lines across all seasons. Dwight, Jim, and Pam are always the next three, but the orders change a bit between seasons. The fifth spot is where we see some movement, with Oscar and Jan sneaking in before Andy joins the show in Season 3. And check out Ryan in S4!\n\n\nSometimes I’ll start a sentence and I don’t even know where it’s going\nSo, above, we just looked at the number of lines each character had. Another option is to do some analyses at the word level. For instance, we can look at patterns of word usage for individual characters, between characters, and over time.\nTo start with this, I’m going to restructure the data so we have one word per row in our tibble. I’m also going to remove “stop words” (e.g. “a,” “the,” “at”), since these will show up a lot but (for our purposes) aren’t actually all that meaningful:\n\noffice_words <- office %>%\n  unnest_tokens(word, text) %>%\n  anti_join(stop_words)\nglimpse(office_words)\n\nRows: 125,040\nColumns: 12\n$ index            <int> 1, 1, 1, 2, 2, 3, 3, 3, 4, 4, 6, 6, 6, 6, 6, 6, 6, 6,…\n$ season           <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ episode          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ episode_name     <chr> \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\",…\n$ director         <chr> \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis…\n$ writer           <chr> \"Ricky Gervais;Stephen Merchant;Greg Daniels\", \"Ricky…\n$ character        <chr> \"Michael\", \"Michael\", \"Michael\", \"Jim\", \"Jim\", \"Micha…\n$ text_w_direction <chr> \"All right Jim. Your quarterlies look very good. How …\n$ imdb_rating      <dbl> 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6…\n$ total_votes      <int> 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706,…\n$ air_date         <fct> 2005-03-24, 2005-03-24, 2005-03-24, 2005-03-24, 2005-…\n$ word             <chr> \"jim\", \"quarterlies\", \"library\", \"told\", \"close\", \"ma…\n\n\nWe can see that we have a new column, word, with one word per row. We can also see that the only words in the first line of dialogue (All right Jim. Your quarterlies look very good. How are things at the library?) that make it through the stop words filter are jim, quarterlies, and library. We could fiddle with the stop words list if we wanted to keep words like “good” or “things,” but I’m not too concerned about that for now.\nAs a first pass, let’s take a look at our 20 characters with the most lines of dialogue and see what each character’s most commonly-used word is:\n\noffice_words %>%\n  filter(is.element(character, top_20_chars)) %>%\n  count(character, word, sort = TRUE) %>%\n  group_by(character) %>%\n  top_n(n = 1) %>%\n  kable(format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"condensed\", \"hover\"))\n\n\n\n \n  \n    character \n    word \n    n \n  \n \n\n  \n    Michael \n    yeah \n    563 \n  \n  \n    Dwight \n    michael \n    280 \n  \n  \n    Jim \n    yeah \n    274 \n  \n  \n    Pam \n    michael \n    257 \n  \n  \n    Jan \n    michael \n    159 \n  \n  \n    Andy \n    yeah \n    138 \n  \n  \n    Kevin \n    yeah \n    79 \n  \n  \n    David \n    michael \n    67 \n  \n  \n    Ryan \n    yeah \n    66 \n  \n  \n    Oscar \n    michael \n    65 \n  \n  \n    Phyllis \n    michael \n    59 \n  \n  \n    Toby \n    michael \n    50 \n  \n  \n    Darryl \n    na \n    48 \n  \n  \n    Kelly \n    god \n    44 \n  \n  \n    Angela \n    dwight \n    40 \n  \n  \n    Holly \n    yeah \n    39 \n  \n  \n    Erin \n    michael \n    37 \n  \n  \n    Karen \n    yeah \n    28 \n  \n  \n    Stanley \n    michael \n    27 \n  \n  \n    Meredith \n    wait \n    22 \n  \n\n\n\n\n\nSo, that’s not great. We can see that our stop words didn’t pick up “yeah.” One way around this would be to filter out additional words like “yeah,” “hey,” etc. that aren’t in our stop words list. But we’ll probably still leave out some common words that we might not want to show up in our exploration. A better approach is probably to use the tf-idf statistics (term frequency-inverse document frequency), which adjusts the weight a term is given in the analysis for each character by how commonly the word is used by all characters, with more common words getting lower weights. Essentially, this lets us figure out which words are important/unique to each of our characters.\n\noffice_words %>%\n  filter(is.element(character, top_20_chars)) %>%\n  count(character, word, sort = TRUE) %>%\n  bind_tf_idf(word, character, n) %>%\n  group_by(character) %>%\n  top_n(n = 5, wt = tf_idf) %>%\n  slice(1:5) %>%\n  ungroup() %>%\n  ggplot() +\n    geom_col(aes(x = reorder_within(word, tf_idf, within = character), y = tf_idf), fill = purple) +\n    facet_wrap(~character, scales = \"free\") +\n    coord_flip() +\n    scale_x_reordered() +\n    theme_minimal() +\n    labs(\n      x = \"\",\n      y = \"\",\n      title = \"Which Words are Important to Which Characters?\"\n    ) +\n    theme(\n      axis.text.x = element_blank()\n    )\n\n\n\n\nThis looks right – we see that “tuna” and “nard” are important to Andy, which totally makes sense. Some other gems in here are “wuphf” for Ryan, “wimowheh” for Jim, and “awesome” for Kevin.\nNext, let’s take a closer look at how Michael’s speech compares to some of the other main characters – Dwight, Jim, and Pam. We’ll also leave Kelly in here because I think she’ll be interesting to compare to Michael.\n\nmain_char_words <-  office_words %>%\n  filter(character %in% c(\"Michael\", \"Dwight\", \"Jim\", \"Pam\", \"Kelly\"),\n         str_detect(word, \"\\\\d+\", negate = TRUE)) %>%\n  count(character, word) %>%\n  group_by(character) %>%\n  mutate(word_prop = n/sum(n)) %>%\n  ungroup() %>%\n  select(-n) %>%\n  pivot_wider(names_from = character,\n              values_from = word_prop)\nchar_plot <- function(df, char) {\n  df %>%\n  select(word, Michael, {{char}}) %>%\n  mutate(color = log(abs(Michael-{{char}}))) %>%\n  ggplot(aes(y = Michael, x = {{char}})) +\n    geom_text(aes(label = word, color = color), check_overlap = TRUE, vjust = 1) +\n    geom_abline(color = \"grey50\", lty = 2) +\n    scale_x_log10(labels = scales::percent_format()) +\n    scale_y_log10(labels = scales::percent_format()) +\n    scale_color_distiller(\n      type = \"seq\",\n      palette = \"Purples\",\n      direction = 1\n    ) +\n    theme_minimal() +\n    theme(\n      legend.position = \"none\"\n    )\n}\nmain_char_words %>%\n  char_plot(Dwight)\n\n\n\n\nOk, so let’s walk through how to read this. For a given word, the y-axis shows how frequently Michael uses that word, and the x-axis shows how frequently Dwight uses that word. The diagonal dotted line represents equal usage – words that appear on or close to the line are words that Michael and Dwight use about as frequently as one another. Words above the line are those that Michael uses more; words below the line are those that Dwight uses more. Words closer to the line will appear lighter in the graph, whereas words farther way will have more color. So, looking at the graph, we can see that Dwight and Michael both say “hey” pretty often and use the word more or less equally. Dwight says “Mose” way more often than Michael does (because it’s farther from the line), whereas Michael says “Scott” more often than Dwight.\nLet’s take a look at what these graphs look like for Jim and Pam\n\nmain_char_words %>%\n  char_plot(Jim)\n\n\n\n\n\nmain_char_words %>%\n  char_plot(Pam)\n\n\n\n\nAand let’s throw Kelly in there too because it might be interesting.\n\nmain_char_words %>%\n  char_plot(Kelly)\n\n\n\n\nWhat we see here is that, at least when compared to Michael, Kelly’s speech is pretty idiosyncratic – there are lots of words (“blah”, “bitch”, “god”) that she uses waaaayy more frequently than Michael does.\nAnd finally (for this section), I would be remiss if I made it through an analysis of how characters from The Office speak without giving a “that’s what she said” tally…\n\noffice %>%\n  filter(str_detect(text, \"what she said\")) %>%\n  count(character) %>%\n  ggplot(aes(x = fct_reorder(character, n), y = n)) +\n    geom_col(fill = purple) +\n    labs(\n      x = \"\",\n      y = \"Count\",\n      title = \"That's What She Said!\"\n    ) +\n    coord_flip()\n\n\n\n\nNot at all a surprise….\n\n\nIdentity theft is not a joke, Jim!\nFinally, I want to visualize who characters talk to. To do this, I’m going to put together a network plot showing links between characters based on how frequently they interact.\n\nset.seed(0408)\noffice_links <- office %>%\n  filter(character %in% top_20_chars) %>%\n  group_by(episode) %>%\n  mutate(to = lead(character)) %>%\n  ungroup() %>%\n  rename(from = character) %>%\n  count(from, to) %>%\n  filter(from != to,\n         !is.na(to),\n         n > 25)\noffice_verts <- office_links %>%\n  group_by(from) %>%\n  summarize(size = log(sum(n), base = 2)) %>%\n  ungroup()\nnetwork_graph <- graph_from_data_frame(office_links, vertices = office_verts)\nnetwork_graph %>%\n  ggraph(layout = \"igraph\", algorithm = \"fr\") +\n  geom_edge_link(aes(edge_alpha = n^.5), color = purple, edge_width = 1) +\n  geom_node_point(aes(size = size, color = size)) +\n  geom_node_text(aes(label = name, size = size), repel = TRUE, family = \"Garamond\", fontface = \"bold\") +\n  scale_color_distiller(\n      type = \"seq\",\n      palette = \"Purples\",\n      direction = 1\n    ) +\n  labs(\n    title = \"Who Talks to Whom in The Office?\"\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(hjust = .5)\n  )\n\n\n\n\nThe network graph shows links between characters. The size and color of the node (point) associated with a person corresponds to the the total number of interactions they have, with larger and purple-r nodes representing more interactions. The color of the link between characters also corresponds to the number of interactions between two characters, with darker purple links representing more interactions and lighter links representing fewer interactions. Also, characters with more total interactions are sorted toward the center of the network, which is where we see Michael, Jim, Pam, and Dwight. Finally, interactions are only shown if characters have more than 25 total interactions (this prevents the graph from showing a jillion lines).\nI’m going to wrap this one up here, but later on I’ll probably play around a bit with doing some statistical modeling – predicting who is speaking, who a character is speaking to, something like that."
  },
  {
    "objectID": "posts/riddler-express-march-20-2020/index.html",
    "href": "posts/riddler-express-march-20-2020/index.html",
    "title": "Riddler Express - March 20, 2020",
    "section": "",
    "text": "One of my personal goals for 2020 is to improve my proficiency doing data-y things – mostly using R, but potentially other software as well. Typically, I’ve been using data from the #TidyTuesday project to practice data visualization and data from Kaggle, personal research projects, and other potentially interesting datasets to work on statistical modeling. I recently discovered The Riddler series – a weekly math/logic puzzle – that seems to be a good medium for brushing up on other skills (e.g. certain types of math and programming) that may not come up as often when I do visualizations or statistics."
  },
  {
    "objectID": "posts/riddler-express-march-20-2020/index.html#the-problem",
    "href": "posts/riddler-express-march-20-2020/index.html#the-problem",
    "title": "Riddler Express - March 20, 2020",
    "section": "The Problem",
    "text": "The Problem\nAnyway, this post solves the Riddler Express puzzle from March 20, 2020. The problem is this:\n\nA manager is trying to produce sales of his company’s widget, so he instructs his team to hold a sale every morning, lowering the price of the widget by 10 percent. However, he gives very specific instructions as to what should happen in the afternoon: Increase the price by 10 percent from the sale price, with the (incorrect) idea that it would return it to the original price. The team follows his instructions quite literally, lowering and then raising the price by 10 percent every day.\n\n\nAfter N days, the manager walks through the store in the evening, horrified to see that the widgets are marked more than 50 percent off of their original price. What is the smallest possible value of N?\n\nI’ll walk through a couple of ways to solve this – first, I’ll solve it algebraically, and next, I’ll solve it by “brute force” using the accumulate() function from the {purrr} package."
  },
  {
    "objectID": "posts/riddler-express-march-20-2020/index.html#solving-algebraically",
    "href": "posts/riddler-express-march-20-2020/index.html#solving-algebraically",
    "title": "Riddler Express - March 20, 2020",
    "section": "Solving Algebraically",
    "text": "Solving Algebraically\nSo, the first thing that strikes me when reading this is that it’s essentially a compounding interest problem, except in this case the interest is negative. That is, rather than gaining value exponentially over the number of compounding periods, we’re losing value exponentially. The formula for calculating compound interest is:\n\\[A = P(1 + r)^n\\]\nwhere A equals the final amount, P equals the principal (our initial value), r equals the interest rate, and n equals the number of compounding periods (the number of days in this case). We’re interested in solving for the value of n where our final amount, A, is less than .5. Our principal amount, P, in this case, is 1 (i.e. 100% of the value). So, our equation looks like this:\n\\[.5 > ((1-1*.1)*1.1)^n\\]\nThe internal logic here is that we subtract 10% from our initial value (1-1*.1) to represent the 10% decrease in price in the morning, then multiply this resulting value by 1.1 to represent the subsequent 10& increase in price in the afternoon. This simplifies to:\n\\[.5 > .99^n\\]\nFrom here, we can just solve by taking the log of each side and then dividing, which get us our answer\n\nn <- log(.5)/log(.99)\nn\n\n[1] 68.96756\n\n\nRounding this up (since we’re dealing in full days), we can say that after 69 days, the price of the widget will be below 50% of its initial price."
  },
  {
    "objectID": "posts/riddler-express-march-20-2020/index.html#solving-using-accumulate",
    "href": "posts/riddler-express-march-20-2020/index.html#solving-using-accumulate",
    "title": "Riddler Express - March 20, 2020",
    "section": "Solving using accumulate()",
    "text": "Solving using accumulate()\nWe can also solve this problem using the accumulate() function from the {purrr} package, which is part of the {tidyverse}. Essentially, accumulate() will take a function, evaluate it, and then pass the result of the evaluation back into the function, evaluate it again, pass the new result back into the function, etc. This makes it useful for solving problems like this one, where the end price of the widget on the previous day is the starting price of the widget on the current day.\nFirst, let’s load our packages. For this, we’ll just use {tidyverse}\n\nlibrary(tidyverse)\n\nNext, let’s set up a function that, if we give it the price of the widget at the beginning of the day, will calculate the price of the widget at the end of the day.\n\ndiscount_func <- function(x) {\n  (x-x*.1)*1.1\n}\n\nAnd then let’s test this function manually a few times.\n\ndiscount_func(1)\n\n[1] 0.99\n\ndiscount_func(.99)\n\n[1] 0.9801\n\ndiscount_func(.9801)\n\n[1] 0.970299\n\n\nNow, we can use accumulate() to automate what we just did manually. The first argument in accumulate() is, in this case, each day that we want to pass into the function. In the code below, I’m testing this for days 0-3 (but coded as 1-4 because we want the start value to be 1). The second argument is the function we just wrote.\n\naccumulate(1:4, ~discount_func(.))\n\n[1] 1.000000 0.990000 0.980100 0.970299\n\n\nAnd we can see that the values returned match our manual tests above, which is good!\nNow, we can use accumulate() to make a table with the end price of the widget each day. Note that because we want to start the widget price at 1, our first “day” in the table is day 0, which represents the beginning price of the widget on day 1.\n\ndays_tbl <- tibble(\n  day = c(0:1000),\n  end_price = accumulate(c(1:1001), ~discount_func(.))\n)\nhead(days_tbl)\n\n# A tibble: 6 × 2\n    day end_price\n  <int>     <dbl>\n1     0     1    \n2     1     0.99 \n3     2     0.980\n4     3     0.970\n5     4     0.961\n6     5     0.951\n\n\nAnd then we can plot the end price over time. I’ve added a little bit of transparency to each point so we can more easily see the clustering/overlap.\n\nggplot(days_tbl, aes(x = day, y = end_price)) +\n  geom_point(alpha = .3) +\n  theme_minimal() +\n  labs(\n    title = \"End Price of Widget over Time\"\n  )\n\n\n\n\nFinally, we can find the day where the end price is below .5 by filtering our table to only those where the price is less than .5 and then returning the first row.\n\ndays_tbl %>%\n  filter(end_price <= .5) %>%\n  slice(1)\n\n# A tibble: 1 × 2\n    day end_price\n  <int>     <dbl>\n1    69     0.500\n\n\nAnd we can see that this matches our algebraic result – great success!"
  },
  {
    "objectID": "posts/rva-pets/index.html",
    "href": "posts/rva-pets/index.html",
    "title": "RVA Pets",
    "section": "",
    "text": "First, let’s load our packages and set our plot themes/colors\n\nlibrary(tidyverse)\nlibrary(osmdata)\nlibrary(sf)\nlibrary(janitor)\nlibrary(hrbrthemes)\nlibrary(wesanderson)\nlibrary(tidytext)\nlibrary(kableExtra)\nlibrary(ggtext)\ntheme_set(theme_ipsum())\npal <- wes_palette(\"Zissou1\")\ncolors <- c(\"Dog\" = pal[1], \"Cat\" = pal[3])\n\nNext, we’ll read in the data and clean it up a little bit. In this dataset, each row represents a licensed pet in Richmond, Virginia. The dataset includes animal type (dog, cat, puppy, kitten) and the address of the owners. Whoever set up the data was also nice enough to include longitude and latitude for each address in the dataset, which means I don’t need to go out and get it. For our purposes here, I’m going to lump puppies in with dogs and kittens in with cats. I’m also going to extract the “location” column into a few separate columns. Let’s take a look at the first few entries.\n\npets_raw <- read_csv(here::here(\"data/rva_pets_2019.csv\"))\npets_clean <- pets_raw %>%\n  clean_names() %>%\n  extract(col = location_1,\n          into = c(\"address\", \"zip\", \"lat\", \"long\"),\n          regex = \"(.*)\\n.*(\\\\d{5})\\n\\\\((.*), (.*)\\\\)\") %>%\n  mutate(animal_type = str_replace_all(animal_type, c(\"Puppy\" = \"Dog\", \"Kitten\" = \"Cat\")))\nhead(pets_clean) %>%\n  kable(format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n \n  \n    animal_type \n    animal_name \n    address \n    zip \n    lat \n    long \n    load_date \n  \n \n\n  \n    Cat \n    Molly \n    301 Virginia Street APT 1008 \n    23219 \n    37.53294 \n    -77.433825 \n    20180627 \n  \n  \n    Dog \n    Sam \n    1407 Wilmington Avenue \n    23227 \n    37.58294 \n    -77.455213 \n    20180627 \n  \n  \n    Cat \n    Taffy \n    114 N Harvie Street \n    23220 \n    37.548414 \n    -77.45745 \n    20180627 \n  \n  \n    Dog \n    Jackson \n    4804 Riverside Drive \n    23225 \n    37.527326 \n    -77.483249 \n    20180627 \n  \n  \n    Dog \n    Cirrus \n    3107 E Marshall Street \n    23223 \n    37.52904 \n    -77.412272 \n    20180627 \n  \n  \n    Dog \n    Henri \n    1900 Maple Shade Lane \n    23227 \n    37.581979 \n    -77.466207 \n    20180627 \n  \n\n\n\n\n\nOk, now that our data is set up, let’s see if there are more cats or dogs in the city.\n\npets_clean %>%\n  count(animal_type) %>%\n  ggplot(aes(x = n, y = animal_type)) +\n  geom_col(color = pal[1], fill = pal[1]) +\n  geom_text(aes(x = n-50, label = n), hjust = 1, color = \"white\", fontface = \"bold\") +\n  labs(\n    title = \"Number of Cats vs Dogs\"\n  )\n\n\n\n\nAlright, so, lots more dogs. Like almost 4 to 1 dogs to cats. Which is something I can get behind. I’m a firm believer in the fact that dogs are wayyy better than cats.\nI’m also interested in the most common names for pets in RVA.\n\npets_clean %>%\n  group_by(animal_type) %>%\n  count(animal_name, sort = TRUE) %>%\n  slice(1:15) %>%\n  ungroup() %>%\n  ggplot(aes(x = n, y = reorder_within(animal_name, n, animal_type))) +\n    geom_col(color = pal[1], fill = pal[1]) +\n    geom_text(aes(x = if_else(animal_type == \"Cat\", n - .25, n - 1), label = n), hjust = 1, color = \"white\", fontface = \"bold\") +\n    facet_wrap(~animal_type, scales = \"free\") +\n    scale_y_reordered() +\n    labs(\n      title = \"Top Pet Names\",\n      y = NULL\n    )\n\n\n\n\nThese seem pretty standard to me, and unfortunately, nothing is screaming “RVA” here. No “Bagels,” no “Gwars,” etc.\nI also pulled out zip codes into their own column earlier, so we can take a look at which zip codes have the most dogs and cats.\n\npets_clean %>%\n  filter(!is.na(zip)) %>%\n  group_by(zip) %>%\n  count(animal_type, sort = TRUE)%>%\n  ungroup() %>%\n  group_by(animal_type) %>%\n  top_n(n = 10) %>%\n  ungroup() %>%\n  ggplot(aes(x = n, y = reorder_within(zip, n, animal_type))) +\n    geom_col(color = pal[1], fill = pal[1]) +\n    geom_text(aes(x = if_else(animal_type == \"Cat\", n - 1, n - 4), label = n), hjust = 1, color = \"white\", fontface = \"bold\") +\n    facet_wrap(~animal_type, scales = \"free\") +\n    scale_y_reordered() +\n    labs(\n      title = \"Number of Pets by Zipcode\",\n      y = NULL\n    )\n\n\n\n\nAlright, so most of the pets here live in Forest Hill/generally south of the river in 23225, and another big chunk live in 23220, which covers a few neighborhoods & includes The Fan, which is probably where most of the pet action is.\nAnd finally, since we have the latitude and longitude, I can put together a streetmap of the city showing where all of these little critters live. To do this, I’m going to grab some shape files through the OpenStreetMaps API and plot the pet datapoints on top of those.\n\npets_map <- st_as_sf(pets_clean %>%\n                       filter(!is.na(long)), coords = c(\"long\", \"lat\"),\n                     crs = 4326)\n\nget_rva_maps <- function(key, value) {\n  getbb(\"Richmond Virginia United States\") %>%\n    opq() %>%\n    add_osm_feature(key = key,\n                    value = value) %>%\n    osmdata_sf()\n}\n\nrva_streets <- get_rva_maps(key = \"highway\", value = c(\"motorway\", \"primary\", \"secondary\", \"tertiary\"))\n\nsmall_streets <- get_rva_maps(key = \"highway\", value = c(\"residential\", \"living_street\",\n                                                         \"unclassified\",\n                                                         \"service\", \"footway\", \"cycleway\"))\n\nriver <- get_rva_maps(key = \"waterway\", value = \"river\")\n\ndf <- tibble(\n  type = c(\"big_streets\", \"small_streets\", \"river\"),\n  lines = map(\n    .x = lst(rva_streets, small_streets, river),\n    .f = ~pluck(., \"osm_lines\")\n  )\n)\n\ncoords <- pluck(rva_streets, \"bbox\")\n\nannotations <- tibble(\n  label = c(\"<span style='color:#FFFFFF'><span style='color:#EBCC2A'>**Cats**</span> and <span style='color:#3B9AB2'>**Dogs**</span> in RVA</span>\"),\n  x = c(-77.555),\n  y = c(37.605),\n  hjust = c(0)\n)\n\nrva_pets <- ggplot() +\n  geom_sf(data = df$lines[[1]],\n          inherit.aes = FALSE,\n          size = .3,\n          alpha = .8, \n          color = \"white\") +\n  geom_sf(data = pets_map, aes(color = animal_type), alpha = .6, size = .75) +\n  geom_richtext(data = annotations, aes(x = x, y = y, label = label, hjust = hjust), fill = NA, label.color = NA, \n                label.padding = grid::unit(rep(0, 4), \"pt\"), size = 11) + \n  coord_sf(\n    xlim = c(-77.55, -77.4),\n    ylim = c(37.5, 37.61),\n    expand = TRUE\n  ) +\n  theme_void() +\n  scale_color_manual(\n    values = colors\n  ) +\n  theme(\n    legend.position = \"none\",\n    plot.background = element_rect(fill = \"grey10\"),\n    panel.background = element_rect(fill = \"grey10\"),\n    text = element_markdown()\n  )\n\n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-nc/4.0/CitationBibTeX citation:@online{ekholm2020,\n  author = {Eric Ekholm},\n  title = {RVA {Pets}},\n  date = {2020-04-23},\n  url = {https://www.ericekholm.com/posts/rva-pets},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEric Ekholm. 2020. “RVA Pets.” April 23, 2020. https://www.ericekholm.com/posts/rva-pets."
  },
  {
    "objectID": "posts/scrantonicity-part-2/index.html",
    "href": "posts/scrantonicity-part-2/index.html",
    "title": "Scrantonicity - Part 2",
    "section": "",
    "text": "A few weeks ago, I did some exploratory analyses of dialogue from The Office. That blog could easily have been a lot longer than it was, and so instead of writing some gigantic post that would have taken 30 minutes+ to read, I decided to separate it out into several different blog posts. And so here’s volume 2.\nIn this post, I want to try using k-means clustering to identify patterns in who talks to whom in different episodes.\nOnce again, huge thanks to Brad Lindblad, the creator of the {schrute} package for R, which makes the dialogue from The Office easy to work with.\n\n\nAs in the previous blog, I’ll be using the {schrute} package to get the transcripts from the show, and I’m going to limit the dialogue to the first 7 seasons of the show, which is when Michael Scott was around. I’ll also use a handful of other packages for data cleaning, analysis, and visualization. Let’s load all of this in and do some general setup.\n\nset.seed(0408)\n\nlibrary(schrute) #office dialogue\nlibrary(tidyverse) #data wrangling tools\nlibrary(broom) #tidying models\nlibrary(tidytext) #tools for working with text data\nlibrary(knitr) #markdown functionality\nlibrary(kableExtra) #styling for tables\nlibrary(hrbrthemes) #ggplot themes\n\ntheme_set(theme_ipsum())\n\noffice <- theoffice %>%\n  filter(as.numeric(season) <= 7)"
  },
  {
    "objectID": "posts/scrantonicity-part-2/index.html#im-not-superstitious-but-i-am-a-little-stitious.",
    "href": "posts/scrantonicity-part-2/index.html#im-not-superstitious-but-i-am-a-little-stitious.",
    "title": "Scrantonicity - Part 2",
    "section": "I’m not superstitious, but I am a little stitious.",
    "text": "I’m not superstitious, but I am a little stitious.\nNow that we have our data read in and our packages loaded, let’s start with the cluster analysis. The goal here is going to be to figure out if there are certain “types” (clusters, groups, whatever you want to call them) of episodes. There are several frameworks we could use to go about doing this. One approach would be a mixture modeling approach (e.g. latent profile analysis, latent class analysis). I’m not doing that here because I want each episode to be an observation when we cluster, and I’m not sure we have enough episodes here to get good model fits using this approach. Instead, I’m going to use k-means clustering, which basically places observations (episodes, in this case) into one of k groups (where k is supplied by the user) by trying to minimize the “distance” between each observation and the center of the group. The algorithm iteratively assigns observations to groups, updates the center of each group, reassigns observations to groups, etc. until it reaches a stable solution.\nWe can also include all sorts of different variables in the k-means algorithm to serve as indicators. For this analysis, I’m going to use the number of exchanges between different characters per episode – i.e. the number of exchanges between Michael and Jim, between Jim and Dwight, etc. – to estimate groups. This could tell us, for instance, that one “type” of Office episode features lots of exchanges between Michael and Dwight, lots between Pam and Jim, and few between Pam and Michael. One consideration when we use the k-means algorithm is that, because we’re looking at distance between observations, we typically want our observations to be on the same scale. Fortunately, since all of our indicators will be “number of lines per episode,” they’re already on the same scale, so we don’t need to worry about standardizing.\nLet’s go ahead and set up our data. I’m also going to decide to only use the 5 characters who speak the most during the first 7 seasons in this analysis, otherwise the number of combinations of possible exchanges would be huge. These five characters are:\n\ntop5_chars <- office %>%\n  count(character, sort = TRUE) %>%\n  top_n(5) %>%\n  pull(character)\n\ntop5_chars\n\n[1] \"Michael\" \"Dwight\"  \"Jim\"     \"Pam\"     \"Andy\"   \n\n\nOk, so our top 5 characters here are Michael, Dwight, Jim, Pam, and Andy. Since Andy doesn’t join the show until season 3, I’m actually going to narrow our window of usable episodes to those in seasons 3-7. Otherwise, the clustering algorithm would likely group episodes with a focus on those in seasons 1 and 2, where Andy will obviously have 0 lines, vs episodes in later seasons.\nAdditionally, we want to code our changes so that Michael & Jim is the same as Jim & Michael.\n\ncombos <- t(combn(top5_chars, 2)) %>%\n  as_tibble() %>%\n  mutate(comb = glue::glue(\"{V1}&{V2}\"),\n         comb_inv = glue::glue(\"{V2}&{V1}\"))\n\nreplace_comb <- combos$comb\n\nnames(replace_comb) <- combos$comb_inv\n\noffice_exchanges <- office %>%\n  filter(as.numeric(season) >= 3) %>%\n  mutate(char2 = lead(character)) %>% #this will tell us who the speaker is talking to\n  filter(character %in% top5_chars &\n         char2 %in% top5_chars &\n         character != char2) %>% #this filters down to just exchanges between our top 5 characters\n  mutate(exchange = glue::glue(\"{character}&{char2}\") %>%\n           str_replace_all(replace_comb)) %>% #these lines ensure that, e.g. Michael & Jim is coded the same as Jim & Michael\n  select(season, episode_name, character, char2, exchange) %>%\n  count(season, episode_name, exchange) %>%\n  pivot_wider(names_from = exchange,\n              values_from = n,\n              values_fill = list(n = 0))\n\nhead(office_exchanges)\n\n# A tibble: 6 × 12\n  season episode_name         `Dwight&Andy` `Dwight&Jim` `Dwight&Pam` `Jim&Andy`\n   <int> <chr>                        <int>        <int>        <int>      <int>\n1      3 A Benihana Christma…             6           10           17         10\n2      3 Back from Vacation               1           16            6          2\n3      3 Beach Games                      8            8            3          1\n4      3 Ben Franklin                     0           14            2          0\n5      3 Branch Closing                   0            5            1          4\n6      3 Business School                  0           10            3          0\n# … with 6 more variables: `Jim&Pam` <int>, `Michael&Andy` <int>,\n#   `Michael&Dwight` <int>, `Michael&Jim` <int>, `Michael&Pam` <int>,\n#   `Pam&Andy` <int>\n\n\nGreat – now our data is all set up so that we know the number of lines exchanged between main characters in each episode. We can run some clustering algorithms now to see if there are patterns in these exchanges. To do this, we’ll fit models testing out 1-10 clusters. We’ll then look at the error for each of these models graphically and use this to choose how many clusters we want to include in our final model.\n\nclusters_fit <- tibble(\n  k = c(1:10),\n  km_fit = map(c(1:10), ~kmeans(office_exchanges %>% select(-c(1:2)), centers = .))\n) %>%\n  mutate(within_ss = map_dbl(km_fit, ~pluck(., 5)))\n\nclusters_fit %>%\n  ggplot(aes(x = k, y = within_ss)) +\n  geom_point() +\n  geom_line() +\n  labs(\n    title = \"Within Cluster Sum of Squares vs K\"\n  )\n\n\n\n\nWe can see that error decreases as we add more clusters, and error will always decrease as k increases. But we can also see that the rate of decrease slows down a bit as we increase our number of clusters. Ideally, there would be a definitive bend, or “elbow” in this plot where the rate of decrease levels off (which is also the number of clusters we’d choose), but that’s not quite the case here. It seems like there’s some slight elbow-ing at 5 clusters, so let’s just go ahead and choose that. Now we can look at the patterns of exchanges in each of these clusters.\n\noffice_clustered <- augment(clusters_fit$km_fit[[5]], data = office_exchanges)\n\nclusters_long <- office_clustered %>%\n  mutate(season = as_factor(season)) %>%\n  group_by(.cluster) %>%\n  summarize_if(is.numeric, mean, na.rm = TRUE) %>%\n  ungroup() %>%\n  pivot_longer(cols = -c(\".cluster\"),\n               names_to = \"chars\",\n               values_to = \"lines\")\n\nclusters_long %>%\n  ggplot(aes(x = lines, y = chars, fill = .cluster)) +\n    geom_col() +\n    facet_wrap(~.cluster, ncol = 2, scales = \"free_y\") +\n    #scale_y_reordered() +\n    scale_fill_ipsum() +\n    theme_minimal() +\n    labs(\n      title = \"Types of Office Episodes\"\n    ) +\n    theme(\n      legend.position = \"none\"\n    )\n\n\n\n\nSo, these plots show us the average number of exchanges between characters by cluster. Cluster 1 episodes seem to center around exchanges between Michael and Pam, and we also see a fair amount of exchanges between Michael & Jim, Michael & Dwight, and Jim & Pam. Cluster 2 episodes overwhelmingly feature interactions between Michael and Dwight. Cluster 3 episodes have relatively few exchanges between all of our main characters – this probably means that there’s a lot of side character action going on (recall that we didn’t include exchanges between anyone other than Michael, Dwight, Jim, Pam, and Andy in our clustering algorithm). Cluster 4 episodes have a lot of Michael and Andy interactions, along with a fair number of Michael-Dwight and Jim-Pam interactions. And Cluster 5 seems to be predominantly Michael and Jim, but also a fair amount of Michael-Dwight and Dwight-Jim, which makes sense. Usually when Jim talks to Michael in the show, Dwight finds a way to intrude.\nOne thing to remember is that these clusters aren’t necessarily balanced. As the table below shows, most episodes fit into Cluster 3.\n\noffice_clustered %>%\n  count(.cluster, name = \"num_episodes\") %>%\n  kable(format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"condensed\", \"striped\", \"hover\"))\n\n\n\n \n  \n    .cluster \n    num_episodes \n  \n \n\n  \n    1 \n    16 \n  \n  \n    2 \n    10 \n  \n  \n    3 \n    60 \n  \n  \n    4 \n    8 \n  \n  \n    5 \n    17 \n  \n\n\n\n\n\nAnother thing to keep in mind is that, across the all of the characters, Michael has far and away the most lines, so his interactions tend to drive this clustering. If we centered and scaled our variables, this would likely change, but we’d also lose some of the interpretability that comes with working in the raw metrics.\nFinally, let’s just choose a random episode from each cluster to see which episodes are falling into which categories.\n\noffice_clustered %>%\n  group_by(.cluster) %>%\n  sample_n(size = 1) %>%\n  select(.cluster, season, episode_name) %>%\n  kable(format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"striped\"))\n\n\n\n \n  \n    .cluster \n    season \n    episode_name \n  \n \n\n  \n    1 \n    3 \n    Women's Appreciation \n  \n  \n    2 \n    3 \n    The Coup \n  \n  \n    3 \n    3 \n    Diwali \n  \n  \n    4 \n    7 \n    Andy's Play \n  \n  \n    5 \n    3 \n    The Merger \n  \n\n\n\n\n\nThat’s all for now. I might do one more with some predictive modeling in the future."
  },
  {
    "objectID": "posts/writing-window-functions/index.html",
    "href": "posts/writing-window-functions/index.html",
    "title": "Writing Window Functions",
    "section": "",
    "text": "Setup\nFirst, I’m going to load packages. For this, I’m only using {tidyverse} (and within tidyverse, mostly {purrr} for iteration) and {RcppRoll} as a ground-truth to test my functions. I’m also going to use the {glue} package later on, but that’s less central and I’ll load it when I need it.\n\nlibrary(tidyverse)\nlibrary(RcppRoll)\n\nNext, I’m going to set up a minimal tibble to use for calculations. This will have an day column and a val column. The val column is the one I’m going to be doing calculations on, and the day column is going to serve as an index for the rolling average.\n\nset.seed(0408)\n\ndf <- tibble(\n  day = c(1:250),\n  val = rnorm(250, mean = 5, sd = 1)\n)\n\ndf\n\n# A tibble: 250 × 2\n     day   val\n   <int> <dbl>\n 1     1  2.64\n 2     2  5.30\n 3     3  4.29\n 4     4  5.76\n 5     5  3.75\n 6     6  4.89\n 7     7  4.50\n 8     8  3.84\n 9     9  4.74\n10    10  6.41\n# … with 240 more rows\n\n\n\n\nStep 1: Testing Iteration\nSo, my process for building this function is going to be to create something very basic with few variables first and then gradually abstract this out to make a more responsive function. Eventually, I’ll get to a point where the rolling aggregation function will be general enough to allow for the specification of arbitrary aggregate functions and windows.\nThe first step, then, is just to test the logic of the calculation I need to create to calculate rolling averages. I’ll do this by assuming a 28 day window (we’ll be able to change the window later), create a “truth” to test against using RcppRoll’s roll_mean() function, and then iterate using map().\n\ntruth <- roll_mean(df$val, n = 28, align = \"right\")\n\ntest <- map_dbl(\n  c(28:length(df$val)), #this represents the days I want to calculate the average for. I'm starting on day 28 (because I want a 28-day rolling average, \n  #and the first time I'll have 28 days of data is on day 28) and going through the last day\n  function(a) {\n    mean(df$val[(a - 27):a], na.rm = FALSE) \n  } #this specifies what I'm doing -- taking the mean of the 'val' column for each 28 day window \n  #(day 1-28, day 2-29, etc). If I don't subtract 1 window value when I subset, \n  #I'll actually get 29 days.\n)\n\nall.equal(truth, test) #this tests to see that the vectors are equal.\n\n[1] TRUE\n\n\n\n\nStep 2: Building Out Functions\nGreat, so the logic of the calculation works. Now, let’s extend it a little bit to create a function where I can specify the variable I want to use as well as the window I want to take the rolling average over.\n\nee_roll_mean <- function(x, window) {\n\n  map_dbl(\n    c(window:length(x)),\n    function(a) {\n      mean(x[(a - window+1):a], na.rm = FALSE)\n    }\n\n  )\n}\n\ntest_2 <- ee_roll_mean(df$val, 28)\n\nall.equal(test_2, truth)\n\n[1] TRUE\n\n\nIt works when we set the window value to 28, but let’s also test that it works when we use a different window just to be safe.\n\ntruth_win8 <- roll_mean(df$val, n = 8, align = \"right\")\n\ntest_win8 <- ee_roll_mean(df$val, window = 8)\n\nall.equal(truth_win8, test_win8)\n\n[1] TRUE\n\n\nThis works well for taking the rolling average – we can specify the values we want to take the average over as well as the window for that average. But there are other functions we might be interested in getting rolling aggregates for as well. For instance, we might want to know the minimum or standard deviation of a value during some windows of time. Rather than write separate functions to do this, we can just extend our previous function to allow us to supply whichever aggregation function we want.\n\nee_roll_func <- function(x, window, fn = mean) {\n\n  map_dbl(\n    c(window:length(x)),\n    function(a) {\n      fn(x[(a - window+1):a], na.rm = FALSE)\n    }\n  ) \n}\ntest_3 <- ee_roll_func(df$val, window = 8, fn = sd)\n\n#testing against the RcppRoll function that does the same thing\n\ntruth_3 <- roll_sd(df$val, n = 8, align = \"right\")\n\nall.equal(test_3, truth_3)\n\n[1] TRUE\n\n\n\n\nStep 3: Pad the Output\nOne thing I’m noticing when looking at the output of each of these functions is that the length of the output vectors differ depending on the value we pass to the window argument.\n\nlength(test)\n\n[1] 223\n\nlength(test_win8)\n\n[1] 243\n\n\nI’m also noticing that these outputs are shorter than the length of the input vector (which is length 250). This makes sense because the function can’t take, for example, the 28 day average before the 28th day, and so the length of the output vector will be 27 elements shorter than the length of the input vector.\nThis isn’t so great if we want to add the results of this function back into our original df, though, because all of the vectors in a df need to be the same length. One solution is to “pad” our output vector with the appropriate amount of NA values so that it is the same length as the input vector and can therefore get added as a column in our df. So let’s do that.\n\nee_roll_func_padded <- function(x, window, fn = mean) {\n\n  map_dbl(\n    c(window:length(x)),\n    function(a) {\n      fn(x[(a - window+1):a], na.rm = FALSE)\n    }\n  ) %>%\n    append(rep(NA_real_, times = window-1), values = .)   #this will pad the front with a number of NAs equal\n  #to the window value minus 1\n\n}\ntest_pad1 <- ee_roll_func_padded(df$val, window = 8) #note that if we don't supply a function, it will use the mean\n\ntest_pad2 <- ee_roll_func_padded(df$val, window = 20)\n\ntest_pad1\n\n  [1]       NA       NA       NA       NA       NA       NA       NA 4.372225\n  [9] 4.634703 4.773530 4.751241 4.837210 4.835834 4.947405 5.023067 5.159392\n [17] 5.259393 4.897024 5.154236 4.748580 4.790054 4.403228 4.522648 4.519479\n [25] 4.480582 4.687750 4.701154 4.851093 4.652568 4.847791 4.811578 4.864686\n [33] 4.672642 4.530416 4.582749 4.682431 4.717240 4.746443 4.652665 4.466197\n [41] 4.611190 4.706513 4.568209 4.517622 4.872942 5.065789 5.186852 5.390533\n [49] 5.395041 5.507209 5.403271 5.174482 5.179670 5.038712 5.020135 4.838939\n [57] 4.875701 4.755078 4.865224 5.176775 5.202352 5.000563 4.797047 4.894503\n [65] 4.810376 5.004196 4.977340 4.848640 4.753013 4.961929 5.142875 5.096611\n [73] 5.248953 5.181127 4.941060 4.842180 4.693671 4.603321 4.722901 4.707204\n [81] 4.667018 4.490093 4.642128 4.688560 4.940980 5.010917 4.865457 5.077085\n [89] 4.943111 5.104771 5.225281 5.405689 5.459406 5.772019 5.873998 5.653444\n [97] 5.727537 5.800159 5.719428 5.649400 5.519840 5.130266 4.799206 5.049435\n[105] 4.941485 4.868625 4.976469 5.154863 5.039641 5.037770 5.202060 4.829763\n[113] 5.054458 5.091318 5.113392 5.056769 4.999436 5.110106 5.070160 5.305183\n[121] 5.148242 5.163269 5.116071 5.209866 5.295613 5.295760 5.642222 5.797642\n[129] 5.800138 5.454873 5.221126 5.037245 5.077385 5.216140 5.121762 4.768109\n[137] 4.833714 5.100003 5.221173 5.314504 5.166415 4.883192 4.762374 4.661057\n[145] 4.620171 4.638887 4.789642 4.625148 4.791990 5.013448 4.746997 5.084247\n[153] 4.989471 4.899552 4.728081 4.728852 4.656302 4.596832 4.789755 4.571342\n[161] 4.750549 4.828835 4.946644 4.904696 4.951820 4.962249 4.952014 5.015733\n[169] 4.920095 4.695109 4.624958 4.687815 5.038474 5.314062 5.471601 5.659262\n[177] 5.667469 5.904322 5.968823 6.073087 5.663232 5.407968 5.177870 5.237016\n[185] 5.445955 5.679831 5.614257 5.233444 5.227926 5.097925 5.119121 4.940067\n[193] 4.803742 4.593282 4.749424 5.008870 4.902099 5.014811 5.048332 5.111487\n[201] 5.059727 4.972699 4.866232 4.952064 4.924344 5.077133 5.166955 5.172722\n[209] 5.304330 5.370433 5.299762 5.238768 5.450415 5.399515 5.197358 5.101200\n[217] 5.005289 5.243733 5.194603 5.205039 5.192346 5.082026 5.030877 5.072784\n[225] 5.032299 4.637538 4.781121 4.812846 4.758887 4.541770 4.712547 4.636478\n[233] 4.876790 5.177345 4.831910 4.870811 5.106333 5.162062 4.990127 5.058875\n[241] 4.603333 4.441803 4.618171 4.585108 4.444892 4.505732 4.827083 4.840013\n[249] 5.098275 5.081742\n\n\nNotice that when we call test_pad1 we get a vector with several NA values appended to the front. And when we look at the length of each of these vectors, we can see that they’re length 250\n\nlength(test_pad1)\n\n[1] 250\n\nlength(test_pad2)\n\n[1] 250\n\n\n\n\nStep 4: Use Functions to Add Columns to Data\nNow that we have a function that reliably outputs a vector the same length as the columns in our dataframe, we can use it in conjunction with other tidyverse operations to add columns to our dataframe.\n\ndf %>%\n  mutate(roll_avg = ee_roll_func_padded(val, window = 8, fn = mean))\n\n# A tibble: 250 × 3\n     day   val roll_avg\n   <int> <dbl>    <dbl>\n 1     1  2.64    NA   \n 2     2  5.30    NA   \n 3     3  4.29    NA   \n 4     4  5.76    NA   \n 5     5  3.75    NA   \n 6     6  4.89    NA   \n 7     7  4.50    NA   \n 8     8  3.84     4.37\n 9     9  4.74     4.63\n10    10  6.41     4.77\n# … with 240 more rows\n\n\nFinally, what if we wanted to get the rolling mean, standard deviation, min, and max all as new columns in our dataframe using the function we created. Our function allows us to pass in whichever aggregation function we want to use (well, probably not any function), so we can use pmap() from {purrr} to iterate over multiple functions and, in combination with the {glue} package, also set meaningful names for the new variables.\nI’ll set up a dataframe called params that has the names of the new variables and the corresponding functions, then I’ll loop over these names and functions to create new columns in our original dataframe. I’m not going to go over all of the code here, but if you’re curious, it might be helpful to look at the documentation for {glue}, {purrr}, and possibly {rlang} (for the := operator).\n\nlibrary(glue)\n\nparams <- tibble(\n  names = c(\"roll_avg\", \"roll_sd\", \"roll_min\", \"roll_max\"),\n  fn = lst(mean, sd, min, max)\n)\n\nparams %>%\n  pmap_dfc(~df %>%\n             transmute(\"{.x}\" := ee_roll_func_padded(val, window = 8, fn = .y))) %>%\n  bind_cols(df, .)\n\n# A tibble: 250 × 6\n     day   val roll_avg roll_sd roll_min roll_max\n   <int> <dbl>    <dbl>   <dbl>    <dbl>    <dbl>\n 1     1  2.64    NA     NA        NA       NA   \n 2     2  5.30    NA     NA        NA       NA   \n 3     3  4.29    NA     NA        NA       NA   \n 4     4  5.76    NA     NA        NA       NA   \n 5     5  3.75    NA     NA        NA       NA   \n 6     6  4.89    NA     NA        NA       NA   \n 7     7  4.50    NA     NA        NA       NA   \n 8     8  3.84     4.37   0.982     2.64     5.76\n 9     9  4.74     4.63   0.691     3.75     5.76\n10    10  6.41     4.77   0.918     3.75     6.41\n# … with 240 more rows\n\n\nThis gives us, for each 8-day window (e.g. day 1-8, day 2-9, etc) an average, standard deviation, minimum, and maximum of the val column.\n\n\nWrapping Up\nAs sort of a final note, this activity was meant to be both an exercise for me in working through some programming using window functions as well as a walkthrough/tutorial for others interested in writing functions. That said, when I dive back into the Kaggle data I mentioned earlier, I’ll use the functions from the {RcppRoll} package rather than my own. These are optimized to run quickly because they use C++ code and they’re going to be more efficient than anything I just wrote. This doesn’t matter much when we use a little 250 observation dataframe for demonstration, but it will make a difference working with several thousand observations at once.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-nc/4.0/CitationBibTeX citation:@online{ekholm2020,\n  author = {Eric Ekholm},\n  title = {Writing {Window} {Functions}},\n  date = {2020-05-06},\n  url = {https://www.ericekholm.com/posts/writing-window-functions},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEric Ekholm. 2020. “Writing Window Functions.” May 6, 2020.\nhttps://www.ericekholm.com/posts/writing-window-functions."
  },
  {
    "objectID": "posts/unconsciousness-in-the-xmen/index.html",
    "href": "posts/unconsciousness-in-the-xmen/index.html",
    "title": "Unconsciousness in the Xmen",
    "section": "",
    "text": "A part of me has always wanted to get into comic books. I think it would be a really good fit for me – I’m definitely a nerd. I play video games, I read fantasy novels, I code/do data science for fun. Comic books should be right up my alley. But for whatever reason, I’ve never taken the plunge. Maybe it’s a time commitment thing. Maybe I know I’ll like them too much. Maybe it’s too daunting to figure out how to start. Regardless, even thought I’m not into comic books, they are intriguing to me, and the X-Men particularly so, which is why I wanted to take a little bit of time to analyze this X-men data promoted by the #tidytuesday project.\nThe other main purpose of this blog post is to toy around with running a Poisson regression. A few months ago, I saw a post about how the tidymodels framework had some new “parsnip-adjacent” packages, with one being {poissonreg} which fits – you guessed it – Poisson regressions. I haven’t had much reason to use Poisson regression in any of my previous work or in datasets I’ve toyed around with, but this X-men dataset seems like a good excuse to try it out. So, onward and upward!"
  },
  {
    "objectID": "posts/unconsciousness-in-the-xmen/index.html#setup",
    "href": "posts/unconsciousness-in-the-xmen/index.html#setup",
    "title": "Unconsciousness in the Xmen",
    "section": "Setup",
    "text": "Setup\nFirst, I’ll load some packages, set some miscellaneous options, and import the data. This data comes from the Claremont Run project, which mines data from Chris Claremont’s run (1975-1991) writing the X-men comics. To learn more about the project, you can visit the website. There are several datasets available, but for this analysis, I’m going to use data from the characters dataset, the character_visualization dataset, and the locations dataset.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(hrbrthemes)\nlibrary(gameofthrones)\nlibrary(vip)\n\ntheme_set(theme_ipsum())\n\nlann <- got(1, option = \"Lannister\")\n\ncharacters <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-06-30/characters.csv')\nlocations <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-06-30/locations.csv')\ncharacter_visualization <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-06-30/character_visualization.csv')"
  },
  {
    "objectID": "posts/unconsciousness-in-the-xmen/index.html#exploring-the-data",
    "href": "posts/unconsciousness-in-the-xmen/index.html#exploring-the-data",
    "title": "Unconsciousness in the Xmen",
    "section": "Exploring the Data",
    "text": "Exploring the Data\nLet’s first look at the characters dataset. In this dataset, each row corresponds to a character in an issue, and each column corresponds to actions or events relevant to that character. Here’s a glimpse of that data:\n\ncharacters %>%\n  glimpse()\n\nRows: 4,209\nColumns: 34\n$ issue                                         <dbl> 97, 97, 97, 97, 97, 97, …\n$ character                                     <chr> \"Professor X\", \"Wolverin…\n$ rendered_unconcious                           <dbl> 0, 0, 0, 1, 0, 0, 0, 0, …\n$ captured                                      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ declared_dead                                 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ redressed                                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ depowered                                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ clothing_torn                                 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ subject_to_torture                            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ quits_team                                    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ surrenders                                    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ number_of_kills_humans                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ number_of_kills_non_humans                    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ initiates_physical_conflict                   <chr> NA, NA, \"1\", NA, NA, NA,…\n$ expresses_reluctance_to_fight                 <dbl> NA, NA, 1, NA, NA, NA, N…\n$ on_a_date_with_which_character                <chr> NA, NA, NA, NA, NA, NA, …\n$ kiss_with_which_character                     <chr> NA, NA, NA, NA, NA, NA, …\n$ hand_holding_with_which_character             <chr> \"Moira MacTaggert\", NA, …\n$ dancing_with_which_character                  <chr> NA, NA, NA, NA, NA, NA, …\n$ flying_with_another_character                 <chr> NA, NA, NA, \"Storm\", \"Je…\n$ arm_in_arm_with_which_character               <chr> NA, NA, NA, NA, NA, NA, …\n$ hugging_with_which_character                  <chr> NA, NA, NA, NA, NA, NA, …\n$ physical_contact_other                        <chr> \"Moira MacTaggert\", \"Sto…\n$ carrying_with_which_character                 <chr> NA, NA, NA, NA, NA, NA, …\n$ shared_bed_with_which_character               <lgl> NA, NA, NA, NA, NA, NA, …\n$ shared_room_domestically_with_which_character <lgl> NA, NA, NA, NA, NA, NA, …\n$ explicitly_states_i_love_you_to_whom          <chr> NA, NA, NA, NA, NA, NA, …\n$ shared_undress                                <chr> NA, NA, NA, NA, NA, NA, …\n$ shower_number_of_panels_shower_lasts          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ bath_number_of_panels_bath_lasts              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ depicted_eating_food                          <dbl> 1, 0, 0, 0, 0, 0, 0, 0, …\n$ visible_tears_number_of_panels                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ visible_tears_number_of_intances              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ special_notes                                 <chr> NA, NA, NA, NA, NA, NA, …\n\n\nSo, we can see in this dataset things like who Professor X held hands with in issue 97, how many humans were killed by Magneto in issue 105, etc. We see lots of NAs and 0s in this dataset. The only column I’m going to use from this is the rendered unconscious column, which will be outcome variable in the models later.\nIn the character_visualization dataset, each row represents a per-issue count of the number of times a character is depicted, speaks, thinks, has a narrative statement (I think this is probably only relevant for the narrator character?), either when the character is in costume or not in costume.\n\ncharacter_visualization %>%\n  glimpse()\n\nRows: 9,800\nColumns: 7\n$ issue     <dbl> 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, …\n$ costume   <chr> \"Costume\", \"Costume\", \"Costume\", \"Costume\", \"Costume\", \"Cost…\n$ character <chr> \"Editor narration\", \"Omnipresent narration\", \"Professor X = …\n$ speech    <dbl> 0, 0, 0, 7, 24, 0, 11, 9, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ thought   <dbl> 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ narrative <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ depicted  <dbl> 0, 0, 0, 10, 23, 0, 9, 17, 17, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\nIn the location dataset, each row corresponds to a location in which part of the issue takes place, with as many locations listed per issue as appear in that issue. The dataset also includes a “context” column that describes things like whether the location is shown in the present, as part of a flashback, in a dream, etc. Here’s a glimpse:\n\nlocations %>%\n  glimpse()\n\nRows: 1,413\nColumns: 4\n$ issue    <dbl> 97, 97, 97, 97, 97, 98, 98, 98, 98, 98, 99, 99, 99, 99, 99, 9…\n$ location <chr> \"Space\", \"X-Mansion\", \"Rio Diablo Research Facility\", \"Kenned…\n$ context  <chr> \"Dream\", \"Present\", \"Present\", \"Present\", \"Present\", \"Present…\n$ notes    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Cuts back and fo…\n\n\nAcross these datasets, it probably makes the most sense to aggegrate data up to the issue level, since that’s kind of the lowest common denominator here. So, essentially the question I’m going to try to answer in this blog post is:\nWhat features of an X-men issue predict how many characters are rendered unconscious in that issue?\nFirst, let’s look at the distribution of rendered unconscious:\n\ncharacters %>%\n  count(issue, wt = rendered_unconcious, sort = TRUE) %>%\n  ggplot(aes(x = n)) +\n  geom_histogram(fill = lann, bins = 8)\n\n\n\n\nRight, so, this is a pretty strongly right-skewed distribution, which is sort of what we’d expect from a Poisson distribution, especially one with a low expected number of events (which I’d imagine is the case in comic books)."
  },
  {
    "objectID": "posts/unconsciousness-in-the-xmen/index.html#cleaning-aggregating-and-joining",
    "href": "posts/unconsciousness-in-the-xmen/index.html#cleaning-aggregating-and-joining",
    "title": "Unconsciousness in the Xmen",
    "section": "Cleaning, Aggregating, and Joining",
    "text": "Cleaning, Aggregating, and Joining\nNext, let’s aggregate our data up to the issue level. This will give us data where a row represents an issue rather than a character within an issue or a location within an issue. We’ll start with the characters dataset. There’s a lot we could do with this data, but because there are only 183 issues represented in this dataset, we need to be cognizant about how many predictors we’re including. So the only variable I’m going to use here is rendered unconscious as the outcome, which will represent the number of characters rendered unconscious in a given issue.\n\nrend_df <- characters %>%\n  group_by(issue) %>%\n  summarize(rendered_unconscious = sum(rendered_unconcious, na.rm = FALSE))\n\nNext, let’s work on the character_visualization dataset. Again, trying to keep the number of predictors relatively small, I’m going to winnow this down to represent counts of how many times a handful of key characters are depicted in each issue. I don’t know a ton about the X-men, but I know who some of the more important characters are, so I’m going to choose Wolverine, Professor X, Magneto, and Jean Grey here.\n\nchar_sum <- character_visualization %>%\n  filter(str_detect(character, \"Wolverine|Xavier|Jean Grey|Magneto\")) %>%\n  group_by(issue, character) %>%\n  summarize(depict = sum(depicted, na.rm = FALSE)) %>%\n  mutate(character = case_when(\n    str_detect(character, \"Jean Grey\") ~ \"Jean_Grey\",\n    str_detect(character, \"Wolv\") ~ \"Wolverine\",\n    str_detect(character, \"Magneto\") ~ \"Magneto\",\n    str_detect(character, \"Xavier\") ~ \"Professor_X\"\n  )) %>%\n  pivot_wider(\n    names_from = character,\n    values_from = depict\n  )\n\nNext, let’s work on our locations dataset. First, let’s look at the most common locations. Again, since we only have 183 rows in our dataset that we’re modeling with, I only want to choose a handful of variables to include in the model here.\n\nlocations %>%\n  count(location, sort = TRUE)\n\n# A tibble: 785 × 2\n   location                             n\n   <chr>                            <int>\n 1 X-Mansion                          100\n 2 Danger Room                         27\n 3 Space                               19\n 4 Muir Island, Scotland               14\n 5 Unspecified region in Australia     14\n 6 Eagle Plaza, Dallas Texas           11\n 7 Central Park                        10\n 8 Morlock residence under New York    10\n 9 Princess Lilandra's Home Planet     10\n10 San Francisco                       10\n# … with 775 more rows\n\n\nOk, so, I’m just going to go with the 3 most common locations: the X-mansion, the Danger Room (whatever that is), and Space. Danger Room sounds to me like a place where people might be rendered unconscious.\n\nuse_locs <- locations %>%\n  count(location, sort = TRUE) %>%\n  top_n(3) %>%\n  pull(location)\n\nlocs_sum <- locations %>%\n  group_by(issue) %>%\n  summarize(mansion = use_locs[[1]] %in% location,\n            danger_room = use_locs[[2]] %in% location,\n            space = use_locs[[3]] %in% location) %>%\n  mutate(across(where(is_logical), as.numeric))\n\nThis will return a dataset that tells us whether a given issue has the X-mansion, the Danger Room, or Space as a location.\n\nlocs_sum %>%\n  glimpse()\n\nRows: 183\nColumns: 4\n$ issue       <dbl> 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 1…\n$ mansion     <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0…\n$ danger_room <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0…\n$ space       <dbl> 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\nNow we can join the three datasets into one useful for modeling. I’m using an inner join here because, for whatever reason, the character visualization dataset has more issues represented than the others, and we only want issues that are represented in all 3 dataframes.\n\nissues_joined <- reduce(list(rend_df, char_sum, locs_sum), ~inner_join(.x, .y, by = \"issue\"))"
  },
  {
    "objectID": "posts/unconsciousness-in-the-xmen/index.html#modeling",
    "href": "posts/unconsciousness-in-the-xmen/index.html#modeling",
    "title": "Unconsciousness in the Xmen",
    "section": "Modeling",
    "text": "Modeling\nCool, so now we’re done preprocessing our data – now we can specify our model.\nI mentioned before that one issue here is that this is a small set of data. We have 183 observations (again, each observation is an issue), which isn’t many. One way to make our modeling more robust is to use bootstrap resampling (see our good friend Wikipedia for an explanation) and to fit models to several resamples.\n\nset.seed(0408)\n\nbooties <- bootstraps(issues_joined, times = 100)\n\nhead(booties$splits, n = 5)\n\n[[1]]\n<Analysis/Assess/Total>\n<183/68/183>\n\n[[2]]\n<Analysis/Assess/Total>\n<183/66/183>\n\n[[3]]\n<Analysis/Assess/Total>\n<183/66/183>\n\n[[4]]\n<Analysis/Assess/Total>\n<183/70/183>\n\n[[5]]\n<Analysis/Assess/Total>\n<183/64/183>\n\n\nWhat we can see here is that every bootstrap sample has 183 rows in the analysis set, which is what the model will be trained on, and then some other number of rows in the assessment set. This other number is the out-of-bag sample – the rows that weren’t randomly sampled by the bootstrap process.\nNext, I’m going to set up a workflow. I think of this as like a little suitcase that can carry things I want to use in my model around – I think that analogy might be from Julia Silge? Anyway, I’m going to start by adding the formula I want to use in my model.\n\nxmen_wf <- workflow() %>%\n  add_formula(rendered_unconscious ~ Magneto + Jean_Grey + Wolverine + Professor_X + mansion + danger_room + space)\n\nNow we can further specify the model. Remember that since our outcome is a count, we’ll be fitting a Poisson regression. Looking at the outcome distribution earlier, I don’t think I need to use a zero-inflated model here (although maybe? Again, this isn’t really my expertise), so I’m just going to proceed with a regular Poisson regression, fit using the {glmnet} engine. I’m also going to tune the penalty and mixture arguments, which control the amount of total regularization applied to the model as well as the proportion of the penalty that is L1 (lasso) vs L2 (ridge regression).\nBrief Interpolation on what a Poisson regression is A Poisson regression is a generalized linear model (GLM) used to model count data. Like the name implies, GLMs are generalizations of linear models that use a link function, g(), to transform the expected value of the response (outcome) to a linear function of the predictor variables. Poisson regression uses a log link function to accomplish this transformation. For people interested in reading more, I really like John Fox’s book, Applied Regression Analysis.\n\nlibrary(poissonreg)\n\npoisson_mod <- poisson_reg(\n  penalty = tune(),\n  mixture = tune()\n) %>%\n  set_engine(\"glmnet\")\n\nSince I’m tuning a couple of parameters, I need to make a grid with possible values to tune across\n\npoisson_tune <- grid_max_entropy(\n  penalty(),\n  mixture(), \n  size = 10\n)\n\nAnd I’ll drop the model spec into the previous workflow.\n\nxmen_wf <- xmen_wf %>%\n  add_model(poisson_mod)\n\nxmen_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: poisson_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nrendered_unconscious ~ Magneto + Jean_Grey + Wolverine + Professor_X + \n    mansion + danger_room + space\n\n── Model ───────────────────────────────────────────────────────────────────────\nPoisson Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = tune()\n\nComputational engine: glmnet \n\n\nAnd now we can fit the model using our bootstrap resamples.\n\nxmen_fit <- tune_grid(\n  xmen_wf,\n  resamples = booties,\n  grid = poisson_tune\n)\n\nOur models have fit, so now we can look at our results:\n\nxmen_fit %>%\n  collect_metrics()\n\n# A tibble: 20 × 8\n    penalty mixture .metric .estimator   mean     n std_err .config             \n      <dbl>   <dbl> <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n 1 8.81e- 4  0.0155 rmse    standard   1.93     100 0.0894  Preprocessor1_Model…\n 2 8.81e- 4  0.0155 rsq     standard   0.0277   100 0.00398 Preprocessor1_Model…\n 3 4.67e- 7  0.0676 rmse    standard   1.93     100 0.0896  Preprocessor1_Model…\n 4 4.67e- 7  0.0676 rsq     standard   0.0277   100 0.00398 Preprocessor1_Model…\n 5 5.56e- 1  0.148  rmse    standard   1.71     100 0.0201  Preprocessor1_Model…\n 6 5.56e- 1  0.148  rsq     standard   0.0288   100 0.00426 Preprocessor1_Model…\n 7 4.76e-10  0.190  rmse    standard   1.93     100 0.0895  Preprocessor1_Model…\n 8 4.76e-10  0.190  rsq     standard   0.0277   100 0.00398 Preprocessor1_Model…\n 9 1.09e- 2  0.500  rmse    standard   1.92     100 0.0841  Preprocessor1_Model…\n10 1.09e- 2  0.500  rsq     standard   0.0278   100 0.00403 Preprocessor1_Model…\n11 2.44e- 7  0.517  rmse    standard   1.94     100 0.0896  Preprocessor1_Model…\n12 2.44e- 7  0.517  rsq     standard   0.0277   100 0.00398 Preprocessor1_Model…\n13 1.73e-10  0.622  rmse    standard   1.94     100 0.0896  Preprocessor1_Model…\n14 1.73e-10  0.622  rsq     standard   0.0277   100 0.00398 Preprocessor1_Model…\n15 1.10e- 5  0.881  rmse    standard   1.94     100 0.0897  Preprocessor1_Model…\n16 1.10e- 5  0.881  rsq     standard   0.0277   100 0.00398 Preprocessor1_Model…\n17 1.99e- 1  0.942  rmse    standard   1.69     100 0.0190  Preprocessor1_Model…\n18 1.99e- 1  0.942  rsq     standard   0.0302   100 0.00404 Preprocessor1_Model…\n19 5.97e-10  0.985  rmse    standard   1.94     100 0.0897  Preprocessor1_Model…\n20 5.97e-10  0.985  rsq     standard   0.0277   100 0.00398 Preprocessor1_Model…\n\n\nOk, so, my limited understanding of Poisson regression is that neither RMSE or R-squared values are ideal metrics, and some googling led me to find that there’s an open issue to add a Poisson log loss metric to the yardstick package, so we’ll gloss over these for now.\nAnyway, let’s pick the best model here, finalize the model, and then fit it to our full training data.\n\nbest_params <- xmen_fit %>%\n  select_best(metric = \"rmse\")\n\nfinal_mod <- xmen_wf %>%\n  finalize_workflow(best_params) %>%\n  fit(data = issues_joined)\n\nAnd let’s check out how important how variables are. This should give us the coefficients from our model.\n\nfinal_mod %>%\n  pull_workflow_fit() %>% \n  vi()\n\n# A tibble: 7 × 3\n  Variable    Importance Sign \n  <chr>            <dbl> <chr>\n1 mansion        0.157   NEG  \n2 danger_room    0.113   NEG  \n3 Professor_X    0.0197  POS  \n4 Jean_Grey      0.0133  POS  \n5 Wolverine      0.00938 POS  \n6 Magneto        0.00701 POS  \n7 space          0       NEG  \n\n\n\nfinal_mod %>%\n  pull_workflow_fit() %>% \n  vip(num_features = 7, fill = lann)\n\n\n\n\nRight, so, one thing to keep in mind here is that the location variables and the character variables are on different scales, so the effects aren’t directly comparable. But the interpretation here is that more appearances of Professor X are more strongly associated with more characters rendered unconscious in an issue than are more appearances of Magneto, although all of these coefficients are positive, suggesting that more appearances of any of these four characters are associated with more renderings unconscious in that issue. Similarly, the effects of danger_room and mansion are negative, suggesting that if the issue features either of those locations, there tend to be fewer characters rendered unconscious. The coefficient for space is 0, which probably means it got regularized out. Probably the most important piece, here, though, is that these effects seem to be very small, which means they likely don’t actually matter.\nI’m going to call it right here. Even though the model I built doesn’t seem to have much explanatory power, it forced me to read some more about Poisson regression and to dig back into the tidymodels framework, which I’ll count as a win. Plus it gives me an excuse to gather “domain knowledge” about comic books so I can do a better job next time."
  },
  {
    "objectID": "posts/dungeons-and-dragons-part-1/index.html",
    "href": "posts/dungeons-and-dragons-part-1/index.html",
    "title": "Dungeons and Dragons - Part 1",
    "section": "",
    "text": "I’ve been playing Dungeons and Dragons 5th edition (D&D 5e) for a few years now and really enjoy it, although COVID has really hindered my opportunity to play. That said, I recently discovered a D&D 5e API, so I figured I’d do a series of blog posts analyzing D&D data from this API. In this first post, I wanted to do a quick walkthrough of how to get data from this API using R and wrangling it into a structure that’s more or less conducive to later analysis. In later posts, I’ll explore the data and then get into some modeling.\nAs something of an aside – the API has data for character classes, spells, races, monsters, etc. I’m mostly going to focus on the monsters data, but might use some of the other data later on."
  },
  {
    "objectID": "posts/dungeons-and-dragons-part-1/index.html#setup",
    "href": "posts/dungeons-and-dragons-part-1/index.html#setup",
    "title": "Dungeons and Dragons - Part 1",
    "section": "Setup",
    "text": "Setup\nFirst, I’ll load the packages I need to get and wrangle the data, which is really just {tidyverse}, {jsonlite} and good old base R. I’m also adding in the base URL of the API.\n\nlibrary(tidyverse)\nlibrary(jsonlite)\n\ndnd_base <- \"https://www.dnd5eapi.co/api/monsters/\""
  },
  {
    "objectID": "posts/dungeons-and-dragons-part-1/index.html#fetching-data",
    "href": "posts/dungeons-and-dragons-part-1/index.html#fetching-data",
    "title": "Dungeons and Dragons - Part 1",
    "section": "Fetching Data",
    "text": "Fetching Data\nSo, the first step here is to actually get the data from the API. Let’s walk through the process here, illustrating this with a single monster (the aboleth) and then applying the process to all of the monsters.\nWe’ll use the fromJSON() function to get JSON data from the API. We’ll see that this gives us a pretty gnarly nested list.\n\nexample <- fromJSON(paste0(dnd_base, \"aboleth\"))\n\nglimpse(example)\n\nList of 28\n $ index                 : chr \"aboleth\"\n $ name                  : chr \"Aboleth\"\n $ size                  : chr \"Large\"\n $ type                  : chr \"aberration\"\n $ alignment             : chr \"lawful evil\"\n $ armor_class           : int 17\n $ hit_points            : int 135\n $ hit_dice              : chr \"18d10\"\n $ speed                 :List of 2\n  ..$ walk: chr \"10 ft.\"\n  ..$ swim: chr \"40 ft.\"\n $ strength              : int 21\n $ dexterity             : int 9\n $ constitution          : int 15\n $ intelligence          : int 18\n $ wisdom                : int 15\n $ charisma              : int 18\n $ proficiencies         :'data.frame': 5 obs. of  2 variables:\n  ..$ value      : int [1:5] 6 8 6 12 10\n  ..$ proficiency:'data.frame': 5 obs. of  3 variables:\n  .. ..$ index: chr [1:5] \"saving-throw-con\" \"saving-throw-int\" \"saving-throw-wis\" \"skill-history\" ...\n  .. ..$ name : chr [1:5] \"Saving Throw: CON\" \"Saving Throw: INT\" \"Saving Throw: WIS\" \"Skill: History\" ...\n  .. ..$ url  : chr [1:5] \"/api/proficiencies/saving-throw-con\" \"/api/proficiencies/saving-throw-int\" \"/api/proficiencies/saving-throw-wis\" \"/api/proficiencies/skill-history\" ...\n $ damage_vulnerabilities: list()\n $ damage_resistances    : list()\n $ damage_immunities     : list()\n $ condition_immunities  : list()\n $ senses                :List of 2\n  ..$ darkvision        : chr \"120 ft.\"\n  ..$ passive_perception: int 20\n $ languages             : chr \"Deep Speech, telepathy 120 ft.\"\n $ challenge_rating      : int 10\n $ xp                    : int 5900\n $ special_abilities     :'data.frame': 3 obs. of  3 variables:\n  ..$ name: chr [1:3] \"Amphibious\" \"Mucous Cloud\" \"Probing Telepathy\"\n  ..$ desc: chr [1:3] \"The aboleth can breathe air and water.\" \"While underwater, the aboleth is surrounded by transformative mucus. A creature that touches the aboleth or tha\"| __truncated__ \"If a creature communicates telepathically with the aboleth, the aboleth learns the creature's greatest desires \"| __truncated__\n  ..$ dc  :'data.frame':    3 obs. of  3 variables:\n  .. ..$ dc_type     :'data.frame': 3 obs. of  3 variables:\n  .. ..$ dc_value    : int [1:3] NA 14 NA\n  .. ..$ success_type: chr [1:3] NA \"none\" NA\n $ actions               :'data.frame': 4 obs. of  7 variables:\n  ..$ name        : chr [1:4] \"Multiattack\" \"Tentacle\" \"Tail\" \"Enslave\"\n  ..$ desc        : chr [1:4] \"The aboleth makes three tentacle attacks.\" \"Melee Weapon Attack: +9 to hit, reach 10 ft., one target. Hit: 12 (2d6 + 5) bludgeoning damage. If the target i\"| __truncated__ \"Melee Weapon Attack: +9 to hit, reach 10 ft. one target. Hit: 15 (3d6 + 5) bludgeoning damage.\" \"The aboleth targets one creature it can see within 30 ft. of it. The target must succeed on a DC 14 Wisdom savi\"| __truncated__\n  ..$ options     :'data.frame':    4 obs. of  2 variables:\n  .. ..$ choose: int [1:4] 1 NA NA NA\n  .. ..$ from  :List of 4\n  ..$ attack_bonus: int [1:4] NA 9 9 NA\n  ..$ dc          :'data.frame':    4 obs. of  3 variables:\n  .. ..$ dc_type     :'data.frame': 4 obs. of  3 variables:\n  .. ..$ dc_value    : int [1:4] NA 14 NA 14\n  .. ..$ success_type: chr [1:4] NA \"none\" NA \"none\"\n  ..$ damage      :List of 4\n  .. ..$ : NULL\n  .. ..$ :'data.frame': 2 obs. of  2 variables:\n  .. ..$ :'data.frame': 1 obs. of  2 variables:\n  .. ..$ : NULL\n  ..$ usage       :'data.frame':    4 obs. of  2 variables:\n  .. ..$ type : chr [1:4] NA NA NA \"per day\"\n  .. ..$ times: int [1:4] NA NA NA 3\n $ legendary_actions     :'data.frame': 3 obs. of  4 variables:\n  ..$ name        : chr [1:3] \"Detect\" \"Tail Swipe\" \"Psychic Drain (Costs 2 Actions)\"\n  ..$ desc        : chr [1:3] \"The aboleth makes a Wisdom (Perception) check.\" \"The aboleth makes one tail attack.\" \"One creature charmed by the aboleth takes 10 (3d6) psychic damage, and the aboleth regains hit points equal to \"| __truncated__\n  ..$ attack_bonus: int [1:3] NA NA 0\n  ..$ damage      :List of 3\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ :'data.frame': 1 obs. of  2 variables:\n $ url                   : chr \"/api/monsters/aboleth\"\n\n\nTo clean this list up a bit, we’ll use the enframe() function (from {tibble}) to convert the lists into a dataframe and then the pivot_wider() function to reshape this into a single-row tibble.\n\nexample %>%\n  enframe() %>%\n  pivot_wider(names_from = name,\n              values_from = value) %>%\n  glimpse()\n\nRows: 1\nColumns: 28\n$ index                  <list> \"aboleth\"\n$ name                   <list> \"Aboleth\"\n$ size                   <list> \"Large\"\n$ type                   <list> \"aberration\"\n$ alignment              <list> \"lawful evil\"\n$ armor_class            <list> 17\n$ hit_points             <list> 135\n$ hit_dice               <list> \"18d10\"\n$ speed                  <list> [\"10 ft.\", \"40 ft.\"]\n$ strength               <list> 21\n$ dexterity              <list> 9\n$ constitution           <list> 15\n$ intelligence           <list> 18\n$ wisdom                 <list> 15\n$ charisma               <list> 18\n$ proficiencies          <list> [<data.frame[5 x 2]>]\n$ damage_vulnerabilities <list> []\n$ damage_resistances     <list> []\n$ damage_immunities      <list> []\n$ condition_immunities   <list> []\n$ senses                 <list> [\"120 ft.\", 20]\n$ languages              <list> \"Deep Speech, telepathy 120 ft.\"\n$ challenge_rating       <list> 10\n$ xp                     <list> 5900\n$ special_abilities      <list> [<data.frame[3 x 3]>]\n$ actions                <list> [<data.frame[4 x 7]>]\n$ legendary_actions      <list> [<data.frame[3 x 4]>]\n$ url                    <list> \"/api/monsters/aboleth\"\n\n\nGreat. This is more or less the structure we want. You might notice that all of our columns are lists rather than atomic vectors – we’ll deal with that later once we get all of the data.\nNow that we know the basic process, we’ll just apply this to all of the monsters with data available through the API. To do that, I’ll write a function that executes the previous steps, get a list of all of the monsters available in the API, use map() to iterate the “fetch” function for each monster, and then bind all of the resulting rows together.\n\nfetch_monster <- function(monster) {\n  dnd_url <- \"https://www.dnd5eapi.co/api/monsters/\"\n  \n  ret <- fromJSON(paste0(dnd_url, monster)) %>%\n    enframe() %>%\n    pivot_wider(names_from = name,\n                values_from = value)\n  \n  return(ret)\n}\n\n#this gets all of the monster indices to plug into the fetch function\nmons <- fromJSON(dnd_base)$results %>%\n  pull(index)\n\nmonster_lists <- map(mons, fetch_monster)\n\nmons_bind <- bind_rows(monster_lists)\n\nglimpse(mons_bind)\n\nRows: 332\nColumns: 31\n$ index                  <list> \"aboleth\", \"acolyte\", \"adult-black-dragon\", \"a…\n$ name                   <list> \"Aboleth\", \"Acolyte\", \"Adult Black Dragon\", \"A…\n$ size                   <list> \"Large\", \"Medium\", \"Huge\", \"Huge\", \"Huge\", \"Hu…\n$ type                   <list> \"aberration\", \"humanoid\", \"dragon\", \"dragon\", …\n$ alignment              <list> \"lawful evil\", \"any alignment\", \"chaotic evil\"…\n$ armor_class            <list> 17, 10, 19, 19, 18, 19, 18, 19, 19, 19, 19, 18…\n$ hit_points             <list> 135, 9, 195, 225, 172, 212, 184, 256, 207, 256…\n$ hit_dice               <list> \"18d10\", \"2d8\", \"17d12\", \"18d12\", \"15d12\", \"17…\n$ speed                  <list> [\"10 ft.\", \"40 ft.\"], [\"30 ft.\"], [\"40 ft.\", \"…\n$ strength               <list> 21, 10, 23, 25, 23, 25, 23, 27, 23, 27, 27, 22…\n$ dexterity              <list> 9, 10, 14, 10, 10, 10, 12, 14, 12, 10, 10, 10,…\n$ constitution           <list> 15, 10, 21, 23, 21, 23, 21, 25, 21, 25, 25, 22…\n$ intelligence           <list> 18, 10, 14, 16, 14, 16, 18, 16, 18, 16, 16, 8,…\n$ wisdom                 <list> 15, 14, 13, 15, 13, 15, 15, 15, 15, 13, 13, 12…\n$ charisma               <list> 18, 11, 17, 19, 17, 19, 17, 24, 17, 21, 21, 12…\n$ proficiencies          <list> [<data.frame[5 x 2]>], [<data.frame[2 x 2]>], …\n$ damage_vulnerabilities <list> [], [], [], [], [], [], [], [], [], [], [], []…\n$ damage_resistances     <list> [], [], [], [], [], [], [], [], [], [], [], []…\n$ damage_immunities      <list> [], [], \"acid\", \"lightning\", \"fire\", \"lightnin…\n$ condition_immunities   <list> [], [], [], [], [], [], [], [], [<data.frame[1…\n$ senses                 <list> [\"120 ft.\", 20], [12], [\"60 ft.\", \"120 ft.\", 2…\n$ languages              <list> \"Deep Speech, telepathy 120 ft.\", \"any one lan…\n$ challenge_rating       <list> 10, 0.25, 14, 16, 13, 15, 14, 17, 15, 17, 16, …\n$ xp                     <list> 5900, 50, 11500, 15000, 10000, 13000, 11500, 1…\n$ special_abilities      <list> [<data.frame[3 x 3]>], [<data.frame[1 x 3]>], …\n$ actions                <list> [<data.frame[4 x 7]>], [<data.frame[1 x 4]>], …\n$ legendary_actions      <list> [<data.frame[3 x 4]>], [], [<data.frame[3 x 4]…\n$ url                    <list> \"/api/monsters/aboleth\", \"/api/monsters/acolyt…\n$ subtype                <list> <NULL>, \"any race\", <NULL>, <NULL>, <NULL>, <N…\n$ reactions              <list> <NULL>, <NULL>, <NULL>, <NULL>, <NULL>, <NULL>…\n$ forms                  <list> <NULL>, <NULL>, <NULL>, <NULL>, <NULL>, <NULL>…\n\n\nNotice that we have the same structure as in the previous example, but now with 322 rows instead of 1. Now we can take care of coercing some of these list columns into atomic vectors."
  },
  {
    "objectID": "posts/dungeons-and-dragons-part-1/index.html#restructuring-data",
    "href": "posts/dungeons-and-dragons-part-1/index.html#restructuring-data",
    "title": "Dungeons and Dragons - Part 1",
    "section": "Restructuring Data",
    "text": "Restructuring Data\nOne problem here, though, is that the possible variable values for each column differ depending on the monster (for some variables). Variables like strength, hit points, challenge rating, and xp will always be a single integer value, but variables like legendary_actions can differ greatly. People who play D&D will know that normal monsters don’t have any legendary actions, and so this will be NULL for those monsters. But some monsters might have 1 or 2 legendary actions, whereas big baddies like ancient dragons can have several. This same varying structure applies to columns like proficiencies, special abilities, reactions, etc. Ultimately, this means that a list column is probably the best way to represent this type of data, since lists are more flexible, whereas some columns can be represented as an atomic vector, and so we need to figure out how to address this.\nTo do this, we can write a couple of functions. The first, compare_lens() (below), will determine if the length of each element of a list is equal to whatever size we want to compare against (I’ve set the default to 1, which is what we want to use in this case). It then uses the all() function to determine if all of these comparisons are equal to TRUE, and will return a single value of TRUE if this is the case (and a single FALSE if not).\n\ncompare_lens <- function(x, size = 1) {\n  all(map_lgl(x, ~length(unlist(.x)) == size))\n}\n\nNext, we’ll use the compare_lens() function as the test expression in another function, cond_unlist (or conditionally unlist), below. The idea here is if compare_lens() is TRUE, then we will unlist the list (simplify it to a vector) passed to the function; otherwise, we’ll leave it as is (as a list). Putting these functions together, the logic is:\n\nDetermine if all elements of a list have a length equal to 1.\nIf so, turn that list into a vector.\nIf not, leave it as a list.\n\n\ncond_unlist <- function(x) {\n  if (compare_lens(x) == TRUE) {\n    unlist(x)\n  } else {\n    x\n  }\n}\n\nThe final step is to apply this function to all of the columns (which, recall, are lists) in our mons_bind tibble. We can do this using a combination of mutate() and across(). After doing this, we’ll see that some of the columns in our data frame have been simplified to character, integer, and double vectors, whereas others remain lists (lists of lists, lists of data frames).\n\nmons_df <- mons_bind %>%\n  mutate(across(.cols = everything(), ~cond_unlist(x = .x)))\n\nglimpse(mons_df)\n\nRows: 332\nColumns: 31\n$ index                  <chr> \"aboleth\", \"acolyte\", \"adult-black-dragon\", \"ad…\n$ name                   <chr> \"Aboleth\", \"Acolyte\", \"Adult Black Dragon\", \"Ad…\n$ size                   <chr> \"Large\", \"Medium\", \"Huge\", \"Huge\", \"Huge\", \"Hug…\n$ type                   <chr> \"aberration\", \"humanoid\", \"dragon\", \"dragon\", \"…\n$ alignment              <chr> \"lawful evil\", \"any alignment\", \"chaotic evil\",…\n$ armor_class            <int> 17, 10, 19, 19, 18, 19, 18, 19, 19, 19, 19, 18,…\n$ hit_points             <int> 135, 9, 195, 225, 172, 212, 184, 256, 207, 256,…\n$ hit_dice               <chr> \"18d10\", \"2d8\", \"17d12\", \"18d12\", \"15d12\", \"17d…\n$ speed                  <list> [\"10 ft.\", \"40 ft.\"], [\"30 ft.\"], [\"40 ft.\", \"…\n$ strength               <int> 21, 10, 23, 25, 23, 25, 23, 27, 23, 27, 27, 22,…\n$ dexterity              <int> 9, 10, 14, 10, 10, 10, 12, 14, 12, 10, 10, 10, …\n$ constitution           <int> 15, 10, 21, 23, 21, 23, 21, 25, 21, 25, 25, 22,…\n$ intelligence           <int> 18, 10, 14, 16, 14, 16, 18, 16, 18, 16, 16, 8, …\n$ wisdom                 <int> 15, 14, 13, 15, 13, 15, 15, 15, 15, 13, 13, 12,…\n$ charisma               <int> 18, 11, 17, 19, 17, 19, 17, 24, 17, 21, 21, 12,…\n$ proficiencies          <list> [<data.frame[5 x 2]>], [<data.frame[2 x 2]>], …\n$ damage_vulnerabilities <list> [], [], [], [], [], [], [], [], [], [], [], []…\n$ damage_resistances     <list> [], [], [], [], [], [], [], [], [], [], [], []…\n$ damage_immunities      <list> [], [], \"acid\", \"lightning\", \"fire\", \"lightnin…\n$ condition_immunities   <list> [], [], [], [], [], [], [], [], [<data.frame[1…\n$ senses                 <list> [\"120 ft.\", 20], [12], [\"60 ft.\", \"120 ft.\", 2…\n$ languages              <chr> \"Deep Speech, telepathy 120 ft.\", \"any one lang…\n$ challenge_rating       <dbl> 10.00, 0.25, 14.00, 16.00, 13.00, 15.00, 14.00,…\n$ xp                     <int> 5900, 50, 11500, 15000, 10000, 13000, 11500, 18…\n$ special_abilities      <list> [<data.frame[3 x 3]>], [<data.frame[1 x 3]>], …\n$ actions                <list> [<data.frame[4 x 7]>], [<data.frame[1 x 4]>], …\n$ legendary_actions      <list> [<data.frame[3 x 4]>], [], [<data.frame[3 x 4]…\n$ url                    <chr> \"/api/monsters/aboleth\", \"/api/monsters/acolyte…\n$ subtype                <list> <NULL>, \"any race\", <NULL>, <NULL>, <NULL>, <N…\n$ reactions              <list> <NULL>, <NULL>, <NULL>, <NULL>, <NULL>, <NULL>…\n$ forms                  <list> <NULL>, <NULL>, <NULL>, <NULL>, <NULL>, <NULL>…\n\n\nAnd there we have it. Our data is now in a pretty good state for some analysis. Depending on what we’re interested in doing, we could also do some additional feature engineering on the list columns, but the choices there will be contingent on the analyses we want to do.\nFor my next blog in this series, I’ll use this data to do some exploratory analysis, which I hope to get to in the next week or so."
  },
  {
    "objectID": "posts/dungeons-and-dragons-part-2/index.html",
    "href": "posts/dungeons-and-dragons-part-2/index.html",
    "title": "Dungeons and Dragons - Part 2",
    "section": "",
    "text": "In my last blog post, I walked through how to extract and wrangle/rectangle monster data from the Dungeons and Dragons 5th edition (D&D 5e) API. In this post, I want to explore this data a little bit – looking at some counts, descriptive statistics, etc. In later posts, I’m planning to do some different statistical analyses, potentially include cluster analysis and some predictive modeling.\nOne note – the monsters represented in this data aren’t all of the monsters in D&D. The API I’m using has monsters from the systems reference document (SRD), and this doesn’t include all of the monsters introduced in specific campaigns or in books like Xanathar’s Guide to Everything.\nWith all of that said, let’s get to it."
  },
  {
    "objectID": "posts/dungeons-and-dragons-part-2/index.html#setup",
    "href": "posts/dungeons-and-dragons-part-2/index.html#setup",
    "title": "Dungeons and Dragons - Part 2",
    "section": "Setup",
    "text": "Setup\nTo start, I’m going to load in some packages and get the data. If you’re interested in the process for getting the data, you might want to check out my previous post in this series.\n\nlibrary(tidyverse)\nlibrary(eemisc)\nlibrary(jsonlite)\nlibrary(ggridges)\nlibrary(gt)\nlibrary(corrr)\n\nherm <- harrypotter::hp(n = 1, option = \"HermioneGranger\")\n\nopts <- options(\n  ggplot2.discrete.fill = list(\n    harrypotter::hp(n = 3, option = \"HermioneGranger\"),\n    harrypotter::hp(n = 7, option = \"Always\")\n  )\n)\n\ntheme_set(theme_ee())\n\ndnd_base <- \"https://www.dnd5eapi.co/api/monsters/\"\n\n### Functions and process for getting data; described in previous post\nfetch_monster <- function(monster) {\n  dnd_url <- \"https://www.dnd5eapi.co/api/monsters/\"\n  \n  ret <- fromJSON(paste0(dnd_url, monster)) %>%\n    enframe() %>%\n    pivot_wider(names_from = name,\n                values_from = value)\n  \n  return(ret)\n}\n\ncompare_lens <- function(x, size = 1) {\n  all(map_lgl(x, ~length(unlist(.x)) == size))\n}\ncond_unlist <- function(x) {\n  if (compare_lens(x) == TRUE) {\n    unlist(x)\n  } else {\n    x\n  }\n}\n\nmons <- fromJSON(dnd_base)$results %>%\n  pull(index)\n\nmonster_lists <- map(mons, fetch_monster)\n\nmons_bind <- bind_rows(monster_lists)\n\nmons_df <- mons_bind %>%\n  mutate(across(.cols = everything(), ~cond_unlist(x = .x)))"
  },
  {
    "objectID": "posts/dungeons-and-dragons-part-2/index.html#challenge-ratings-and-hit-points",
    "href": "posts/dungeons-and-dragons-part-2/index.html#challenge-ratings-and-hit-points",
    "title": "Dungeons and Dragons - Part 2",
    "section": "Challenge Ratings and Hit Points",
    "text": "Challenge Ratings and Hit Points\nFor whatever reason, the first thing that strikes me to look at is the monsters’ challenge ratings (CRs). As the name suggests, CRs are an indication of how difficult a monster is for a group of players to fight, with higher CRs corresponding to a more difficult fight. The general rule of thumb is that a party of players can fight monsters about equal to their own level (or lower), and that higher CR monsters could be pretty tough.\n\nmons_df %>%\n  ggplot(aes(x = challenge_rating)) +\n  geom_histogram(fill = herm, color = herm, alpha = .8) +\n  labs(\n    title = \"Distribution of Challenge Ratings\"\n  )\n\n\n\n\nThe general rule makes sense in conjunction with this distribution of CRs. Most of the action in D&D campaigns tends to occur in lower levels, so it makes sense that the majority of the monsters here have CRs < 10. We do see one dude hanging out at CR 30, though, so let’s see who that is.\n\nmons_df %>%\n  slice_max(order_by = challenge_rating, n = 1)\n\n# A tibble: 1 × 31\n  index name  size  type  alignment armor_class hit_points hit_dice speed       \n  <chr> <chr> <chr> <chr> <chr>           <int>      <int> <chr>    <list>      \n1 tarr… Tarr… Garg… mons… unaligned          25        676 33d20    <named list>\n# … with 22 more variables: strength <int>, dexterity <int>,\n#   constitution <int>, intelligence <int>, wisdom <int>, charisma <int>,\n#   proficiencies <list>, damage_vulnerabilities <list>,\n#   damage_resistances <list>, damage_immunities <list>,\n#   condition_immunities <list>, senses <list>, languages <chr>,\n#   challenge_rating <dbl>, xp <int>, special_abilities <list>, actions <list>,\n#   legendary_actions <list>, url <chr>, subtype <list>, reactions <list>, …\n\n\nRight, so we can see the monster with the highest challenge rating is the Tarrasque. That’s this guy.\n\nNext, let’s take a look at the distribution of monster hit points. For those unfamiliar with D&D/video games more broadly, hit points represent the amount of health a character/monster has, and reducing someone to 0 hit points will defeat them. Typically (and I’ll explore this relationship more momentarily), hit points will increase as CR increases.\n\nmons_df %>%\n  ggplot(aes(x = hit_points)) +\n  geom_histogram(fill = herm, color = herm, alpha = .8) +\n  labs(\n    title = \"Distribution of Monster Hit Points\"\n  )\n\n\n\n\nLet’s look a little bit more at descriptives for these previous two stats:\n\nsummary(mons_df[c(\"challenge_rating\", \"hit_points\")])\n\n challenge_rating   hit_points    \n Min.   : 0.000   Min.   :  1.00  \n 1st Qu.: 0.500   1st Qu.: 19.00  \n Median : 2.000   Median : 45.00  \n Mean   : 4.515   Mean   : 81.49  \n 3rd Qu.: 6.000   3rd Qu.:114.00  \n Max.   :30.000   Max.   :676.00  \n\n\nAnd, again, presumably there’s a strong correlation between them, but let’s check that as well.\n\ncor(mons_df$hit_points, mons_df$challenge_rating)\n\n[1] 0.9414071\n\n\nSo, we see a very strong correlation between hit points and challenge rating. Let’s plot this.\n\nmons_df %>%\n  ggplot(aes(x = hit_points, y = challenge_rating)) +\n  geom_point(color = herm) +\n  labs(\n    title = \"Monster Hit Points vs Challenge Rating\"\n  )\n\n\n\n\nYeah…that’s what we’d expect a strong correlation to look like. One thing to note is that, although it looks like the Tarrasque fits on the general trend line here, outliers can have a strong influence on the correlation coefficient, so I’ll do a quick check to see what the value would be if we didn’t include the Tarrasque.\n\nmons_df %>%\n  filter(name != \"Tarrasque\") %>%\n  select(hit_points, challenge_rating) %>%\n  cor() %>%\n  .[[1,2]]\n\n[1] 0.9409642\n\n\nOur correlation coefficient is pretty much identical to the previous, Tarrasque-included calculation, which makes sense given what we see in the scatterplot, but still a reasonable check to include."
  },
  {
    "objectID": "posts/dungeons-and-dragons-part-2/index.html#ability-scores",
    "href": "posts/dungeons-and-dragons-part-2/index.html#ability-scores",
    "title": "Dungeons and Dragons - Part 2",
    "section": "Ability Scores",
    "text": "Ability Scores\nAbility scores are central to D&D and are the thing I’m most interested in looking at here. A quick Google search will tell you all you want to know (& more) about ability scores, but as a quick tl;dr – ability scores represent a character’s (player, monster, or non-player-character) abilities in several different areas, and each does different things. Furthermore, different character classes will value different ability scores. Fighters, for example, will tend to value strength and constitution; rangers and rogues will value dexterity; wizards will value intelligence, etc. Characters’ ability scores affect how well they do in combat, how they cast spells, how well they can persuade others, whether or not they can successfully climb a trees, etc – they affect pretty much anything you want to do in the game. And, due to how ability scores are allocated, characters will not have high scores on every ability, and the differential prioritizations and limited availability makes the distributions (and relationships among the scores) interesting to me.\nSo, let’s first check out the distributions.\n\nabs <- c(\"strength\", \"charisma\", \"dexterity\", \"intelligence\", \"wisdom\", \"constitution\")\n\nab_scores <- mons_df %>%\n  select(name, all_of(abs)) %>%\n  pivot_longer(cols = 2:ncol(.),\n               names_to = \"ability\",\n               values_to = \"score\")\n\nab_scores %>%\n  ggplot(aes(x = score, y = ability, fill = ability)) +\n  geom_density_ridges(alpha = .7) +\n  labs(\n    title = \"Monster Ability Score Distributions\",\n    y = NULL\n  ) +\n  theme(\n    legend.position = \"none\"\n  )\n\n\n\n\nIt’s interesting to me that the distributions all have different shapes, and especially that wisdom appears to the be only ability score that has a normal distribution for monsters.\nAnother interesting question to me is to what extent are these correlated. To check this out, we could use base R’s cor() function, but I want to try the {corrr} package, which provides some helpers for doing correlation analyses.\n\nabs_corrs <- mons_df %>%\n  select(all_of(abs)) %>%\n  correlate() %>%\n  rearrange() %>%\n  shave()\n\nabs_corrs %>%\n  fashion() %>%\n  gt() %>%\n  tab_header(\n    title = \"Ability Score Correlations\",\n    subtitle = \"D&D 5e Monsters\"\n  )\n\n\n\n\n\n  \n    \n      Ability Score Correlations\n    \n    \n      D&D 5e Monsters\n    \n  \n  \n    \n      term\n      constitution\n      strength\n      charisma\n      intelligence\n      wisdom\n      dexterity\n    \n  \n  \n    constitution\n\n\n\n\n\n\n    strength\n .86\n\n\n\n\n\n    charisma\n .60\n .52\n\n\n\n\n    intelligence\n .51\n .42\n .90\n\n\n\n    wisdom\n .45\n .42\n .74\n .65\n\n\n    dexterity\n-.19\n-.23\n .21\n .21\n .36\n\n  \n  \n  \n\n\n\n\nWe see some interesting stuff here:\n\nDexterity has weak to moderate correlations with everything.\nExcept for dexterity, constitution has moderate to large correlations with everything. This makes sense, since constitution relates to how many hit points creatures have, and so we’d expect cons to increase with level, and so regardless of what a monster’s primary ability is, higher level monsters will likely have high cons scores.\nThe strongest correlations are cons:strength and intelligence:charisma, which both seem reasonable. Int/charisma are useful for spellcasting and spell saving throws (and so are likely to travel together in monster stat blocks), and strength/cons are likely going to travel together in the form of big beefy melee combat type monsters.\n\nWe can also look at the same data using the rplot() function in {corrr}, although it’s not super easy to see:\n\nabs_corrs %>%\n  rplot()\n\n\n\n\nBeyond looking at the distributions and correlations of these ability scores, I think it’s also worth it to just look at the means. This will give us a sense of what the average monster in D&D is like, stats-wise.\n\nab_scores %>%\n  group_by(ability) %>%\n  summarize(avg = mean(score)) %>%\n  mutate(ability = fct_reorder(ability, avg)) %>%\n  ungroup() %>%\n  ggplot(aes(x = avg, y = ability)) +\n  geom_col(fill = herm) +\n  geom_text(aes(label = round(avg, 1), x = avg - .2), hjust = 1, color = \"white\") +\n  labs(\n    x = \"Average Ability Score\",\n    y = NULL,\n    title = \"Average Monster Ability Scores in D&D 5e\"\n  )\n\n\n\n\nSo, on average, monsters tend to be stronger and have higher constitutions, but have lower intelligence. Which makes sense if we look at the distributions again.\nFinally, we might want to look at which monster scores the highest on each ability score.\n\nab_scores %>%\n  select(ability, name, score) %>%\n  mutate(ability = str_to_title(ability)) %>%\n  group_by(ability) %>%\n  slice_max(order_by = score, n = 1) %>%\n  ungroup() %>%\n  gt() %>%\n  tab_header(\n    title = \"Highest Ability Scores\",\n    subtitle = \"...and the monsters that own them\"\n  ) %>%\n  cols_label(\n    name = \"Monster\",\n    ability = \"Ability\",\n    score = \"Score\"\n  ) %>%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels(columns = everything())\n  )\n\n\n\n\n\n  \n    \n      Highest Ability Scores\n    \n    \n      ...and the monsters that own them\n    \n  \n  \n    \n      Ability\n      Monster\n      Score\n    \n  \n  \n    Charisma\nSolar\n30\n    Constitution\nTarrasque\n30\n    Dexterity\nWill-o'-Wisp\n28\n    Intelligence\nSolar\n25\n    Strength\nAncient Gold Dragon\n30\n    Strength\nAncient Red Dragon\n30\n    Strength\nAncient Silver Dragon\n30\n    Strength\nKraken\n30\n    Strength\nTarrasque\n30\n    Wisdom\nSolar\n25\n  \n  \n  \n\n\n\n\nOne thing to keep in mind is that ability scores are capped at 30, so we see a handful of abilities that, according to the game rules, cannot be any higher (cons, strength, and charisma). We also see multiple monsters hitting the strength cap, including our old friend the Tarrasque. And we see the Solar represented in 3 categories (charisma, int, wisdom). I wasn’t familiar with the Solar before making this table, so I looked it up, and it seems like a sword-welding angel, which is pretty cool."
  },
  {
    "objectID": "posts/dungeons-and-dragons-part-2/index.html#other-questions",
    "href": "posts/dungeons-and-dragons-part-2/index.html#other-questions",
    "title": "Dungeons and Dragons - Part 2",
    "section": "Other Questions",
    "text": "Other Questions\nI’ll cap this post off by looking at a few odds and ends. This one might be a little silly, but since the game is called Dungeons and Dragons, it might be relevant to see how many monsters are actually dragons.\n\nmons_df %>%\n  mutate(is_dragon = if_else(type == \"dragon\", \"Dragon\", \"Not Dragon\")) %>%\n  count(is_dragon) %>%\n  ggplot(aes(x = n, y = is_dragon)) +\n  geom_col(fill = herm) +\n  geom_text(aes(label = n, x = n - 3), hjust = 1, color = \"white\", fontface = \"bold\") +\n  labs(\n    title = \"More Dungeons than Dragons\",\n    subtitle = \"Most monsters in D&D 5e are not dragons\",\n    y = NULL,\n    x = \"Monster Count\"\n  )\n\n\n\n\nGiven the above, we might be interested in seeing which monster types are the most common:\n\nmons_df %>%\n  count(type) %>%\n  ggplot(aes(x = n, y = fct_reorder(type, n))) +\n  geom_col(fill = herm) +\n  geom_text(aes(label = n, x = n - 1), hjust = 1, color = \"white\", fontface = \"bold\") +\n  labs(\n    title = \"Most Common Monster Types\",\n    subtitle = \"D&D 5e\",\n    y = NULL,\n    x = \"Monster Count\"\n  )\n\n\n\n\nWe can see here that beasts are the most common monster type, followed by humanoids and then dragons. I’m not really sure what’s going on with the “swarm of tiny beasts,” so let’s take a peek at those.\n\nmons_df %>%\n  filter(str_detect(type, \"swarm\")) %>%\n  pull(name)\n\n [1] \"Swarm of Bats\"             \"Swarm of Beetles\"         \n [3] \"Swarm of Centipedes\"       \"Swarm of Insects\"         \n [5] \"Swarm of Poisonous Snakes\" \"Swarm of Quippers\"        \n [7] \"Swarm of Rats\"             \"Swarm of Ravens\"          \n [9] \"Swarm of Spiders\"          \"Swarm of Wasps\"           \n\n\nI suppose that makes sense – there are stat blocks for swarms of bats, spiders, etc.\nThe last thing I’m going to check out here is the experience points (xp) distributions. My sense is that it’ll look similar to the distributions for challenge rating and hit points, since tougher monsters will award more experience for beating them. But still worth checking out.\n\nmons_df %>%\n  ggplot(aes(x = xp)) +\n  geom_histogram(fill = herm, color = herm, alpha = .8) +\n  labs(\n    title = \"Distribution of XP from Monsters\"\n  )\n\n\n\n\nSo this plot isn’t great since the Tarrasque is worth over 150k xp, which makes it harder to see the distribution at the lower end. There are also a few monsters that award ~50k xp, which again also makes it difficult to distinguish values at the lower end. I’ll filter put this on a log scale to make it easier to see the lower end.\n\nmons_df %>%\n  ggplot(aes(x = xp)) +\n  geom_histogram(fill = herm, color = herm, alpha = .8) +\n  labs(\n    title = \"Distribution of XP from Monsters\"\n  ) +\n  scale_x_log10()\n\n\n\n\nThis is a lot easier to read – we can see that the modal value for xp is maybe 500ish, although it’s also quite common for monsters to award 100xp or less. Monsters that award over 10k xp are pretty rare.\nAnd that’ll be it for this one. In the next blog post in this series, I’ll likely do some sort of clustering – probably latent profile analysis because I want to brush back up on it – to examine different groups of monsters based on their ability scores."
  },
  {
    "objectID": "posts/predicting-mobile-phone-subscription-growth/index.html",
    "href": "posts/predicting-mobile-phone-subscription-growth/index.html",
    "title": "Predicting Mobile Phone Subscription Gorwth",
    "section": "",
    "text": "I’ll keep the intro short and sweet for this one. A few weeks ago, I watched this screencast from Julia Silge in which she used splines to model the relationship between wins and seed in the Women’s NCAA Tournament. I don’t have a ton of experience using splines, and so that screencast made me want to learn a bit more and practice using them myself. Lo and behold, the phone subscription data from week 46 (2020) of #TidyTuesday seemed like a pretty good opportunity, so that’s what I’m doing here. More specifically, I’m using splines to model the relationship between year and the number of mobile phone subscriptions per 100 people across different continents and then investigating which countries these models perform best and worst for.\nLet’s get right into it, then."
  },
  {
    "objectID": "posts/predicting-mobile-phone-subscription-growth/index.html#setup",
    "href": "posts/predicting-mobile-phone-subscription-growth/index.html#setup",
    "title": "Predicting Mobile Phone Subscription Gorwth",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(eemisc)\nlibrary(harrypotter)\nlibrary(splines)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(kableExtra)\n\nherm <- harrypotter::hp(n = 1, option = \"HermioneGranger\")\n\nopts <- options(\n  ggplot2.discrete.fill = list(\n    harrypotter::hp(n = 3, option = \"HermioneGranger\"),\n    harrypotter::hp(n = 5, option = \"Always\")\n  )\n)\n\ntheme_set(theme_ee())\n\nmobile <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-11-10/mobile.csv')"
  },
  {
    "objectID": "posts/predicting-mobile-phone-subscription-growth/index.html#very-brief-exploration",
    "href": "posts/predicting-mobile-phone-subscription-growth/index.html#very-brief-exploration",
    "title": "Predicting Mobile Phone Subscription Gorwth",
    "section": "Very Brief Exploration",
    "text": "Very Brief Exploration\nSo, I kinda already know what I want to do, so I’m going to keep the exploration pretty minimal here. I will plot the number of mobile phone subscriptions for each country over time, though, just so I can get some feel for the relationships.\n\nmobile %>%\n  ggplot(aes(x = year, y = mobile_subs, group = entity)) +\n  geom_line(alpha = .7, color = herm) +\n  facet_wrap(~ continent)\n\n\n\n\nThe most apparent takeaway here to me is that the relationship between year and mobile phone subscriptions is not linear – it looks sigmoidal to me. It also seems to differ by continent (although that may just be an artifact of faceting the continents).\nEither way, I got into this to do some splines, so that’s what I’m going to do. First, let’s do a very brief and not statistically rigorous overview of what a spline is (n.b. for a better summary, go here). At a very high level, splines allow us to estimate flexible models to data. They do this by allowing us to include “knots” in our regression models and fitting smooth functions that model the data between consecutive knots. The person building the model can specify the number of knots (or degrees of freedom) they want to include in the model. Including more knots makes the function more flexible (but also, maybe obviously, increases model complexity), whereas including fewer knots makes the model simpler but less flexible.\nLet’s try plotting a few different splines to the full dataset here to illustrate this.\n\nplot_spline <- function(df) {\n  ggplot(mobile, aes(x = year, y = mobile_subs)) +\n    geom_smooth(\n      method = lm,\n      se = FALSE,\n      formula = y ~ ns(x, df = df),\n      color = herm\n    ) +\n    labs(\n      title = glue::glue(\"{ df } degrees of freedom\")\n    ) +\n    theme_minimal()\n}\n\nplots_list <- map(c(2, 3, 4, 6, 8, 10), plot_spline)\n\nwrap_plots(plots_list)\n\n\n\n\nNote that the number of knots is equal to 1 - the degrees of freedom. Anyway – looking at this, we can see noticeable differences between the models wtih 2, 3, and 4 degrees of freedom, but very little difference once we get beyond that, which makes me think that a 4 df spline is the way to go. I could more rigorously tune the degrees of freedom by fitting models with each and comparing the accuracy on holdout data, but visually examining it feels good enough here."
  },
  {
    "objectID": "posts/predicting-mobile-phone-subscription-growth/index.html#fitting-models",
    "href": "posts/predicting-mobile-phone-subscription-growth/index.html#fitting-models",
    "title": "Predicting Mobile Phone Subscription Gorwth",
    "section": "Fitting Models",
    "text": "Fitting Models\nNow, let’s fit a spline model for each continent. To do this, I’m going to first filter down to only the countries that have mobile phone data for every year in the dataset (1990 - 2017). Next, I’m going to create a nested tibble for each continent using the nest() function, fit a spline for each continent using a combination of map(), lm(), and ns() (which is a function for natural splines). Finally, I’m going to use glance() from the {broom} package to get the R-squared for each continent’s model. I like this workflow for fitting multiple models because it keeps everything together in a tibble and really facilitates iterating with map().\n\ncomplete_countries <- mobile %>%\n  group_by(entity) %>%\n  summarize(miss = sum(is.na(mobile_subs))) %>%\n  filter(miss == 0) %>%\n  pull(entity)\n\ncontinent_df <- mobile %>%\n  select(continent, entity, year, mobile_subs) %>%\n  filter(entity %in% complete_countries) %>%\n  group_by(continent) %>%\n  nest() %>%\n  mutate(model = map(data, ~lm(mobile_subs ~ ns(year, df = 4), data = .x)),\n         rsq = map_dbl(model, ~glance(.x) %>% pull(1)))\n\ncontinent_df\n\n# A tibble: 5 × 4\n# Groups:   continent [5]\n  continent data                 model    rsq\n  <chr>     <list>               <list> <dbl>\n1 Asia      <tibble [1,202 × 3]> <lm>   0.689\n2 Europe    <tibble [1,119 × 3]> <lm>   0.869\n3 Africa    <tibble [1,146 × 3]> <lm>   0.684\n4 Americas  <tibble [846 × 3]>   <lm>   0.822\n5 Oceania   <tibble [221 × 3]>   <lm>   0.628\n\n\nSo, the R-squared values here seem pretty good considering we’re only using year as a predictor. In Europe we’re getting .86, which seems very high, and suggests that most countries follow similar trajectories (which we can see in the very first plot above). What could be interesting, though, is to see which country in each continent this model performs best on and which it performs worst on. This will give us a sense of what the most “typical” country is (the country that most closely follows the overall continent model) and what the most atypical country is (the country that least closely follows the overall continent model) in each continent."
  },
  {
    "objectID": "posts/predicting-mobile-phone-subscription-growth/index.html#examining-predictions-accuracy",
    "href": "posts/predicting-mobile-phone-subscription-growth/index.html#examining-predictions-accuracy",
    "title": "Predicting Mobile Phone Subscription Gorwth",
    "section": "Examining Predictions & Accuracy",
    "text": "Examining Predictions & Accuracy\nTo do this, I’m first going to predict values for each observation (each year for each country) using augment(), again from {broom}. I’m then going to do a little bit of binding and straightening up, ending by unnesting the data.\n\npreds_data <- continent_df %>%\n  ungroup() %>%\n  mutate(preds = map(model, ~augment(.x) %>% select(.fitted)),\n         joined_data = map2(data, preds, bind_cols)) %>%\n    select(joined_data, \n         continent_rsq = rsq,\n         continent) %>%\n    unnest(joined_data)\n\nNext, I’m going to calculate the R-squared for each country. There’s probably another way to do this, but it’s a pretty easy calculation to do by hand, so I’m just going to do that.\n\nrsq_df <- preds_data %>%\n  group_by(entity) %>%\n  mutate(avg = mean(mobile_subs),\n         res = (mobile_subs - .fitted)^2,\n         tot = (mobile_subs - .fitted)^2 + (.fitted - avg)^2) %>%\n  summarize(country_rsq = 1 - (sum(res)/sum(tot))) %>%\n  ungroup() %>%\n  left_join(x = preds_data %>% distinct(entity, continent), y = ., by = \"entity\")\n\nNow, I’m going to pick filter down to the countries in each continent that have the highest R-squared (the country the model performs best on) and the lowest R-squared (the country the model performs worst on). One thing to note is that a low R-squared doesn’t mean the country has few mobile phone subscriptions, it just means that the model does a relatively bad job predicting the mobile phone subscriptions for that country. This could be for a number of reasons, only one of which is that the country has considerably fewer subscriptions each year.\n\nselected_rsq <- rsq_df %>%\n  group_by(continent) %>%\n  filter(country_rsq == max(country_rsq) | country_rsq == min(country_rsq)) %>%\n  mutate(type = if_else(country_rsq == max(country_rsq), \"best fit\", \"worst fit\")) %>%\n  select(continent, entity, country_rsq, type) %>%\n  arrange(continent, country_rsq)\n\nselected_rsq %>%\n  select(-type) %>%\n  kable(format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"condensed\", \"striped\"))\n\n\n\n \n  \n    continent \n    entity \n    country_rsq \n  \n \n\n  \n    Africa \n    Seychelles \n    0.5109688 \n  \n  \n    Africa \n    Kenya \n    0.9895082 \n  \n  \n    Americas \n    Curacao \n    0.4431118 \n  \n  \n    Americas \n    Paraguay \n    0.9864706 \n  \n  \n    Asia \n    Hong Kong \n    0.5080236 \n  \n  \n    Asia \n    Philippines \n    0.9916221 \n  \n  \n    Europe \n    Moldova \n    0.6943629 \n  \n  \n    Europe \n    Faeroe Islands \n    0.9868045 \n  \n  \n    Oceania \n    Kiribati \n    0.5775901 \n  \n  \n    Oceania \n    French Polynesia \n    0.9451227 \n  \n\n\n\n\n\nRight, so, for example, we can see that the model fit using Europe’s data does the worst job predicting for Moldova and the best job for the Faeroe Islands.\nFinally, let’s take a look at these best- and worst-fitting countries graphically\n\nuse_countries <- pull(selected_rsq, entity)\n\nmobile_small_joined <- mobile %>%\n  filter(entity %in% use_countries) %>%\n  left_join(selected_rsq, by = c(\"entity\", \"continent\"))\n\nlabel_df <- mobile_small_joined %>%\n  group_by(entity) %>%\n  filter(mobile_subs == max(mobile_subs)) %>%\n  ungroup()\n  \nmobile %>%\n  filter(entity %in% use_countries) %>%\n  left_join(selected_rsq, by = c(\"entity\", \"continent\")) %>%\n  ggplot(aes(x = year, y = mobile_subs, group = entity, color = type)) +\n  geom_line() +\n  geom_text(data = label_df, aes(label = entity), x = max(mobile$year), hjust = 1, fontface = \"bold\", show.legend = FALSE) +\n  facet_wrap(~ continent) +\n  scale_color_hp_d(option = \"HermioneGranger\",\n                   name = \"Model Fit\") +\n  labs(\n    title = \"Best and Worst Fitting Models\"\n  )\n\n\n\n\nLet’s take a look at Hong Kong. It has the worst fit for the Asian model, but that’s because Hong Kong has way more mobile phones per person than other countries in Asia. On the other hand, we see that Kiribati (in Oceania) has way fewer than the model would predict. Both have (relatively) poor accuracy.\nOkie dokie, I think that’s it for now. Hopefully this is helpful for others in dipping your toes into spline models but also for demonstrating a workflow for nesting data and using map() along with some other functions to fit and interrogate multiple models."
  }
]