[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am an educator, researcher, data scientist, and writer. I am currently an Education Data Specialist for Chesterfield County Public Schools, where I use data to help guide decisions and improve students’ educational experiences. In my free time, I enjoy reading, writing, data visualization/analysis, going to the gym, and spending time with friends & family (including my pup, Nala).\nI earned my PhD in educational psychology from the VCU School of Education in July 2019, where I worked in the Motivation in Context Lab and in the Discourse and Learning Lab to study students’ motivation, behaviors, and achievement. In my dissertation, I studied how writers’ daily emotional states (think: anxiety, enjoyment, boredom) influenced their productivity as well as the day-to-day inertia of these emotional states.\nDuring grad school, I discovered a love for working with data – exploring, visualizing, modeling, and communicating. I mostly work (and blog) in R, but I’m always learning new languages and technologies. Well, as much as I can with a toddler and an infant in the house."
  },
  {
    "objectID": "about.html#projects-and-publications",
    "href": "about.html#projects-and-publications",
    "title": "About Me",
    "section": "Projects and Publications",
    "text": "Projects and Publications\nI’ve published some stuff – both academic and not – which you can find on my publications page."
  },
  {
    "objectID": "about.html#consulting-and-contracted-work",
    "href": "about.html#consulting-and-contracted-work",
    "title": "About Me",
    "section": "Consulting and Contracted Work",
    "text": "Consulting and Contracted Work\nTime permitting, I occasionally do some consulting and contract work. Although I specialize in educational research, I’m generally proficient with a wide range of statistical modeling frameworks as well as data visualization and dashboard design (via {shiny}), among other things. If you’re interested in working with me, send me an email."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "RVA\n\n\nGeography\n\n\nEDA\n\n\n\n\nAnalyzing pet ownership in RVA\n\n\n\n\n\n\nApr 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPuzzles\n\n\nCoding Challenge\n\n\n\n\nSolving a math puzzle and exploring the accumulate() function\n\n\n\n\n\n\nMar 31, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nThe Office\n\n\nEDA\n\n\nText Analysis\n\n\n\n\nAn initial exploration of dialogue from The Office\n\n\n\n\n\n\nMar 14, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Eric Ekholm",
    "section": "",
    "text": "I am an educator, researcher, data scientist, and writer. I am currently an Education Data Specialist for Chesterfield County Public Schools, where I use data to help guide decisions and improve students’ educational experiences."
  },
  {
    "objectID": "posts/scrantonicity-part-1/index.html",
    "href": "posts/scrantonicity-part-1/index.html",
    "title": "Scratonicity - Part 1",
    "section": "",
    "text": "Who doesn’t love The Office? I went through high school and college following the on-again off-again romance of Jim and Pam, the Icarus-esque ascendancy and fall of Ryan the Temp, and the perpetual cringey-ness of Michael Scott. And aside from that handful of people who fled the room in a cold panic at even the mention of “Scott’s Tots,” I think this was probably true for most of my generation. You’d be hard pressed to go to a Halloween party in the late aughts without seeing someone dressed in the tan-and-yellow palette of Dwight Schrute, and before the modern era of Netflix and Hulu, we regularly set aside Thursday nights to tune into NBC.\nAnd although I was a big Office fan several years ago, I haven’t thought too too much about it recently – at least until I stumbled across the release of the {schrute} package recently. {schrute} is an R package with one purpose – presenting the entire transcripts of The Office in tibble format, making the dialogue of the show much easier to analyze. I played around with the package and a quick sentiment analysis back in December when I looked at the sentiments expressed by Jim and Pam over the course of the series:\n\n\n\n\n\nThere’s a ton more we can do with the package, though, and with the transcripts available and in a clean format, plus all of the tools R has available for text analysis, I figured I’d do a mini-series of blog posts analyzing some of the data. The plan (as of now) is to start this first post with some exploratory analyses and visualizations, then move into some other types of modeling in later posts. I’ll also include all of my code throughout.\n\nAs a quick aside, a lot of the text analyses I’m going to work through in this first post come from the Text Mining with R book by Julia Silge and David Robinson. I’d strongly recommend this to anyone looking to dive into analyzing text data."
  },
  {
    "objectID": "pubs.html",
    "href": "pubs.html",
    "title": "Publications",
    "section": "",
    "text": "Peer-Reviewed Publications\nBack when I was more actively involved in academia, I published some papers, mostly related to student motivation, how people write, or statistical methodology. Publishing peer-reviewed work has been less of a priority for me lately, but I still dabble here and there.\n\nMarrs, S., Zumbrunn, S., & Ekholm, E. (2022). Measuring students’ perceptions of writing feedback. Current Research in Psychology and Behavioral Science, 3(4).\nZumbrunn, S., Ekholm, E., Broda, M., & Koenka, A. C. (2022). Trajectories of students’ writing feedback attitudes. Journal of Experimental Education. https://doi.org/10.1080/00220973.2022.2064413\nChow, J., Ekholm, E., & Bae, C. (2021). Relative contribution of verbal working memory and attention to child language. Assessment for Effective Intervention, 47(1), 3-13.\nBroda, M., Ekholm, E., & Zumbrunn, S. (2020). Assessing the predictive nature of teacher and student writing self-regulation discrepancy. Frontline Learning Research, 8(4), 52.\nGellock, J., Ekholm, E., Greenhalgh, G., LeCrom, C., Reina, C., & Kudesia, R. (2019). Women’s lacrosse players’ perceptions of teammate leadership: Examining athlete leadership behaviors, attributes, and interactions. Journal of Athlete Development and Experience, 1(2), 2. https://scholarworks.bgsu.edu/jade/vol1/iss2/2.\nZumbrunn, S., Marrs, S., Broda, M., Ekholm, E., Jackson, L., & DeBusk-Lane, M. (2019). Toward a more complete understanding of writing enjoyment: A mixed methods study of elementary students. AERA Open, 5(2), https://doi.org/10.1177/2332858419850792.\nBroda, M., Tucker, S., Ekholm, E., Johnson, T., & Liang, Q. (2019). Small fingers, big data: Preschoolers’ subitizing speed and accuracy during interactions with multi-touch technology. The Journal of Educational Research, 112(2), 211-222. https://doi.org/10.1080/00220671.2018.1486281.\nEkholm, E. & Chow. J. C. (2018). Addressing publication bias in educational psychology. Translational Issues in Psychological Science, 4(4), 425-439. http://dx.doi.org/10.1037/tps0000181.\nBroda, M., Ekholm, E., Schneider, B., & Hutton, A. (2018). Teachers’ social networks, college-going practices, and the diffusion of a school-based reform initiative. SAGE Open. https://doi.org/10.1177/2158244018817397.\nChow, J.C., Ekholm, E., & Coleman, H. (2018). Does oral language underpin the development of later behavior problems? A longitudinal meta-analysis. School Psychology Quarterly, 33(3), 337-349. http://dx.doi.org/10.1037/spq0000255.\nChow, J.C., & Ekholm, E. (2018). Do published studies yield larger effect sizes than unpublished studies in education and special education? A meta-review. Educational Psychology Review, 30(3), 727-744. https://doi.org/10.1007/s10648-018-9437-7.\nChow, J. C., & Ekholm, E. (2018). Language domains differentially predict mathematics performance in young children. Early Childhood Research Quarterly, 46, 179-186. https://doi.org/10.1016/j.ecresq.2018.02.011\nEkholm, E., Zumbrunn, S. & DeBusk-Lane, M. (2018). Clarifying an elusive construct: A systematic review of writing attitudes. Educational Psychology Review, 30(3), 827-856. https://doi.org/10.1007/s10648-017-9423-5.\nEkholm, E. (2017). Ethical concerns of using texts involving death in the English classroom. English Journal, 107(2), 25-30.\nZumbrunn, S., Ekholm, E., Stringer, J. K., McKnight, K., & DeBusk-Lane, M. (2017). Student experiences with writing: Taking the temperature of the classroom. The Reading Teacher, 70(6), 667-677. https://doi.org/10.1002/trtr.1574.\nEkholm, E., Zumbrunn, S., & Conklin, S. (2015). The relation of college student self-efficacy toward writing and writing self-regulation aptitude: writing feedback perceptions as a mediating variable. Teaching in Higher Education, 20(2), 197-207. https://doi.org/10.1080/13562517.2014.974026.\n\n\n\n\nOther Publications\n\nFiction and Humor\n\nEkholm, E. “If You Take My Survey, I’ll Let You Watch Me Cut My Finger Off.” McSweeney’s Internet Tendency (June 7, 2016).\nEkholm, E. “Call for Submissions for APA’s Annual Conference, Sponsored by Shark Week.” McSweeney’s Internet Tendency (August 11, 2014).\n\n\n\nBook Reviews\n\nEkholm, E. (September, 2016). A review of Kelly Gallagher’s In the Best Interests of Students.\n\n\n\nOther\n\nGuest blog post, AERA Motivation SIG Blog, Ekholm, E. (2018). “What Motivation Researchers Can Learn from Professional Sports Teams.” November 6, 2018.\n\n\n\n\n\nConference Presentations & Invited Talks\nOnly conference presentations since 2018 included\n\nEkholm, E. (May, 2022). Data Viz Tips & Tricks. Invited talk presented to the Educational Research & Evaluation Network (EREN).\nEkholme, E., & Fox, P. (October, 2021). Using machine learning to support student attendance practices. Session presented at the annual Metropolitan Education Research Consortium (MERC) conference, Richmond, VA. Video available here (~27:30 timestamp).\nEkholm, E., & Zumbrun, S. (April, 2020). Investigating relations between writers’ emotional experiences and attention regulation: A daily diary study. Paper presented at the American Educational Research Association (AERA) Annual Meeting, San Francisco, CA.\nEkholm, E., Zumbrunn, S., Broda, M., & Luther, T. C. (April, 2019). The development of student writing feedback attitudes in grades 3-7: A latent growth analysis. Poster presented at the American Educational Research Association (AERA) Annual Meeting, Toronto, Canada.\nZumbrunn, S., Broda, M., Marrs, S., & Ekholm, E. (April, 2019). The complexities and challenges of understanding individual differences and the development of multiple dimensions of writing self-efficacy. Poster presented at the American Educational Research Association (AERA) Annual Meeting, Toronto, Canada.\nBroda, M., Ekholm, E., Hutton, C., & Schneider, B. (April, 2019). Teachers’ social networks, college-going practices, and the diffusion of a school-based reform initiative. Paper presented at the American Educational Research Association (AERA) Annual Meeting, Toronto, Canada.\nZumbrunn, S., Broda, M., Ekholm, E., & Luther, T. C. (April, 2019). RoboCogger: Using mobile technology to assess and increase student writing metacognition, motivation, and performance. Paper presented at the American Educational Research Association (AERA) Annual Meeting, Toronto, Canada.\nBroda, M., Ekholm, E., & Zumbrunn, S. (October, 2018). Are we grading the “write” stuff? The relationship between teachers’ expectations, students’ self-regulation, and writing achievement. Paper presented at the CREATE Conference, Williamsburg, VA.\nZumbrunn, S., Marrs, S., Malmberg, L., Ekholm, E., Broda, M., & DeBusk-Lane, M. (August, 2018). Individual differences and the development of multiple dimensions of writing self-efficacy. Paper presented at the International Conference of the EARLI Special Interest Group on Writing, Antwerp, Belgium.\nEkholm, E., Zumbrunn, S., Hope, S., & Stim, H. (April, 2018). Teachers’ writing beliefs and instructional practices: A mixed-methods study. Poster presented at the American Educational Research Association (AERA) Annual Meeting, New York City, NY.\nMarrs, S., Ekholm, E., & Zumbrunn, S. (April, 2018). Exploring profiles of student perceptions of writing feedback. Roundtable presented at the American Educational Research Association (AERA) Annual Meeting, New York City, NY.\nTucker, S., Broda, M., Ekholm, E., Johnson, T., & Liang, Q. (April, 2018). Preschoolers’ subitizing speed and accuracy during interactions with multi-touch technology. Roundtable presented at the American Educational Research Association (AERA) Annual Meeting, New York City, NY.\nChow, J.C. & Ekholm, E. (February, 2018). Estimating the magnitude of the difference between published and unpublished studies in special education: A metareview. Poster presented at the Council for Exceptional Children Annual Convention, Tampa, FL."
  },
  {
    "objectID": "posts/scrantonicity-part-1/index.html#setup",
    "href": "posts/scrantonicity-part-1/index.html#setup",
    "title": "Scratonicity - Part 1",
    "section": "Setup",
    "text": "Setup\nFirst, let’s read in the data. I’m also going to limit the data to the first seven seasons, which spans the “Michael Scott” era. Not only because these are the best seasons (which they undoubtedly are), but also because doing so eliminates a major confounding factor (i.e. Steve Carell leaving the show) from the analysis.\n\noffice <- theoffice %>%\n  filter(as.numeric(season) <= 7)\n\nglimpse(office)\n\nRows: 41,348\nColumns: 12\n$ index            <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16…\n$ season           <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ episode          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ episode_name     <chr> \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\",…\n$ director         <chr> \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis…\n$ writer           <chr> \"Ricky Gervais;Stephen Merchant;Greg Daniels\", \"Ricky…\n$ character        <chr> \"Michael\", \"Jim\", \"Michael\", \"Jim\", \"Michael\", \"Micha…\n$ text             <chr> \"All right Jim. Your quarterlies look very good. How …\n$ text_w_direction <chr> \"All right Jim. Your quarterlies look very good. How …\n$ imdb_rating      <dbl> 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6…\n$ total_votes      <int> 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706,…\n$ air_date         <fct> 2005-03-24, 2005-03-24, 2005-03-24, 2005-03-24, 2005-…\n\n\nJust to check that the data we have matches what we’re expecting, let’s take a look at which seasons we have, plus how many episodes we have per season.\n\noffice %>%\n  distinct(season, episode) %>%\n  count(season, name = \"num_episodes\")\n\n# A tibble: 7 × 2\n  season num_episodes\n   <int>        <int>\n1      1            6\n2      2           22\n3      3           23\n4      4           14\n5      5           26\n6      6           24\n7      7           24\n\n\nThis generally matches what Wikipedia is telling me once we account for two-part episodes, and we can see that we only have the first seven seasons.\n\nMe think, why waste time say lot word, when few word do trick\nA few questions we can ask here involve how much/how often different characters speak. Probably the most basic question is: who has the most lines?\n\ntop_20_chars <- office %>%\n  count(character, sort = TRUE) %>%\n  top_n(20) %>%\n  pull(character)\noffice %>%\n  filter(is.element(character, top_20_chars)) %>%\n  count(character, sort = TRUE) %>%\n  ggplot(aes(x = fct_reorder(character, n), y = n)) +\n  geom_col(fill = purple) +\n  labs(\n    x = \"\",\n    y = \"Number of Lines\",\n    title = \"Who Has the Most Lines?\"\n  ) +\n  coord_flip()\n\n\n\n\nIt’s not surprising to me that Michael has the most lines, but the magnitude of the difference between him and Dwight is a bit surprising.\nWhat if we look at the number of lines per season?\n\noffice %>%\n  filter(is.element(character, top_20_chars)) %>%\n  count(character, season, sort = TRUE) %>%\n  ggplot(aes(x = as.numeric(season), y = n, color = character)) +\n    geom_line() +\n    geom_point()\n\n\n\n\nThis isn’t terribly informative – let’s go back to our bar graph.\n\noffice %>%\n  filter(is.element(character, top_20_chars)) %>%\n  count(character, season, sort = TRUE) %>%\n  group_by(season) %>%\n  top_n(n = 5) %>%\n  ungroup() %>%\n  ggplot(aes(x = fct_reorder(character, n), y = n)) +\n    geom_col(fill = purple) +\n    coord_flip() +\n    facet_wrap(~season, scales = \"free\") +\n    labs(\n      title = \"Number of Lines by Season\",\n      x = \"\",\n      y = \"\"\n    ) +\n    theme_minimal()\n\n\n\n\nAgain, not surprising that Michael has the most lines across all seasons. Dwight, Jim, and Pam are always the next three, but the orders change a bit between seasons. The fifth spot is where we see some movement, with Oscar and Jan sneaking in before Andy joins the show in Season 3. And check out Ryan in S4!\n\n\nSometimes I’ll start a sentence and I don’t even know where it’s going\nSo, above, we just looked at the number of lines each character had. Another option is to do some analyses at the word level. For instance, we can look at patterns of word usage for individual characters, between characters, and over time.\nTo start with this, I’m going to restructure the data so we have one word per row in our tibble. I’m also going to remove “stop words” (e.g. “a,” “the,” “at”), since these will show up a lot but (for our purposes) aren’t actually all that meaningful:\n\noffice_words <- office %>%\n  unnest_tokens(word, text) %>%\n  anti_join(stop_words)\nglimpse(office_words)\n\nRows: 125,040\nColumns: 12\n$ index            <int> 1, 1, 1, 2, 2, 3, 3, 3, 4, 4, 6, 6, 6, 6, 6, 6, 6, 6,…\n$ season           <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ episode          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ episode_name     <chr> \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\",…\n$ director         <chr> \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis…\n$ writer           <chr> \"Ricky Gervais;Stephen Merchant;Greg Daniels\", \"Ricky…\n$ character        <chr> \"Michael\", \"Michael\", \"Michael\", \"Jim\", \"Jim\", \"Micha…\n$ text_w_direction <chr> \"All right Jim. Your quarterlies look very good. How …\n$ imdb_rating      <dbl> 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6…\n$ total_votes      <int> 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706,…\n$ air_date         <fct> 2005-03-24, 2005-03-24, 2005-03-24, 2005-03-24, 2005-…\n$ word             <chr> \"jim\", \"quarterlies\", \"library\", \"told\", \"close\", \"ma…\n\n\nWe can see that we have a new column, word, with one word per row. We can also see that the only words in the first line of dialogue (All right Jim. Your quarterlies look very good. How are things at the library?) that make it through the stop words filter are jim, quarterlies, and library. We could fiddle with the stop words list if we wanted to keep words like “good” or “things,” but I’m not too concerned about that for now.\nAs a first pass, let’s take a look at our 20 characters with the most lines of dialogue and see what each character’s most commonly-used word is:\n\noffice_words %>%\n  filter(is.element(character, top_20_chars)) %>%\n  count(character, word, sort = TRUE) %>%\n  group_by(character) %>%\n  top_n(n = 1) %>%\n  kable(format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"condensed\", \"hover\"))\n\n\n\n \n  \n    character \n    word \n    n \n  \n \n\n  \n    Michael \n    yeah \n    563 \n  \n  \n    Dwight \n    michael \n    280 \n  \n  \n    Jim \n    yeah \n    274 \n  \n  \n    Pam \n    michael \n    257 \n  \n  \n    Jan \n    michael \n    159 \n  \n  \n    Andy \n    yeah \n    138 \n  \n  \n    Kevin \n    yeah \n    79 \n  \n  \n    David \n    michael \n    67 \n  \n  \n    Ryan \n    yeah \n    66 \n  \n  \n    Oscar \n    michael \n    65 \n  \n  \n    Phyllis \n    michael \n    59 \n  \n  \n    Toby \n    michael \n    50 \n  \n  \n    Darryl \n    na \n    48 \n  \n  \n    Kelly \n    god \n    44 \n  \n  \n    Angela \n    dwight \n    40 \n  \n  \n    Holly \n    yeah \n    39 \n  \n  \n    Erin \n    michael \n    37 \n  \n  \n    Karen \n    yeah \n    28 \n  \n  \n    Stanley \n    michael \n    27 \n  \n  \n    Meredith \n    wait \n    22 \n  \n\n\n\n\n\nSo, that’s not great. We can see that our stop words didn’t pick up “yeah.” One way around this would be to filter out additional words like “yeah,” “hey,” etc. that aren’t in our stop words list. But we’ll probably still leave out some common words that we might not want to show up in our exploration. A better approach is probably to use the tf-idf statistics (term frequency-inverse document frequency), which adjusts the weight a term is given in the analysis for each character by how commonly the word is used by all characters, with more common words getting lower weights. Essentially, this lets us figure out which words are important/unique to each of our characters.\n\noffice_words %>%\n  filter(is.element(character, top_20_chars)) %>%\n  count(character, word, sort = TRUE) %>%\n  bind_tf_idf(word, character, n) %>%\n  group_by(character) %>%\n  top_n(n = 5, wt = tf_idf) %>%\n  slice(1:5) %>%\n  ungroup() %>%\n  ggplot() +\n    geom_col(aes(x = reorder_within(word, tf_idf, within = character), y = tf_idf), fill = purple) +\n    facet_wrap(~character, scales = \"free\") +\n    coord_flip() +\n    scale_x_reordered() +\n    theme_minimal() +\n    labs(\n      x = \"\",\n      y = \"\",\n      title = \"Which Words are Important to Which Characters?\"\n    ) +\n    theme(\n      axis.text.x = element_blank()\n    )\n\n\n\n\nThis looks right – we see that “tuna” and “nard” are important to Andy, which totally makes sense. Some other gems in here are “wuphf” for Ryan, “wimowheh” for Jim, and “awesome” for Kevin.\nNext, let’s take a closer look at how Michael’s speech compares to some of the other main characters – Dwight, Jim, and Pam. We’ll also leave Kelly in here because I think she’ll be interesting to compare to Michael.\n\nmain_char_words <-  office_words %>%\n  filter(character %in% c(\"Michael\", \"Dwight\", \"Jim\", \"Pam\", \"Kelly\"),\n         str_detect(word, \"\\\\d+\", negate = TRUE)) %>%\n  count(character, word) %>%\n  group_by(character) %>%\n  mutate(word_prop = n/sum(n)) %>%\n  ungroup() %>%\n  select(-n) %>%\n  pivot_wider(names_from = character,\n              values_from = word_prop)\nchar_plot <- function(df, char) {\n  df %>%\n  select(word, Michael, {{char}}) %>%\n  mutate(color = log(abs(Michael-{{char}}))) %>%\n  ggplot(aes(y = Michael, x = {{char}})) +\n    geom_text(aes(label = word, color = color), check_overlap = TRUE, vjust = 1) +\n    geom_abline(color = \"grey50\", lty = 2) +\n    scale_x_log10(labels = scales::percent_format()) +\n    scale_y_log10(labels = scales::percent_format()) +\n    scale_color_distiller(\n      type = \"seq\",\n      palette = \"Purples\",\n      direction = 1\n    ) +\n    theme_minimal() +\n    theme(\n      legend.position = \"none\"\n    )\n}\nmain_char_words %>%\n  char_plot(Dwight)\n\n\n\n\nOk, so let’s walk through how to read this. For a given word, the y-axis shows how frequently Michael uses that word, and the x-axis shows how frequently Dwight uses that word. The diagonal dotted line represents equal usage – words that appear on or close to the line are words that Michael and Dwight use about as frequently as one another. Words above the line are those that Michael uses more; words below the line are those that Dwight uses more. Words closer to the line will appear lighter in the graph, whereas words farther way will have more color. So, looking at the graph, we can see that Dwight and Michael both say “hey” pretty often and use the word more or less equally. Dwight says “Mose” way more often than Michael does (because it’s farther from the line), whereas Michael says “Scott” more often than Dwight.\nLet’s take a look at what these graphs look like for Jim and Pam\n\nmain_char_words %>%\n  char_plot(Jim)\n\n\n\n\n\nmain_char_words %>%\n  char_plot(Pam)\n\n\n\n\nAand let’s throw Kelly in there too because it might be interesting.\n\nmain_char_words %>%\n  char_plot(Kelly)\n\n\n\n\nWhat we see here is that, at least when compared to Michael, Kelly’s speech is pretty idiosyncratic – there are lots of words (“blah”, “bitch”, “god”) that she uses waaaayy more frequently than Michael does.\nAnd finally (for this section), I would be remiss if I made it through an analysis of how characters from The Office speak without giving a “that’s what she said” tally…\n\noffice %>%\n  filter(str_detect(text, \"what she said\")) %>%\n  count(character) %>%\n  ggplot(aes(x = fct_reorder(character, n), y = n)) +\n    geom_col(fill = purple) +\n    labs(\n      x = \"\",\n      y = \"Count\",\n      title = \"That's What She Said!\"\n    ) +\n    coord_flip()\n\n\n\n\nNot at all a surprise….\n\n\nIdentity theft is not a joke, Jim!\nFinally, I want to visualize who characters talk to. To do this, I’m going to put together a network plot showing links between characters based on how frequently they interact.\n\nset.seed(0408)\noffice_links <- office %>%\n  filter(character %in% top_20_chars) %>%\n  group_by(episode) %>%\n  mutate(to = lead(character)) %>%\n  ungroup() %>%\n  rename(from = character) %>%\n  count(from, to) %>%\n  filter(from != to,\n         !is.na(to),\n         n > 25)\noffice_verts <- office_links %>%\n  group_by(from) %>%\n  summarize(size = log(sum(n), base = 2)) %>%\n  ungroup()\nnetwork_graph <- graph_from_data_frame(office_links, vertices = office_verts)\nnetwork_graph %>%\n  ggraph(layout = \"igraph\", algorithm = \"fr\") +\n  geom_edge_link(aes(edge_alpha = n^.5), color = purple, edge_width = 1) +\n  geom_node_point(aes(size = size, color = size)) +\n  geom_node_text(aes(label = name, size = size), repel = TRUE, family = \"Garamond\", fontface = \"bold\") +\n  scale_color_distiller(\n      type = \"seq\",\n      palette = \"Purples\",\n      direction = 1\n    ) +\n  labs(\n    title = \"Who Talks to Whom in The Office?\"\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(hjust = .5)\n  )\n\n\n\n\nThe network graph shows links between characters. The size and color of the node (point) associated with a person corresponds to the the total number of interactions they have, with larger and purple-r nodes representing more interactions. The color of the link between characters also corresponds to the number of interactions between two characters, with darker purple links representing more interactions and lighter links representing fewer interactions. Also, characters with more total interactions are sorted toward the center of the network, which is where we see Michael, Jim, Pam, and Dwight. Finally, interactions are only shown if characters have more than 25 total interactions (this prevents the graph from showing a jillion lines).\nI’m going to wrap this one up here, but later on I’ll probably play around a bit with doing some statistical modeling – predicting who is speaking, who a character is speaking to, something like that."
  },
  {
    "objectID": "posts/riddler-express-march-20-2020/index.html",
    "href": "posts/riddler-express-march-20-2020/index.html",
    "title": "Riddler Express - March 20, 2020",
    "section": "",
    "text": "One of my personal goals for 2020 is to improve my proficiency doing data-y things – mostly using R, but potentially other software as well. Typically, I’ve been using data from the #TidyTuesday project to practice data visualization and data from Kaggle, personal research projects, and other potentially interesting datasets to work on statistical modeling. I recently discovered The Riddler series – a weekly math/logic puzzle – that seems to be a good medium for brushing up on other skills (e.g. certain types of math and programming) that may not come up as often when I do visualizations or statistics."
  },
  {
    "objectID": "posts/riddler-express-march-20-2020/index.html#the-problem",
    "href": "posts/riddler-express-march-20-2020/index.html#the-problem",
    "title": "Riddler Express - March 20, 2020",
    "section": "The Problem",
    "text": "The Problem\nAnyway, this post solves the Riddler Express puzzle from March 20, 2020. The problem is this:\n\nA manager is trying to produce sales of his company’s widget, so he instructs his team to hold a sale every morning, lowering the price of the widget by 10 percent. However, he gives very specific instructions as to what should happen in the afternoon: Increase the price by 10 percent from the sale price, with the (incorrect) idea that it would return it to the original price. The team follows his instructions quite literally, lowering and then raising the price by 10 percent every day.\n\n\nAfter N days, the manager walks through the store in the evening, horrified to see that the widgets are marked more than 50 percent off of their original price. What is the smallest possible value of N?\n\nI’ll walk through a couple of ways to solve this – first, I’ll solve it algebraically, and next, I’ll solve it by “brute force” using the accumulate() function from the {purrr} package."
  },
  {
    "objectID": "posts/riddler-express-march-20-2020/index.html#solving-algebraically",
    "href": "posts/riddler-express-march-20-2020/index.html#solving-algebraically",
    "title": "Riddler Express - March 20, 2020",
    "section": "Solving Algebraically",
    "text": "Solving Algebraically\nSo, the first thing that strikes me when reading this is that it’s essentially a compounding interest problem, except in this case the interest is negative. That is, rather than gaining value exponentially over the number of compounding periods, we’re losing value exponentially. The formula for calculating compound interest is:\n\\[A = P(1 + r)^n\\]\nwhere A equals the final amount, P equals the principal (our initial value), r equals the interest rate, and n equals the number of compounding periods (the number of days in this case). We’re interested in solving for the value of n where our final amount, A, is less than .5. Our principal amount, P, in this case, is 1 (i.e. 100% of the value). So, our equation looks like this:\n\\[.5 > ((1-1*.1)*1.1)^n\\]\nThe internal logic here is that we subtract 10% from our initial value (1-1*.1) to represent the 10% decrease in price in the morning, then multiply this resulting value by 1.1 to represent the subsequent 10& increase in price in the afternoon. This simplifies to:\n\\[.5 > .99^n\\]\nFrom here, we can just solve by taking the log of each side and then dividing, which get us our answer\n\nn <- log(.5)/log(.99)\nn\n\n[1] 68.96756\n\n\nRounding this up (since we’re dealing in full days), we can say that after 69 days, the price of the widget will be below 50% of its initial price."
  },
  {
    "objectID": "posts/riddler-express-march-20-2020/index.html#solving-using-accumulate",
    "href": "posts/riddler-express-march-20-2020/index.html#solving-using-accumulate",
    "title": "Riddler Express - March 20, 2020",
    "section": "Solving using accumulate()",
    "text": "Solving using accumulate()\nWe can also solve this problem using the accumulate() function from the {purrr} package, which is part of the {tidyverse}. Essentially, accumulate() will take a function, evaluate it, and then pass the result of the evaluation back into the function, evaluate it again, pass the new result back into the function, etc. This makes it useful for solving problems like this one, where the end price of the widget on the previous day is the starting price of the widget on the current day.\nFirst, let’s load our packages. For this, we’ll just use {tidyverse}\n\nlibrary(tidyverse)\n\nNext, let’s set up a function that, if we give it the price of the widget at the beginning of the day, will calculate the price of the widget at the end of the day.\n\ndiscount_func <- function(x) {\n  (x-x*.1)*1.1\n}\n\nAnd then let’s test this function manually a few times.\n\ndiscount_func(1)\n\n[1] 0.99\n\ndiscount_func(.99)\n\n[1] 0.9801\n\ndiscount_func(.9801)\n\n[1] 0.970299\n\n\nNow, we can use accumulate() to automate what we just did manually. The first argument in accumulate() is, in this case, each day that we want to pass into the function. In the code below, I’m testing this for days 0-3 (but coded as 1-4 because we want the start value to be 1). The second argument is the function we just wrote.\n\naccumulate(1:4, ~discount_func(.))\n\n[1] 1.000000 0.990000 0.980100 0.970299\n\n\nAnd we can see that the values returned match our manual tests above, which is good!\nNow, we can use accumulate() to make a table with the end price of the widget each day. Note that because we want to start the widget price at 1, our first “day” in the table is day 0, which represents the beginning price of the widget on day 1.\n\ndays_tbl <- tibble(\n  day = c(0:1000),\n  end_price = accumulate(c(1:1001), ~discount_func(.))\n)\nhead(days_tbl)\n\n# A tibble: 6 × 2\n    day end_price\n  <int>     <dbl>\n1     0     1    \n2     1     0.99 \n3     2     0.980\n4     3     0.970\n5     4     0.961\n6     5     0.951\n\n\nAnd then we can plot the end price over time. I’ve added a little bit of transparency to each point so we can more easily see the clustering/overlap.\n\nggplot(days_tbl, aes(x = day, y = end_price)) +\n  geom_point(alpha = .3) +\n  theme_minimal() +\n  labs(\n    title = \"End Price of Widget over Time\"\n  )\n\n\n\n\nFinally, we can find the day where the end price is below .5 by filtering our table to only those where the price is less than .5 and then returning the first row.\n\ndays_tbl %>%\n  filter(end_price <= .5) %>%\n  slice(1)\n\n# A tibble: 1 × 2\n    day end_price\n  <int>     <dbl>\n1    69     0.500\n\n\nAnd we can see that this matches our algebraic result – great success!"
  },
  {
    "objectID": "posts/rva-pets/index.html",
    "href": "posts/rva-pets/index.html",
    "title": "RVA Pets",
    "section": "",
    "text": "First, let’s load our packages and set our plot themes/colors\n\nlibrary(tidyverse)\nlibrary(osmdata)\nlibrary(sf)\nlibrary(janitor)\nlibrary(hrbrthemes)\nlibrary(wesanderson)\nlibrary(tidytext)\nlibrary(kableExtra)\nlibrary(ggtext)\ntheme_set(theme_ipsum())\npal <- wes_palette(\"Zissou1\")\ncolors <- c(\"Dog\" = pal[1], \"Cat\" = pal[3])\n\nNext, we’ll read in the data and clean it up a little bit. In this dataset, each row represents a licensed pet in Richmond, Virginia. The dataset includes animal type (dog, cat, puppy, kitten) and the address of the owners. Whoever set up the data was also nice enough to include longitude and latitude for each address in the dataset, which means I don’t need to go out and get it. For our purposes here, I’m going to lump puppies in with dogs and kittens in with cats. I’m also going to extract the “location” column into a few separate columns. Let’s take a look at the first few entries.\n\npets_raw <- read_csv(here::here(\"data/rva_pets_2019.csv\"))\npets_clean <- pets_raw %>%\n  clean_names() %>%\n  extract(col = location_1,\n          into = c(\"address\", \"zip\", \"lat\", \"long\"),\n          regex = \"(.*)\\n.*(\\\\d{5})\\n\\\\((.*), (.*)\\\\)\") %>%\n  mutate(animal_type = str_replace_all(animal_type, c(\"Puppy\" = \"Dog\", \"Kitten\" = \"Cat\")))\nhead(pets_clean) %>%\n  kable(format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n \n  \n    animal_type \n    animal_name \n    address \n    zip \n    lat \n    long \n    load_date \n  \n \n\n  \n    Cat \n    Molly \n    301 Virginia Street APT 1008 \n    23219 \n    37.53294 \n    -77.433825 \n    20180627 \n  \n  \n    Dog \n    Sam \n    1407 Wilmington Avenue \n    23227 \n    37.58294 \n    -77.455213 \n    20180627 \n  \n  \n    Cat \n    Taffy \n    114 N Harvie Street \n    23220 \n    37.548414 \n    -77.45745 \n    20180627 \n  \n  \n    Dog \n    Jackson \n    4804 Riverside Drive \n    23225 \n    37.527326 \n    -77.483249 \n    20180627 \n  \n  \n    Dog \n    Cirrus \n    3107 E Marshall Street \n    23223 \n    37.52904 \n    -77.412272 \n    20180627 \n  \n  \n    Dog \n    Henri \n    1900 Maple Shade Lane \n    23227 \n    37.581979 \n    -77.466207 \n    20180627 \n  \n\n\n\n\n\nOk, now that our data is set up, let’s see if there are more cats or dogs in the city.\n\npets_clean %>%\n  count(animal_type) %>%\n  ggplot(aes(x = n, y = animal_type)) +\n  geom_col(color = pal[1], fill = pal[1]) +\n  geom_text(aes(x = n-50, label = n), hjust = 1, color = \"white\", fontface = \"bold\") +\n  labs(\n    title = \"Number of Cats vs Dogs\"\n  )\n\n\n\n\nAlright, so, lots more dogs. Like almost 4 to 1 dogs to cats. Which is something I can get behind. I’m a firm believer in the fact that dogs are wayyy better than cats.\nI’m also interested in the most common names for pets in RVA.\n\npets_clean %>%\n  group_by(animal_type) %>%\n  count(animal_name, sort = TRUE) %>%\n  slice(1:15) %>%\n  ungroup() %>%\n  ggplot(aes(x = n, y = reorder_within(animal_name, n, animal_type))) +\n    geom_col(color = pal[1], fill = pal[1]) +\n    geom_text(aes(x = if_else(animal_type == \"Cat\", n - .25, n - 1), label = n), hjust = 1, color = \"white\", fontface = \"bold\") +\n    facet_wrap(~animal_type, scales = \"free\") +\n    scale_y_reordered() +\n    labs(\n      title = \"Top Pet Names\",\n      y = NULL\n    )\n\n\n\n\nThese seem pretty standard to me, and unfortunately, nothing is screaming “RVA” here. No “Bagels,” no “Gwars,” etc.\nI also pulled out zip codes into their own column earlier, so we can take a look at which zip codes have the most dogs and cats.\n\npets_clean %>%\n  filter(!is.na(zip)) %>%\n  group_by(zip) %>%\n  count(animal_type, sort = TRUE)%>%\n  ungroup() %>%\n  group_by(animal_type) %>%\n  top_n(n = 10) %>%\n  ungroup() %>%\n  ggplot(aes(x = n, y = reorder_within(zip, n, animal_type))) +\n    geom_col(color = pal[1], fill = pal[1]) +\n    geom_text(aes(x = if_else(animal_type == \"Cat\", n - 1, n - 4), label = n), hjust = 1, color = \"white\", fontface = \"bold\") +\n    facet_wrap(~animal_type, scales = \"free\") +\n    scale_y_reordered() +\n    labs(\n      title = \"Number of Pets by Zipcode\",\n      y = NULL\n    )\n\n\n\n\nAlright, so most of the pets here live in Forest Hill/generally south of the river in 23225, and another big chunk live in 23220, which covers a few neighborhoods & includes The Fan, which is probably where most of the pet action is.\nAnd finally, since we have the latitude and longitude, I can put together a streetmap of the city showing where all of these little critters live. To do this, I’m going to grab some shape files through the OpenStreetMaps API and plot the pet datapoints on top of those.\n\npets_map <- st_as_sf(pets_clean %>%\n                       filter(!is.na(long)), coords = c(\"long\", \"lat\"),\n                     crs = 4326)\n\nget_rva_maps <- function(key, value) {\n  getbb(\"Richmond Virginia United States\") %>%\n    opq() %>%\n    add_osm_feature(key = key,\n                    value = value) %>%\n    osmdata_sf()\n}\n\nrva_streets <- get_rva_maps(key = \"highway\", value = c(\"motorway\", \"primary\", \"secondary\", \"tertiary\"))\n\nsmall_streets <- get_rva_maps(key = \"highway\", value = c(\"residential\", \"living_street\",\n                                                         \"unclassified\",\n                                                         \"service\", \"footway\", \"cycleway\"))\n\nriver <- get_rva_maps(key = \"waterway\", value = \"river\")\n\ndf <- tibble(\n  type = c(\"big_streets\", \"small_streets\", \"river\"),\n  lines = map(\n    .x = lst(rva_streets, small_streets, river),\n    .f = ~pluck(., \"osm_lines\")\n  )\n)\n\ncoords <- pluck(rva_streets, \"bbox\")\n\nannotations <- tibble(\n  label = c(\"<span style='color:#FFFFFF'><span style='color:#EBCC2A'>**Cats**</span> and <span style='color:#3B9AB2'>**Dogs**</span> in RVA</span>\"),\n  x = c(-77.555),\n  y = c(37.605),\n  hjust = c(0)\n)\n\nrva_pets <- ggplot() +\n  geom_sf(data = df$lines[[1]],\n          inherit.aes = FALSE,\n          size = .3,\n          alpha = .8, \n          color = \"white\") +\n  geom_sf(data = pets_map, aes(color = animal_type), alpha = .6, size = .75) +\n  geom_richtext(data = annotations, aes(x = x, y = y, label = label, hjust = hjust), fill = NA, label.color = NA, \n                label.padding = grid::unit(rep(0, 4), \"pt\"), size = 11) + \n  coord_sf(\n    xlim = c(-77.55, -77.4),\n    ylim = c(37.5, 37.61),\n    expand = TRUE\n  ) +\n  theme_void() +\n  scale_color_manual(\n    values = colors\n  ) +\n  theme(\n    legend.position = \"none\",\n    plot.background = element_rect(fill = \"grey10\"),\n    panel.background = element_rect(fill = \"grey10\"),\n    text = element_markdown()\n  )\n\n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-nc/4.0/CitationBibTeX citation:@online{ekholm2020,\n  author = {Eric Ekholm},\n  title = {RVA {Pets}},\n  date = {2020-04-23},\n  url = {https://www.ericekholm.com/posts/rva-pets},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEric Ekholm. 2020. “RVA Pets.” April 23, 2020. https://www.ericekholm.com/posts/rva-pets."
  }
]