[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am an educator, researcher, data scientist, and writer. I am currently an Education Data Specialist for Chesterfield County Public Schools, where I use data to help guide decisions and improve students’ educational experiences. In my free time, I enjoy reading, writing, data visualization/analysis, going to the gym, and spending time with friends & family (including my pup, Nala).\nI earned my PhD in educational psychology from the VCU School of Education in July 2019, where I worked in the Motivation in Context Lab and in the Discourse and Learning Lab to study students’ motivation, behaviors, and achievement. In my dissertation, I studied how writers’ daily emotional states (think: anxiety, enjoyment, boredom) influenced their productivity as well as the day-to-day inertia of these emotional states.\nDuring grad school, I discovered a love for working with data – exploring, visualizing, modeling, and communicating. I mostly work (and blog) in R, but I’m always learning new languages and technologies. Well, as much as I can with a toddler and an infant in the house."
  },
  {
    "objectID": "about.html#projects-and-publications",
    "href": "about.html#projects-and-publications",
    "title": "About Me",
    "section": "Projects and Publications",
    "text": "Projects and Publications\nI’ve published some stuff – both academic and not – which you can find on my publications page."
  },
  {
    "objectID": "about.html#consulting-and-contracted-work",
    "href": "about.html#consulting-and-contracted-work",
    "title": "About Me",
    "section": "Consulting and Contracted Work",
    "text": "Consulting and Contracted Work\nTime permitting, I occasionally do some consulting and contract work. Although I specialize in educational research, I’m generally proficient with a wide range of statistical modeling frameworks as well as data visualization and dashboard design (via {shiny}), among other things. If you’re interested in working with me, send me an email."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Julia\n\n\nLearning Out Loud\n\n\nMaximum Likelihood\n\n\nLogistic Regression\n\n\n\n\nLearning maximum likelihood estimation by fitting logistic regression ‘by hand’ (sort of)\n\n\n\n\n\n\nSep 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJulia\n\n\nTutorial\n\n\nBrief\n\n\n\n\nUsing Julia to generate a dataset with a given correlation\n\n\n\n\n\n\nSep 8, 2022\n\n\nEE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJulia\n\n\nLearning Out Loud\n\n\nMaximum Likelihood\n\n\n\n\nLearning maximum likelihood estimation by fitting models ‘by hand’ (in Julia!)\n\n\n\n\n\n\nAug 31, 2022\n\n\nEE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nCrossfit\n\n\nEDA\n\n\n\n\nAn exploration of data from the 2022 Crossfit Games\n\n\n\n\n\n\nAug 12, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nQuarto\n\n\nTutorial\n\n\nwebsite\n\n\n\n\nChanging the pages in a default Quarto blog\n\n\n\n\n\n\nJul 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nprogramming\n\n\nmetacognition\n\n\n\n\nMetacognition about writing a simple function to convert percentages to strings\n\n\n\n\n\n\nMay 31, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nprogramming\n\n\npurrr\n\n\n\n\nA pattern to create flexible analysis workflows\n\n\n\n\n\n\nMar 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\ntorch\n\n\nregression\n\n\n\n\nLearning torch by fitting a multiple regression model\n\n\n\n\n\n\nNov 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nshiny\n\n\nVA datathon\n\n\ngeography\n\n\n\n\nLessons learned and takeaways from the 2021 VA Datathon\n\n\n\n\n\n\nOct 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nEDA\n\n\nScooby Doo\n\n\nregression\n\n\n\n\nStream-of-consciousness exploration and modeling\n\n\n\n\n\n\nJul 20, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\ntutorial\n\n\nerrors\n\n\nprogramming\n\n\n\n\nWrapping functions to safeguard against errors\n\n\n\n\n\n\nJul 2, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\ntutorial\n\n\nregression\n\n\nMario Kart\n\n\n\n\nUsing feature engineering and linear regression to predict Mario Kart world records\n\n\n\n\n\n\nMay 30, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\ntutorial\n\n\nRmarkdown\n\n\nGoogle Sites\n\n\n\n\nA workaround for people in organizations that use Google Sites\n\n\n\n\n\n\nMay 3, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\ntutorial\nYour Mom's House\nprogramming\n\n\n\nWrap your R code in Your Mom's House audio drops\n\n\n\nApr 25, 2021\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\ntutorial\n\n\ndistill\n\n\n\n\nHow to modify the distill template to fit your preferences\n\n\n\n\n\n\nApr 5, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\ntutorial\n\n\ndata viz\n\n\nggplot\n\n\n\n\nA walkthrough of how – and when – to replace your legend with colored text\n\n\n\n\n\n\nMar 24, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\ntutorial\n\n\nD&D\n\n\nregression\n\n\n\n\nPredicting monster challenge ratings & examining regression diagnostics\n\n\n\n\n\n\nMar 10, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\ntutorial\n\n\nD&D\n\n\nLPA\n\n\nclustering\n\n\n\n\nGrouping D&D monsters using latent profile analysis\n\n\n\n\n\n\nFeb 3, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nTutorial\n\n\nAPI\n\n\nD&D\n\n\nEDA\n\n\n\n\nExploring monster stats in D&D 5e\n\n\n\n\n\n\nDec 28, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nTutorial\n\n\nAPI\n\n\nD&D\n\n\n\n\nWrangling JSON data from an API\n\n\n\n\n\n\nDec 18, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\ntidymodels\n\n\nstats\n\n\nsplines\n\n\n\n\nUsing splines and iteration via map() to fit and interrogate models\n\n\n\n\n\n\nNov 18, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\ngt\n\n\ntables\n\n\ndata viz\n\n\n\n\nAnd impressions of the {gt} package\n\n\n\n\n\n\nOct 26, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nsimulation\n\n\npuzzle\n\n\n\n\nDetermining whether random numbers can form a triangle using a simulation\n\n\n\n\n\n\nOct 18, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\ntext analysis\nYour Mom's House\nEDA\n\n\n\nAn exploratory analysis of transcripts from the YMH podcast.\n\n\n\nSep 20, 2020\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\ntidymodels\n\n\nThe Office\n\n\nlogistic regression\n\n\n\n\nPredicting the speaker of dialogue from The Office\n\n\n\n\n\n\nAug 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nTutorial\n\n\nglm\n\n\nPoisson Regression\n\n\nstats\n\n\n\n\nPracticing Poisson regression using Xmen data\n\n\n\n\n\n\nMay 6, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nTutorial\n\n\nFunction Writing\n\n\n\n\nExamples and tutorial for writing rolling aggregate/window functions\n\n\n\n\n\n\nMay 6, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nRVA\n\n\nGeography\n\n\nEDA\n\n\n\n\nAnalyzing pet ownership in RVA\n\n\n\n\n\n\nApr 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nThe Office\n\n\nText Analysis\n\n\nClustering\n\n\n\n\nK means clustering with The Office dialogue\n\n\n\n\n\n\nApr 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nPuzzles\n\n\nCoding Challenge\n\n\n\n\nSolving a math puzzle and exploring the accumulate() function\n\n\n\n\n\n\nMar 31, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nThe Office\n\n\nEDA\n\n\nText Analysis\n\n\n\n\nAn initial exploration of dialogue from The Office\n\n\n\n\n\n\nMar 14, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Eric Ekholm",
    "section": "",
    "text": "I am an educator, researcher, data scientist, and writer. I am currently an Education Data Specialist for Chesterfield County Public Schools, where I use data to help guide decisions and improve students’ educational experiences."
  },
  {
    "objectID": "posts/combining-pmap-and-docall/index.html",
    "href": "posts/combining-pmap-and-docall/index.html",
    "title": "Combining pmap and do.call",
    "section": "",
    "text": "The point of this blog post is to walk through a pattern I’ve started using in some of my analyses that combines do.call(), purrr::pmap(), and some wrapper functions to customize how a given analysis gets run. I’ll start by demonstrating do.call() and pmap() separately, then showing how you can use them together to do some cool things. I’m not going to go super in-depth on either do.call() or pmap(), so it might be worthwhile to look into some of the documentation for those functions separately.\nAlso – I’m going to use the {palmerpenguins} data here to illustrate this workflow. And, like, as is typically the case with toy data, the point here isn’t to run a suite of analyses that answer meaningful questions about this data, but rather to demonstrate how to combine these functions in a way that could help you answer meaningful questions for your own data.\nWith all of that said, onward and upward!"
  },
  {
    "objectID": "posts/combining-pmap-and-docall/index.html#combining-with-purrrpmap",
    "href": "posts/combining-pmap-and-docall/index.html#combining-with-purrrpmap",
    "title": "Combining pmap and do.call",
    "section": "Combining with purrr::pmap ()",
    "text": "Combining with purrr::pmap ()\nJust based on the above, do.call() isn’t really doing anything useful for us. It’s just a slightly more verbose way to call a function. But where do.call() really shines is when you pair it with some iteration – which we’ll do now, via purrr::pmap() – and/or some conditional logic (which we’ll add later via a wrapper function). Basically it shines with you program with it, is what I’m trying to say.\nFor those that don’t know, purrr::pmap() extends purrr::map() to allow for an arbitrary number of arguments to map over in parallel. If you’re not familiar with purrr::map(), Hadley’s R for Data Science book has a good chapter on it. But anyway, let’s illustrate pmap() by running a handful of correlations on some sample data\n\n#generate data\na <- rnorm(100)\nb <- rnorm(100)\nd <- rnorm(100)\n\n#put data into a list\nsample_args <- list(\n    x = list(a, b, d),\n    y = list(b, d, a)\n)\n\nThis gives us a list of x and y values, where the first element of x is a, the first element of y is b, etc etc. We can run a bunch of correlations – x[[1]] with y[[1]], x[[2]] with y[[2]] etc – by using pmap() and cor():\n\npmap(sample_args, ~cor(..1, ..2, use = \"pairwise.complete.obs\"))\n\n[[1]]\n[1] 0.0467708\n\n[[2]]\n[1] 0.1479934\n\n[[3]]\n[1] -0.07458596\n\n\nWhich can be a helpful pattern.\nWhat’s potentially more interesting, though, is that we can also use pmap() in conjunction with do.call() to not only iterate through arguments passed to a given function (like we do with cor() above), but to also iterate over various functions:\n\n#create a vector of function names\nfuns <- c(\"mean\", \"cor\", \"lm\")\n\n#create a list of function arguments, where each element of the list is a list of args\nfun_args <- list(\n    list(penguins$body_mass_g, na.rm = TRUE),\n    list(\n        penguins$body_mass_g, \n        penguins$bill_depth_mm, \n        use = \"pairwise.complete.obs\"\n        ),\n    list(\n        formula = body_mass_g ~ bill_depth_mm + sex,\n        data = penguins,\n        na.action = \"na.omit\"\n    )\n)\n\n#combine the function names and args into a tibble\nfun_iterator <- tibble(\n    f = funs,\n    fa = fun_args\n)\n\n#take a look at the tibble\nglimpse(fun_iterator)\n\nRows: 3\nColumns: 2\n$ f  <chr> \"mean\", \"cor\", \"lm\"\n$ fa <list> [<3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, 4250, 3300, 3…\n\n\nWhat we’re doing in the above code is:\n\ncreating a list of function names;\ncreating a list of function arguments (where each element of the list is a list of args);\nbinding these lists together in a tibble.\n\nThen, we can then execute all of these functions with their corresponding arguments with do.call():\n\nres <- pmap(fun_iterator, ~ do.call(..1, ..2))\n\nWithin do.call(), we’re passing the first column of our fun_iterator table to the first argument of do.call() (as denoted by ..1), and the second column of the tibble to the second argument of do.call() (as denoted by ..2). This will give us a list, res, where each element is the result of the function/argument combination in our fun_iterator tibble.\nTo prove it worked, let’s look at the results:\n\n#mean\nres[[1]]\n\n[1] 4201.754\n\n\n\n#cor\nres[[2]]\n\n[1] -0.4719156\n\n\n\n#lm\nbroom::glance(res[[3]])\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.642         0.640  483.      296. 2.34e-74     2 -2529. 5066. 5081.\n# … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n\nIn theory, you can specify an entire set of analyses ahead of time and then execute them using pmap() + do.call() if you wanted to. So let’s at one way we might do that via a wrapper function."
  },
  {
    "objectID": "posts/combining-pmap-and-docall/index.html#wrap-your-analyses",
    "href": "posts/combining-pmap-and-docall/index.html#wrap-your-analyses",
    "title": "Combining pmap and do.call",
    "section": "Wrap Your Analyses",
    "text": "Wrap Your Analyses\nThe real power of this is to write a function that wraps all of these components and allows you to run just a subset of them. And this is how I actually use this pattern in my own work. But I’ll touch on some real-world applications after we go through the code below.\nLet’s start by writing a wrapper function that has 1 argument, include, where include is a character vector of function names.\n\nanalyze_penguins <- function(include = c(\"mean\", \"cor\", \"lm\")) {\n  #some code here\n}\n\nThen let’s drop all of the code that we just ran into the function:\n\nanalyze_penguins <- function(include = c(\"mean\", \"cor\", \"lm\")) {\n    #we already ran all of this\n    funs <- c(\"mean\", \"cor\", \"lm\")\n\n    fun_args <- list(\n        list(penguins$body_mass_g, na.rm = TRUE),\n        list(\n            penguins$body_mass_g,\n            penguins$bill_depth_mm,\n            use = \"pairwise.complete.obs\"\n        ),\n        list(\n            formula = body_mass_g ~ bill_depth_mm + sex,\n            data = penguins,\n            na.action = \"na.omit\"\n        )\n    )\n\n    fun_iterator <- tibble(\n        f = funs,\n        fa = fun_args\n    )\n}\n\nAnd then we subset the fun_iterator tibble to only include the functions we include in the include argument of our wrapper function, and executed only those functions via pmap() + do.call():\n\nanalyze_penguins <- function(include = c(\"mean\", \"cor\", \"lm\")) {\n    #this is all the same as previously\n    funs <- c(\"mean\", \"cor\", \"lm\")\n\n    fun_args <- list(\n        list(penguins$body_mass_g, na.rm = TRUE),\n        list(\n            penguins$body_mass_g,\n            penguins$bill_depth_mm,\n            use = \"pairwise.complete.obs\"\n        ),\n        list(\n            formula = body_mass_g ~ bill_depth_mm + sex,\n            data = penguins,\n            na.action = \"na.omit\"\n        )\n    )\n\n    fun_iterator <- tibble(\n        f = funs,\n        fa = fun_args\n    )\n\n    # filter to only a subset of these functions that we've asked for in the wrapper args\n    fun_iterator <- fun_iterator[fun_iterator$f %in% include, ]\n    \n    #execute these functions\n    pmap(fun_iterator, ~do.call(..1, ..2))\n}\n\nSo, say we just wanted the mean:\n\nanalyze_penguins(\"mean\")\n\n[[1]]\n[1] 4201.754\n\n\nOr just the mean and the correlation:\n\nanalyze_penguins(c(\"mean\", \"cor\"))\n\n[[1]]\n[1] 4201.754\n\n[[2]]\n[1] -0.4719156\n\n\nOr just the linear model:\n\nbroom::glance(analyze_penguins(\"lm\")[[1]])\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.642         0.640  483.      296. 2.34e-74     2 -2529. 5066. 5081.\n# … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n\nI really like this pattern for data cleaning. I have a handful of demographic variables that I regularly work with that need to be cleaned and/or recoded, and I have some helper functions I’ve written to clean/recode each of them individually. But I also have a “meta” recode_demographics() function that can execute any combination of my helper functions depending on what I need for a given project. You can obviously also write your wrapper function to give you more control over the arguments to each constituent function (like by allowing you to pass in a formula to lm(), for instance, rather than hardcoding your formula), which can make this whole approach very flexible! It can be a bit time-consuming to write a wrapper that gives you the right level of flexibility, but if you have a set of related tasks you do frequently, I think it’s worth the time to figure out."
  },
  {
    "objectID": "posts/crossfit-games-2022/index.html",
    "href": "posts/crossfit-games-2022/index.html",
    "title": "Crossfit Games Analysis",
    "section": "",
    "text": "For me, the Crossfit Games is one of the most exciting weekends of the year. And between the shift in programming and the closeness of the competition on both the mens and womens sides, this year was probably the best Games we’ve seen in a while.\nTo prolong the Games-high, I decided to dive into the leaderboard data a bit and see what we can take away from the athletes’ performances. I noticed that Morning Chalk Up has a Games leaderboard that I could scrape, and so I cobbled together a package to let me pull down the data and analyze it. If you’re into R and want to use the package, go ahead, but beware that it’s pretty fragile at the moment (I threw it together in a couple of hours on Tuesday), but I’ll probably put in a bit of work to improve it when I have more time.\nIn general, I won’t explain what all of the code in this post does, but I’ll include it in case folks are curious. If you’re not into R or coding and just want to read the text, then feel free to ignore all of the code throughout :)\nWithout any further ado, let’s get into it."
  },
  {
    "objectID": "posts/crossfit-games-2022/index.html#setup",
    "href": "posts/crossfit-games-2022/index.html#setup",
    "title": "Crossfit Games Analysis",
    "section": "Setup",
    "text": "Setup\nFirst we’ll do a little bit of set up, and we’ll also pull down the data.\n\nlibrary(tidyverse)\nlibrary(cfg)\nlibrary(eemisc)\nlibrary(harrypotter)\nlibrary(glue)\nlibrary(ggtext)\n\ntheme_set(theme_minimal())\n\noptions(\n    ggplot2.discrete.fill = list(\n        hp(n = 2, option = \"HermioneGranger\")\n    ),\n    ggplot2.discrete.color = list(\n        hp(n = 2, option = \"HermioneGranger\")\n    )\n)\n\n#get data\nwomen_df <- fetch_leaderboard(division = \"women\") |>\n    mutate(division = \"Women\")\n\nmen_df <- fetch_leaderboard(division = \"men\") |>\n    mutate(division = \"Men\")\n\ncombined <- bind_rows(women_df, men_df)\n\nlong_by_event <- combined |>\n    select(athlete, division, starts_with(\"event\")) |>\n    pivot_longer(\n        cols = starts_with(\"event\"),\n        names_to = \"event\",\n        values_to = \"event_place\"\n    )"
  },
  {
    "objectID": "posts/crossfit-games-2022/index.html#points",
    "href": "posts/crossfit-games-2022/index.html#points",
    "title": "Crossfit Games Analysis",
    "section": "Points",
    "text": "Points\nLet’s first take a look at all of the athletes’ total points. Hopefully this is pretty self-explanatory.\n\nggplot(combined, aes(x = points, y = fct_reorder(athlete, points), fill = division)) +\n    geom_col() +\n    geom_text(aes(label = points, x = points - 10), hjust = 1, color = \"white\") +\n    facet_wrap(vars(division), scales = \"free_y\") +\n    labs(\n        x = \"Total Points\",\n        y = NULL,\n        title = \"Total Points by Athlete at the 2022 CFG\"\n    ) +\n    theme(\n        legend.position = \"none\"\n    )"
  },
  {
    "objectID": "posts/crossfit-games-2022/index.html#number-of-top-3-finishes",
    "href": "posts/crossfit-games-2022/index.html#number-of-top-3-finishes",
    "title": "Crossfit Games Analysis",
    "section": "Number of Top 3 Finishes",
    "text": "Number of Top 3 Finishes\nAnother thing that might be interesting to check out is the number of top 3 finishes from each athlete:\n\nlong_by_event |>\n    filter(event_place <= 3) |>\n    count(division, athlete) |>\n    filter(n > 1) |>\n    ggplot(aes(x = n, y = fct_reorder(athlete, n), fill = division)) +\n    geom_col() +\n    geom_text(aes(label = n, x = n - .1), hjust = 1, color = \"white\") +\n    labs(\n        y = NULL,\n        x = \"# of Top 3 Finishes\",\n        title = \"Number of Top 3 Finishes by Athlete\",\n        subtitle = \"Only athletes with multiple top 3 finishes shown\"\n    ) +\n    theme(\n        legend.position = \"none\"\n    )\n\n\n\n\nIt’s probably not surprising that Tia dominated here, finishing in the top 3 in 8 of the 14 total scored events. One thing that’s interesting to me, though, is that every athlete who podiumed had 5 (or more) top 3 finishes, while nobody else had more than 3."
  },
  {
    "objectID": "posts/crossfit-games-2022/index.html#event-placement-variability",
    "href": "posts/crossfit-games-2022/index.html#event-placement-variability",
    "title": "Crossfit Games Analysis",
    "section": "Event Placement Variability",
    "text": "Event Placement Variability\nWe hear a lot about the importance of consistency in Crossfit. After all, the whole point of Crossfit is to be able to do everything well. We always hear that you need to be well rounded and need to avoid bombing events. And whenever Pat Vellner or Laura Horvath or whoever has a bad event, we hear about how that’s not the way to win the Games.\nSo given all of that, let’s spend some time looking at event-to-event variability for each athlete. To make it a bit easier, I’ll limit this to the top 20 athletes for the men and women.\nIn the graph below, I’ll show athletes’ average event placement with a dot, and a measure of their variability with a bar. It doesn’t really matter how variability is calculated (it’s just the standard error of the mean in this case), but suffice to say that the wider the bar is, the more variable (less consistent) each athlete’s performance was.\n\ntop_20 <- combined$athlete[combined$place <= 20]\n\nlong_by_event |>\n    filter(athlete %in% top_20) |>\n    group_by(division, athlete) |>\n    summarize(sem = sd(event_place)/sqrt(14), avg = mean(event_place)) |>\n    ungroup() |>\n    ggplot(aes(x = avg, y = fct_reorder(athlete, -avg), color = division)) +\n    geom_point() +\n    geom_errorbarh(aes(xmin = avg - sem, xmax = avg + sem), height = 0) +\n    facet_wrap(vars(division), scales = \"free_y\") +\n    labs(\n        y = NULL,\n        x = \"Event Placement\",\n        title = \"Event Avg Placement and Variability\"\n    ) +\n    scale_color_hp_d(\"HermioneGranger\") +\n    theme(\n        panel.grid.major.y = element_blank(),\n        legend.position = \"none\"\n    )\n\n\n\n\nSo we can see a lot of differences between athletes here. Justin Medeiros and Roman Khrennikov were the two most consistent athletes (followed by Tia Toomey). We can also see that Noah Ohlsen and BKG were very consistent, albeit consistently toward the high-middle. On the flip side, we can see people like Laura Horvath, Gui Malheiros, and Dani Speegle had a lot of variability between events – meaning they finished very well in some and very poorly in others."
  },
  {
    "objectID": "posts/crossfit-games-2022/index.html#finish-range",
    "href": "posts/crossfit-games-2022/index.html#finish-range",
    "title": "Crossfit Games Analysis",
    "section": "Finish Range",
    "text": "Finish Range\nThe analysis above takes all of the athletes’ events into account to calculate variability, but we might just care about each athlete’s best and worst performance, and the spread between those.\n\nfinish_range <- long_by_event |>\n    filter(athlete %in% top_20) |>\n    group_by(division, athlete) |>\n    summarize(\n        worst = max(event_place),\n        best = min(event_place)\n    ) |>\n    ungroup() |>\n    left_join(select(combined, place, athlete), by = \"athlete\")\n\nggplot(finish_range, aes(color = division, y = fct_reorder(athlete, -place))) +\n    geom_segment(aes(x = best, yend = fct_reorder(athlete, -place), xend = worst)) +\n    geom_point(aes(x = worst), size = 7, shape = 21, fill = \"white\") +\n    geom_point(aes(x = best), size = 7, shape = 21, fill = \"white\") +\n    geom_text(aes(x = worst, label = worst)) +\n    geom_text(aes(x = best, label = best)) +\n    scale_color_hp_d(\"HermioneGranger\") + \n    facet_wrap(vars(division), scales = \"free_y\") +\n    labs(\n        x = \"Event Finish\",\n        y = NULL,\n        title = \"Best and Worst Finishes for Top 20 Athletes\"\n    ) +\n    theme(\n        legend.position = \"none\"\n    )\n\n\n\n\nAlthough this shows something slightly different that the previous graph, it basically reaffirms what we saw there. One interesting takeaway is that Justin Medeiros won the Games without actually winning any events. Another interesting point is that Roman Khrennikov had the best “worst event finish” out of everyone, never finishing lower than 15th, which is pretty incredible. On the women’s side, we see that quite a few athletes spanned the whole range between first and last (recall that Emily Rolfe withdrew, so last was 39th for much of the competition). Laura Horvath, Dani Speegle, and Lucy Campbell all went from worst to first, and a few others (Kara Saunders, Amanda Barnhart) managed nearly the same. To be fair, there were a few instances of this on the men’s side, too (Adler, Malheiros, Pepper)."
  },
  {
    "objectID": "posts/crossfit-games-2022/index.html#score-variability-vs-overall-finish",
    "href": "posts/crossfit-games-2022/index.html#score-variability-vs-overall-finish",
    "title": "Crossfit Games Analysis",
    "section": "Score Variability vs Overall Finish",
    "text": "Score Variability vs Overall Finish\nThis doesn’t really tell us the story about how much consistency matters, though. On the one hand, Medeiros and Khrennikov were the most consistent and also finished top 2. On the other hand, Noah Ohlsen was also incredibly consistent, but he finished 12th, whereas Laura Horvath had a lot of variability in her performances and finished 3rd. So let’s plot athletes’ overall finish (on the X axis) against their score variability (on the Y axis). If these are strongly related, we’d expect to see them form a line. If they’re not strongly related, they’ll look like a blob.\n\nsem_by_place <- long_by_event |>\n    group_by(division, athlete) |>\n    summarize(sem = sd(event_place)/sqrt(14)) |>\n    ungroup() |>\n    left_join(select(combined, athlete, place), by = \"athlete\") \n    \nsem_by_place |>\n    ggplot(aes(x = place, y = sem, color = division)) +\n    geom_point() +\n    scale_color_hp_d(\"HermioneGranger\") +\n    facet_wrap(vars(division)) +\n    labs(\n        x = \"Place\",\n        y = \"Variability\"\n    ) +\n    theme(\n        legend.position = \"none\"\n    )\n\n\n\n\nIt doesn’t really seem like there’s much here. But we can calculate the actual correlation coefficient to get a number to summarize the relationship.\n\ncor(sem_by_place$sem, sem_by_place$place)\n\n[1] -0.1818332\n\n\nSo there’s a small negative correlation here – people who varied more between events tended to place slightly worse overall. But it’s a pretty small relationship."
  },
  {
    "objectID": "posts/crossfit-games-2022/index.html#point-trajectories-for-top-5",
    "href": "posts/crossfit-games-2022/index.html#point-trajectories-for-top-5",
    "title": "Crossfit Games Analysis",
    "section": "Point Trajectories for Top 5",
    "text": "Point Trajectories for Top 5\nThe thing that always makes the Games interesting are the races for the podium, and this year had some great races. On the men’s side, there was a lot of jostling between the top 3 for position. On the women’s side, Tia didn’t pull away until about event 10, and even Tia and Mal had locked up the gold and silver, there was an incredibly exciting race for 3rd thanks to a big comeback from Laura Horvath. So let’s plot the point trajectories for the athletes that ended up finishing in the top 5:\n\ntop_5 <- combined$athlete[combined$place <= 5]\n\nscores_men <- running_scores(men_df) |>\n    mutate(division = \"Men\")\n\nscores_women <- running_scores(women_df) |>\n    mutate(division = \"Women\")\n\nscores_combined <- bind_rows(scores_men, scores_women) |>\n    filter(athlete %in% top_5)\n    \nmax_athlete <- scores_combined |>\n    group_by(athlete) |>\n    summarize(score = max(cum_points))\n\nscores_combined <- scores_combined |>\n    left_join(max_athlete, by = \"athlete\")\n\nggplot(scores_combined, aes(x = event, y = cum_points, color = athlete)) +\n    geom_line() +\n    geom_point() +\n    geom_text(aes(label = str_replace_all(athlete, \"^(.*) (.*)\", \"\\\\2\"), y = score), x = 14.5, hjust = 0) +\n    facet_wrap(vars(division)) +\n    scale_x_continuous(limits = c(0, 19), breaks = seq(2, 14, by = 2)) +\n    labs(\n        title = \"Point Trajectories for Top 5 Athletes\",\n        x = \"Event #\",\n        y = \"Total Points\"\n    ) +\n    theme(\n        panel.grid.minor.x = element_blank(),\n        legend.position = \"none\"\n    )"
  },
  {
    "objectID": "posts/crossfit-games-2022/index.html#laura-horvaths-comeback",
    "href": "posts/crossfit-games-2022/index.html#laura-horvaths-comeback",
    "title": "Crossfit Games Analysis",
    "section": "Laura Horvath’s Comeback",
    "text": "Laura Horvath’s Comeback\nAnd the last thing I’ll include here is a visualization of Laura Horvath’s huge comeback. I’m a major LH fan, and even though I had a moment of doubt after the HSPU event, I was stoked to see her come back and take 3rd after absolutely crushing it during Sunday’s events.\n\nscores <- running_scores(women_df)\n\nlh_scores <- scores[scores$athlete == \"Laura Horvath\", ] |>\n    select(event, cum_points, athlete)\n\n# function to get 3rd place for a given event\nthird_place <- function(x, event) {\n    tmp <- event_leaderboard(x, event)\n\n    res <- tmp[tmp$place == 3, c(\"event\", \"cum_points\")]\n\n    res$athlete <- \"third\"\n\n    res\n}\n\nevents <- unique(scores$event)\n\nthirds <- map_dfr(events, ~third_place(women_df, .x))\n\nx <- bind_rows(lh_scores, thirds)\n\nhungary_green <- \"#436F4D\"\nbronze <- \"#CD7F32\"\n\nggplot(x, aes(x = event, y = cum_points, color = athlete)) +\n    geom_line() +\n    geom_point(shape = 21, fill = \"white\", size = 10) +\n    geom_text(aes(label = cum_points)) +\n    scale_color_manual(\n        values = c(hungary_green, bronze)\n    ) +\n    labs(\n        title = glue(\"<span style='color:{hungary_green}'>Laura Horvath's</span> Charge to the <span style='color:{bronze}'>Podium</span>\"),\n        y = \"Total Points\",\n        x = \"Event #\",\n        subtitle = glue(\"The <span style='color:{bronze}'>bronze line</span> represents 3rd place after any given event\")\n    ) +\n    theme_ee(size = 12) +\n    theme(\n        legend.position = \"none\",\n        panel.grid.minor = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        plot.title = element_markdown(size = 16)\n    )\n\n\n\n\nThat’s all for now – maybe I’ll do another once the Rogue Invitational rolls around in October. And who knows, maybe Tia will hang up her shoes and we’ll have a new top dog in the women’s division."
  },
  {
    "objectID": "posts/demo-quarto-site/index.html",
    "href": "posts/demo-quarto-site/index.html",
    "title": "Modifying the Default Quarto Blog Structure",
    "section": "",
    "text": "This is a bit of a quibble, but I’m not a huge fan of the default Quarto blog structure – not necessarily the styling/theming or anything like that, but more so the fact that it sets your blog posts as the index (home page) of the site, whereas I’d prefer a more generic landing page with a little bit about me and some social media links, etc. Which the default Quarto blog does include as an about page! So the point of this post is to show you how to modify the default Quarto layout so that:\n\nyour website index (home page, sitename.com) is a brief “summary” of yourself;\nyour blog posts are listed at sitename.com/blog; and\nyou have a more extensive about page at sitename.com/about\n\nBeyond that, I’m not going to go into how to change the styling, how to publish, or anything like that, because there are already much better tutorials out there (Danielle Navarro’s blog is great if you’re trying to migrate from distill to Quarto).\n\nStep 0: Install Quarto\nMaybe this is obvious, but you’ll need Quarto installed if you want to use it. You can download it here\n\n\nStep 1: Make a site\nI prefer to do this from the command line, but I think you can do it from RStudio as well (I use VSCode, so I’m not 100% up-to-date on all of the RStudio IDE features).\nYou can do this in the command line by changing your working directory to wherever you want your site’s folder to live, then create a default site via:\nquarto create-project PROJECT_NAME --type website:blog\nThis will create a generic sample site with the following file structure:\n\n(n.b. that you may not have the README file in your directory)\n\n\nStep 2: Render the site\nThis is sort of optional, but if you want to get a sense of what the default site looks like, you can render all of the files in it. From the command line, set your current directory to the site folder (that you just created), then run\nquarto render\nand you should get some notifications that your posts are rendering. Once they’re done, you will see a _site folder. This has all of your files rendered inside of it, and if you open index.html, you can navigate to your site’s home page, which (by default) is the blog listing. This is something we’re going to change. You’ll also notice the about.html page, which is what we actually want to make our index.\n\n\nStep 3: Change some file names\nIn your root directory (i.e. whatever you set PROJECT_NAME to earlier; not _site), we want to do the following:\n\nchange the file name of index.qmd to blog.qmd\nchange the file name of about.qmd to index.qmd\n\nYou’ll also want to open up the blog.qmd file and change the title (inside the YAML header) to “Blog”. The header should now look like this:\n\nLikewise, you’ll want to change the title of index.qmd (which was about.qmd) from “About” to something else – maybe your name of the name of the site. So it’ll look something like this:\n\nYou’ll also obviously want to change the image, include your own social media links, etc., but we won’t cover that here.\n\n\nStep 4: Make a new about page\nI like having the home page (index) be like a brief summary of me, and then I like having a more detailed “About” page just in case people are interested in reading more. We just set the index to be that brief intro or whatever you want to call it, so now we need to create a new about page.\nIn your root folder (PROJECT_NAME), create a new file called about.qmd\nAnd then within that you can add in whatever content you want – a brief bio, some pictures, whatever. The only thing I include at the outset (beside the content) is a YAML header that looks like this:\n---\ntitle: \"About Me\"\n---\n\n\nStep 5: Modify your _quarto.yml file\nNow we need to modify the _quarto.yml file, which is in the root directory (PROJECT_NAME). This file provides Quarto with some “big picture” instructions on the overall layout and styling of you site.\nBasically we need to do 2 things in this file:\n\nadd blog.qmd to the navbar menu; and\nadd your site url.\n\nYou also might want to change your website title. Either way, after you’re done, your _quarto.yml file should look something like this:\n\n\n\nStep 6: Render your site (again)\nNow we’re ready to render again! Before you render, make sure you save all of your files. As before, we can render the site from the command line via:\nquarto render\nAnd your files should update in the _site directory. And that’s pretty much it. You can explore the files in your _site directory and see that our “postcard” is now the index/home page, and your blog and about me pages are accessible via the navbar.\n\n\nStep 7: Customize and deploy\nI’m not going to cover these things, but now that we have our website set up the way we want it, the next steps are to customize the style/theming, add content, include your own social media links, etc. And then finally deploy your site! The main Quarto site has some great resources on how to do all of this.\n\n\nWrapping Up\nYou can see this demo site (I didn’t customize anything beyond what we just walked through) here, and you can see the site’s Github repo here\nHope this helps some folks! Happy Quarto-ing!\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-nc/4.0/CitationBibTeX citation:@online{ekholm2022,\n  author = {Eric Ekholm},\n  title = {Modifying the {Default} {Quarto} {Blog} {Structure}},\n  date = {2022-07-22},\n  url = {https://www.ericekholm.com/posts/demo-quarto-site},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEric Ekholm. 2022. “Modifying the Default Quarto Blog\nStructure.” July 22, 2022. https://www.ericekholm.com/posts/demo-quarto-site."
  },
  {
    "objectID": "posts/dungeons-and-dragons-part-1/index.html",
    "href": "posts/dungeons-and-dragons-part-1/index.html",
    "title": "Dungeons and Dragons - Part 1",
    "section": "",
    "text": "I’ve been playing Dungeons and Dragons 5th edition (D&D 5e) for a few years now and really enjoy it, although COVID has really hindered my opportunity to play. That said, I recently discovered a D&D 5e API, so I figured I’d do a series of blog posts analyzing D&D data from this API. In this first post, I wanted to do a quick walkthrough of how to get data from this API using R and wrangling it into a structure that’s more or less conducive to later analysis. In later posts, I’ll explore the data and then get into some modeling.\nAs something of an aside – the API has data for character classes, spells, races, monsters, etc. I’m mostly going to focus on the monsters data, but might use some of the other data later on."
  },
  {
    "objectID": "posts/dungeons-and-dragons-part-1/index.html#setup",
    "href": "posts/dungeons-and-dragons-part-1/index.html#setup",
    "title": "Dungeons and Dragons - Part 1",
    "section": "Setup",
    "text": "Setup\nFirst, I’ll load the packages I need to get and wrangle the data, which is really just {tidyverse}, {jsonlite} and good old base R. I’m also adding in the base URL of the API.\n\nlibrary(tidyverse)\nlibrary(jsonlite)\n\ndnd_base <- \"https://www.dnd5eapi.co/api/monsters/\""
  },
  {
    "objectID": "posts/dungeons-and-dragons-part-1/index.html#fetching-data",
    "href": "posts/dungeons-and-dragons-part-1/index.html#fetching-data",
    "title": "Dungeons and Dragons - Part 1",
    "section": "Fetching Data",
    "text": "Fetching Data\nSo, the first step here is to actually get the data from the API. Let’s walk through the process here, illustrating this with a single monster (the aboleth) and then applying the process to all of the monsters.\nWe’ll use the fromJSON() function to get JSON data from the API. We’ll see that this gives us a pretty gnarly nested list.\n\nexample <- fromJSON(paste0(dnd_base, \"aboleth\"))\n\nglimpse(example)\n\nList of 28\n $ index                 : chr \"aboleth\"\n $ name                  : chr \"Aboleth\"\n $ size                  : chr \"Large\"\n $ type                  : chr \"aberration\"\n $ alignment             : chr \"lawful evil\"\n $ armor_class           : int 17\n $ hit_points            : int 135\n $ hit_dice              : chr \"18d10\"\n $ speed                 :List of 2\n  ..$ walk: chr \"10 ft.\"\n  ..$ swim: chr \"40 ft.\"\n $ strength              : int 21\n $ dexterity             : int 9\n $ constitution          : int 15\n $ intelligence          : int 18\n $ wisdom                : int 15\n $ charisma              : int 18\n $ proficiencies         :'data.frame': 5 obs. of  2 variables:\n  ..$ value      : int [1:5] 6 8 6 12 10\n  ..$ proficiency:'data.frame': 5 obs. of  3 variables:\n  .. ..$ index: chr [1:5] \"saving-throw-con\" \"saving-throw-int\" \"saving-throw-wis\" \"skill-history\" ...\n  .. ..$ name : chr [1:5] \"Saving Throw: CON\" \"Saving Throw: INT\" \"Saving Throw: WIS\" \"Skill: History\" ...\n  .. ..$ url  : chr [1:5] \"/api/proficiencies/saving-throw-con\" \"/api/proficiencies/saving-throw-int\" \"/api/proficiencies/saving-throw-wis\" \"/api/proficiencies/skill-history\" ...\n $ damage_vulnerabilities: list()\n $ damage_resistances    : list()\n $ damage_immunities     : list()\n $ condition_immunities  : list()\n $ senses                :List of 2\n  ..$ darkvision        : chr \"120 ft.\"\n  ..$ passive_perception: int 20\n $ languages             : chr \"Deep Speech, telepathy 120 ft.\"\n $ challenge_rating      : int 10\n $ xp                    : int 5900\n $ special_abilities     :'data.frame': 3 obs. of  3 variables:\n  ..$ name: chr [1:3] \"Amphibious\" \"Mucous Cloud\" \"Probing Telepathy\"\n  ..$ desc: chr [1:3] \"The aboleth can breathe air and water.\" \"While underwater, the aboleth is surrounded by transformative mucus. A creature that touches the aboleth or tha\"| __truncated__ \"If a creature communicates telepathically with the aboleth, the aboleth learns the creature's greatest desires \"| __truncated__\n  ..$ dc  :'data.frame':    3 obs. of  3 variables:\n  .. ..$ dc_type     :'data.frame': 3 obs. of  3 variables:\n  .. ..$ dc_value    : int [1:3] NA 14 NA\n  .. ..$ success_type: chr [1:3] NA \"none\" NA\n $ actions               :'data.frame': 4 obs. of  7 variables:\n  ..$ name        : chr [1:4] \"Multiattack\" \"Tentacle\" \"Tail\" \"Enslave\"\n  ..$ desc        : chr [1:4] \"The aboleth makes three tentacle attacks.\" \"Melee Weapon Attack: +9 to hit, reach 10 ft., one target. Hit: 12 (2d6 + 5) bludgeoning damage. If the target i\"| __truncated__ \"Melee Weapon Attack: +9 to hit, reach 10 ft. one target. Hit: 15 (3d6 + 5) bludgeoning damage.\" \"The aboleth targets one creature it can see within 30 ft. of it. The target must succeed on a DC 14 Wisdom savi\"| __truncated__\n  ..$ options     :'data.frame':    4 obs. of  2 variables:\n  .. ..$ choose: int [1:4] 1 NA NA NA\n  .. ..$ from  :List of 4\n  ..$ attack_bonus: int [1:4] NA 9 9 NA\n  ..$ dc          :'data.frame':    4 obs. of  3 variables:\n  .. ..$ dc_type     :'data.frame': 4 obs. of  3 variables:\n  .. ..$ dc_value    : int [1:4] NA 14 NA 14\n  .. ..$ success_type: chr [1:4] NA \"none\" NA \"none\"\n  ..$ damage      :List of 4\n  .. ..$ : NULL\n  .. ..$ :'data.frame': 2 obs. of  2 variables:\n  .. ..$ :'data.frame': 1 obs. of  2 variables:\n  .. ..$ : NULL\n  ..$ usage       :'data.frame':    4 obs. of  2 variables:\n  .. ..$ type : chr [1:4] NA NA NA \"per day\"\n  .. ..$ times: int [1:4] NA NA NA 3\n $ legendary_actions     :'data.frame': 3 obs. of  4 variables:\n  ..$ name        : chr [1:3] \"Detect\" \"Tail Swipe\" \"Psychic Drain (Costs 2 Actions)\"\n  ..$ desc        : chr [1:3] \"The aboleth makes a Wisdom (Perception) check.\" \"The aboleth makes one tail attack.\" \"One creature charmed by the aboleth takes 10 (3d6) psychic damage, and the aboleth regains hit points equal to \"| __truncated__\n  ..$ attack_bonus: int [1:3] NA NA 0\n  ..$ damage      :List of 3\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ :'data.frame': 1 obs. of  2 variables:\n $ url                   : chr \"/api/monsters/aboleth\"\n\n\nTo clean this list up a bit, we’ll use the enframe() function (from {tibble}) to convert the lists into a dataframe and then the pivot_wider() function to reshape this into a single-row tibble.\n\nexample %>%\n  enframe() %>%\n  pivot_wider(names_from = name,\n              values_from = value) %>%\n  glimpse()\n\nRows: 1\nColumns: 28\n$ index                  <list> \"aboleth\"\n$ name                   <list> \"Aboleth\"\n$ size                   <list> \"Large\"\n$ type                   <list> \"aberration\"\n$ alignment              <list> \"lawful evil\"\n$ armor_class            <list> 17\n$ hit_points             <list> 135\n$ hit_dice               <list> \"18d10\"\n$ speed                  <list> [\"10 ft.\", \"40 ft.\"]\n$ strength               <list> 21\n$ dexterity              <list> 9\n$ constitution           <list> 15\n$ intelligence           <list> 18\n$ wisdom                 <list> 15\n$ charisma               <list> 18\n$ proficiencies          <list> [<data.frame[5 x 2]>]\n$ damage_vulnerabilities <list> []\n$ damage_resistances     <list> []\n$ damage_immunities      <list> []\n$ condition_immunities   <list> []\n$ senses                 <list> [\"120 ft.\", 20]\n$ languages              <list> \"Deep Speech, telepathy 120 ft.\"\n$ challenge_rating       <list> 10\n$ xp                     <list> 5900\n$ special_abilities      <list> [<data.frame[3 x 3]>]\n$ actions                <list> [<data.frame[4 x 7]>]\n$ legendary_actions      <list> [<data.frame[3 x 4]>]\n$ url                    <list> \"/api/monsters/aboleth\"\n\n\nGreat. This is more or less the structure we want. You might notice that all of our columns are lists rather than atomic vectors – we’ll deal with that later once we get all of the data.\nNow that we know the basic process, we’ll just apply this to all of the monsters with data available through the API. To do that, I’ll write a function that executes the previous steps, get a list of all of the monsters available in the API, use map() to iterate the “fetch” function for each monster, and then bind all of the resulting rows together.\n\nfetch_monster <- function(monster) {\n  dnd_url <- \"https://www.dnd5eapi.co/api/monsters/\"\n  \n  ret <- fromJSON(paste0(dnd_url, monster)) %>%\n    enframe() %>%\n    pivot_wider(names_from = name,\n                values_from = value)\n  \n  return(ret)\n}\n\n#this gets all of the monster indices to plug into the fetch function\nmons <- fromJSON(dnd_base)$results %>%\n  pull(index)\n\nmonster_lists <- map(mons, fetch_monster)\n\nmons_bind <- bind_rows(monster_lists)\n\nglimpse(mons_bind)\n\nRows: 332\nColumns: 31\n$ index                  <list> \"aboleth\", \"acolyte\", \"adult-black-dragon\", \"a…\n$ name                   <list> \"Aboleth\", \"Acolyte\", \"Adult Black Dragon\", \"A…\n$ size                   <list> \"Large\", \"Medium\", \"Huge\", \"Huge\", \"Huge\", \"Hu…\n$ type                   <list> \"aberration\", \"humanoid\", \"dragon\", \"dragon\", …\n$ alignment              <list> \"lawful evil\", \"any alignment\", \"chaotic evil\"…\n$ armor_class            <list> 17, 10, 19, 19, 18, 19, 18, 19, 19, 19, 19, 18…\n$ hit_points             <list> 135, 9, 195, 225, 172, 212, 184, 256, 207, 256…\n$ hit_dice               <list> \"18d10\", \"2d8\", \"17d12\", \"18d12\", \"15d12\", \"17…\n$ speed                  <list> [\"10 ft.\", \"40 ft.\"], [\"30 ft.\"], [\"40 ft.\", \"…\n$ strength               <list> 21, 10, 23, 25, 23, 25, 23, 27, 23, 27, 27, 22…\n$ dexterity              <list> 9, 10, 14, 10, 10, 10, 12, 14, 12, 10, 10, 10,…\n$ constitution           <list> 15, 10, 21, 23, 21, 23, 21, 25, 21, 25, 25, 22…\n$ intelligence           <list> 18, 10, 14, 16, 14, 16, 18, 16, 18, 16, 16, 8,…\n$ wisdom                 <list> 15, 14, 13, 15, 13, 15, 15, 15, 15, 13, 13, 12…\n$ charisma               <list> 18, 11, 17, 19, 17, 19, 17, 24, 17, 21, 21, 12…\n$ proficiencies          <list> [<data.frame[5 x 2]>], [<data.frame[2 x 2]>], …\n$ damage_vulnerabilities <list> [], [], [], [], [], [], [], [], [], [], [], []…\n$ damage_resistances     <list> [], [], [], [], [], [], [], [], [], [], [], []…\n$ damage_immunities      <list> [], [], \"acid\", \"lightning\", \"fire\", \"lightnin…\n$ condition_immunities   <list> [], [], [], [], [], [], [], [], [<data.frame[1…\n$ senses                 <list> [\"120 ft.\", 20], [12], [\"60 ft.\", \"120 ft.\", 2…\n$ languages              <list> \"Deep Speech, telepathy 120 ft.\", \"any one lan…\n$ challenge_rating       <list> 10, 0.25, 14, 16, 13, 15, 14, 17, 15, 17, 16, …\n$ xp                     <list> 5900, 50, 11500, 15000, 10000, 13000, 11500, 1…\n$ special_abilities      <list> [<data.frame[3 x 3]>], [<data.frame[1 x 3]>], …\n$ actions                <list> [<data.frame[4 x 7]>], [<data.frame[1 x 4]>], …\n$ legendary_actions      <list> [<data.frame[3 x 4]>], [], [<data.frame[3 x 4]…\n$ url                    <list> \"/api/monsters/aboleth\", \"/api/monsters/acolyt…\n$ subtype                <list> <NULL>, \"any race\", <NULL>, <NULL>, <NULL>, <N…\n$ reactions              <list> <NULL>, <NULL>, <NULL>, <NULL>, <NULL>, <NULL>…\n$ forms                  <list> <NULL>, <NULL>, <NULL>, <NULL>, <NULL>, <NULL>…\n\n\nNotice that we have the same structure as in the previous example, but now with 322 rows instead of 1. Now we can take care of coercing some of these list columns into atomic vectors."
  },
  {
    "objectID": "posts/dungeons-and-dragons-part-1/index.html#restructuring-data",
    "href": "posts/dungeons-and-dragons-part-1/index.html#restructuring-data",
    "title": "Dungeons and Dragons - Part 1",
    "section": "Restructuring Data",
    "text": "Restructuring Data\nOne problem here, though, is that the possible variable values for each column differ depending on the monster (for some variables). Variables like strength, hit points, challenge rating, and xp will always be a single integer value, but variables like legendary_actions can differ greatly. People who play D&D will know that normal monsters don’t have any legendary actions, and so this will be NULL for those monsters. But some monsters might have 1 or 2 legendary actions, whereas big baddies like ancient dragons can have several. This same varying structure applies to columns like proficiencies, special abilities, reactions, etc. Ultimately, this means that a list column is probably the best way to represent this type of data, since lists are more flexible, whereas some columns can be represented as an atomic vector, and so we need to figure out how to address this.\nTo do this, we can write a couple of functions. The first, compare_lens() (below), will determine if the length of each element of a list is equal to whatever size we want to compare against (I’ve set the default to 1, which is what we want to use in this case). It then uses the all() function to determine if all of these comparisons are equal to TRUE, and will return a single value of TRUE if this is the case (and a single FALSE if not).\n\ncompare_lens <- function(x, size = 1) {\n  all(map_lgl(x, ~length(unlist(.x)) == size))\n}\n\nNext, we’ll use the compare_lens() function as the test expression in another function, cond_unlist (or conditionally unlist), below. The idea here is if compare_lens() is TRUE, then we will unlist the list (simplify it to a vector) passed to the function; otherwise, we’ll leave it as is (as a list). Putting these functions together, the logic is:\n\nDetermine if all elements of a list have a length equal to 1.\nIf so, turn that list into a vector.\nIf not, leave it as a list.\n\n\ncond_unlist <- function(x) {\n  if (compare_lens(x) == TRUE) {\n    unlist(x)\n  } else {\n    x\n  }\n}\n\nThe final step is to apply this function to all of the columns (which, recall, are lists) in our mons_bind tibble. We can do this using a combination of mutate() and across(). After doing this, we’ll see that some of the columns in our data frame have been simplified to character, integer, and double vectors, whereas others remain lists (lists of lists, lists of data frames).\n\nmons_df <- mons_bind %>%\n  mutate(across(.cols = everything(), ~cond_unlist(x = .x)))\n\nglimpse(mons_df)\n\nRows: 332\nColumns: 31\n$ index                  <chr> \"aboleth\", \"acolyte\", \"adult-black-dragon\", \"ad…\n$ name                   <chr> \"Aboleth\", \"Acolyte\", \"Adult Black Dragon\", \"Ad…\n$ size                   <chr> \"Large\", \"Medium\", \"Huge\", \"Huge\", \"Huge\", \"Hug…\n$ type                   <chr> \"aberration\", \"humanoid\", \"dragon\", \"dragon\", \"…\n$ alignment              <chr> \"lawful evil\", \"any alignment\", \"chaotic evil\",…\n$ armor_class            <int> 17, 10, 19, 19, 18, 19, 18, 19, 19, 19, 19, 18,…\n$ hit_points             <int> 135, 9, 195, 225, 172, 212, 184, 256, 207, 256,…\n$ hit_dice               <chr> \"18d10\", \"2d8\", \"17d12\", \"18d12\", \"15d12\", \"17d…\n$ speed                  <list> [\"10 ft.\", \"40 ft.\"], [\"30 ft.\"], [\"40 ft.\", \"…\n$ strength               <int> 21, 10, 23, 25, 23, 25, 23, 27, 23, 27, 27, 22,…\n$ dexterity              <int> 9, 10, 14, 10, 10, 10, 12, 14, 12, 10, 10, 10, …\n$ constitution           <int> 15, 10, 21, 23, 21, 23, 21, 25, 21, 25, 25, 22,…\n$ intelligence           <int> 18, 10, 14, 16, 14, 16, 18, 16, 18, 16, 16, 8, …\n$ wisdom                 <int> 15, 14, 13, 15, 13, 15, 15, 15, 15, 13, 13, 12,…\n$ charisma               <int> 18, 11, 17, 19, 17, 19, 17, 24, 17, 21, 21, 12,…\n$ proficiencies          <list> [<data.frame[5 x 2]>], [<data.frame[2 x 2]>], …\n$ damage_vulnerabilities <list> [], [], [], [], [], [], [], [], [], [], [], []…\n$ damage_resistances     <list> [], [], [], [], [], [], [], [], [], [], [], []…\n$ damage_immunities      <list> [], [], \"acid\", \"lightning\", \"fire\", \"lightnin…\n$ condition_immunities   <list> [], [], [], [], [], [], [], [], [<data.frame[1…\n$ senses                 <list> [\"120 ft.\", 20], [12], [\"60 ft.\", \"120 ft.\", 2…\n$ languages              <chr> \"Deep Speech, telepathy 120 ft.\", \"any one lang…\n$ challenge_rating       <dbl> 10.00, 0.25, 14.00, 16.00, 13.00, 15.00, 14.00,…\n$ xp                     <int> 5900, 50, 11500, 15000, 10000, 13000, 11500, 18…\n$ special_abilities      <list> [<data.frame[3 x 3]>], [<data.frame[1 x 3]>], …\n$ actions                <list> [<data.frame[4 x 7]>], [<data.frame[1 x 4]>], …\n$ legendary_actions      <list> [<data.frame[3 x 4]>], [], [<data.frame[3 x 4]…\n$ url                    <chr> \"/api/monsters/aboleth\", \"/api/monsters/acolyte…\n$ subtype                <list> <NULL>, \"any race\", <NULL>, <NULL>, <NULL>, <N…\n$ reactions              <list> <NULL>, <NULL>, <NULL>, <NULL>, <NULL>, <NULL>…\n$ forms                  <list> <NULL>, <NULL>, <NULL>, <NULL>, <NULL>, <NULL>…\n\n\nAnd there we have it. Our data is now in a pretty good state for some analysis. Depending on what we’re interested in doing, we could also do some additional feature engineering on the list columns, but the choices there will be contingent on the analyses we want to do.\nFor my next blog in this series, I’ll use this data to do some exploratory analysis, which I hope to get to in the next week or so."
  },
  {
    "objectID": "posts/dungeons-and-dragons-part-2/index.html",
    "href": "posts/dungeons-and-dragons-part-2/index.html",
    "title": "Dungeons and Dragons - Part 2",
    "section": "",
    "text": "In my last blog post, I walked through how to extract and wrangle/rectangle monster data from the Dungeons and Dragons 5th edition (D&D 5e) API. In this post, I want to explore this data a little bit – looking at some counts, descriptive statistics, etc. In later posts, I’m planning to do some different statistical analyses, potentially include cluster analysis and some predictive modeling.\nOne note – the monsters represented in this data aren’t all of the monsters in D&D. The API I’m using has monsters from the systems reference document (SRD), and this doesn’t include all of the monsters introduced in specific campaigns or in books like Xanathar’s Guide to Everything.\nWith all of that said, let’s get to it."
  },
  {
    "objectID": "posts/dungeons-and-dragons-part-2/index.html#setup",
    "href": "posts/dungeons-and-dragons-part-2/index.html#setup",
    "title": "Dungeons and Dragons - Part 2",
    "section": "Setup",
    "text": "Setup\nTo start, I’m going to load in some packages and get the data. If you’re interested in the process for getting the data, you might want to check out my previous post in this series.\n\nlibrary(tidyverse)\nlibrary(eemisc)\nlibrary(jsonlite)\nlibrary(ggridges)\nlibrary(gt)\nlibrary(corrr)\n\nherm <- harrypotter::hp(n = 1, option = \"HermioneGranger\")\n\nopts <- options(\n  ggplot2.discrete.fill = list(\n    harrypotter::hp(n = 3, option = \"HermioneGranger\"),\n    harrypotter::hp(n = 7, option = \"Always\")\n  )\n)\n\ntheme_set(theme_ee())\n\ndnd_base <- \"https://www.dnd5eapi.co/api/monsters/\"\n\n### Functions and process for getting data; described in previous post\nfetch_monster <- function(monster) {\n  dnd_url <- \"https://www.dnd5eapi.co/api/monsters/\"\n  \n  ret <- fromJSON(paste0(dnd_url, monster)) %>%\n    enframe() %>%\n    pivot_wider(names_from = name,\n                values_from = value)\n  \n  return(ret)\n}\n\ncompare_lens <- function(x, size = 1) {\n  all(map_lgl(x, ~length(unlist(.x)) == size))\n}\ncond_unlist <- function(x) {\n  if (compare_lens(x) == TRUE) {\n    unlist(x)\n  } else {\n    x\n  }\n}\n\nmons <- fromJSON(dnd_base)$results %>%\n  pull(index)\n\nmonster_lists <- map(mons, fetch_monster)\n\nmons_bind <- bind_rows(monster_lists)\n\nmons_df <- mons_bind %>%\n  mutate(across(.cols = everything(), ~cond_unlist(x = .x)))"
  },
  {
    "objectID": "posts/dungeons-and-dragons-part-2/index.html#challenge-ratings-and-hit-points",
    "href": "posts/dungeons-and-dragons-part-2/index.html#challenge-ratings-and-hit-points",
    "title": "Dungeons and Dragons - Part 2",
    "section": "Challenge Ratings and Hit Points",
    "text": "Challenge Ratings and Hit Points\nFor whatever reason, the first thing that strikes me to look at is the monsters’ challenge ratings (CRs). As the name suggests, CRs are an indication of how difficult a monster is for a group of players to fight, with higher CRs corresponding to a more difficult fight. The general rule of thumb is that a party of players can fight monsters about equal to their own level (or lower), and that higher CR monsters could be pretty tough.\n\nmons_df %>%\n  ggplot(aes(x = challenge_rating)) +\n  geom_histogram(fill = herm, color = herm, alpha = .8) +\n  labs(\n    title = \"Distribution of Challenge Ratings\"\n  )\n\n\n\n\nThe general rule makes sense in conjunction with this distribution of CRs. Most of the action in D&D campaigns tends to occur in lower levels, so it makes sense that the majority of the monsters here have CRs < 10. We do see one dude hanging out at CR 30, though, so let’s see who that is.\n\nmons_df %>%\n  slice_max(order_by = challenge_rating, n = 1)\n\n# A tibble: 1 × 31\n  index name  size  type  alignment armor_class hit_points hit_dice speed       \n  <chr> <chr> <chr> <chr> <chr>           <int>      <int> <chr>    <list>      \n1 tarr… Tarr… Garg… mons… unaligned          25        676 33d20    <named list>\n# … with 22 more variables: strength <int>, dexterity <int>,\n#   constitution <int>, intelligence <int>, wisdom <int>, charisma <int>,\n#   proficiencies <list>, damage_vulnerabilities <list>,\n#   damage_resistances <list>, damage_immunities <list>,\n#   condition_immunities <list>, senses <list>, languages <chr>,\n#   challenge_rating <dbl>, xp <int>, special_abilities <list>, actions <list>,\n#   legendary_actions <list>, url <chr>, subtype <list>, reactions <list>, …\n\n\nRight, so we can see the monster with the highest challenge rating is the Tarrasque. That’s this guy.\n\nNext, let’s take a look at the distribution of monster hit points. For those unfamiliar with D&D/video games more broadly, hit points represent the amount of health a character/monster has, and reducing someone to 0 hit points will defeat them. Typically (and I’ll explore this relationship more momentarily), hit points will increase as CR increases.\n\nmons_df %>%\n  ggplot(aes(x = hit_points)) +\n  geom_histogram(fill = herm, color = herm, alpha = .8) +\n  labs(\n    title = \"Distribution of Monster Hit Points\"\n  )\n\n\n\n\nLet’s look a little bit more at descriptives for these previous two stats:\n\nsummary(mons_df[c(\"challenge_rating\", \"hit_points\")])\n\n challenge_rating   hit_points    \n Min.   : 0.000   Min.   :  1.00  \n 1st Qu.: 0.500   1st Qu.: 19.00  \n Median : 2.000   Median : 45.00  \n Mean   : 4.515   Mean   : 81.49  \n 3rd Qu.: 6.000   3rd Qu.:114.00  \n Max.   :30.000   Max.   :676.00  \n\n\nAnd, again, presumably there’s a strong correlation between them, but let’s check that as well.\n\ncor(mons_df$hit_points, mons_df$challenge_rating)\n\n[1] 0.9414071\n\n\nSo, we see a very strong correlation between hit points and challenge rating. Let’s plot this.\n\nmons_df %>%\n  ggplot(aes(x = hit_points, y = challenge_rating)) +\n  geom_point(color = herm) +\n  labs(\n    title = \"Monster Hit Points vs Challenge Rating\"\n  )\n\n\n\n\nYeah…that’s what we’d expect a strong correlation to look like. One thing to note is that, although it looks like the Tarrasque fits on the general trend line here, outliers can have a strong influence on the correlation coefficient, so I’ll do a quick check to see what the value would be if we didn’t include the Tarrasque.\n\nmons_df %>%\n  filter(name != \"Tarrasque\") %>%\n  select(hit_points, challenge_rating) %>%\n  cor() %>%\n  .[[1,2]]\n\n[1] 0.9409642\n\n\nOur correlation coefficient is pretty much identical to the previous, Tarrasque-included calculation, which makes sense given what we see in the scatterplot, but still a reasonable check to include."
  },
  {
    "objectID": "posts/dungeons-and-dragons-part-2/index.html#ability-scores",
    "href": "posts/dungeons-and-dragons-part-2/index.html#ability-scores",
    "title": "Dungeons and Dragons - Part 2",
    "section": "Ability Scores",
    "text": "Ability Scores\nAbility scores are central to D&D and are the thing I’m most interested in looking at here. A quick Google search will tell you all you want to know (& more) about ability scores, but as a quick tl;dr – ability scores represent a character’s (player, monster, or non-player-character) abilities in several different areas, and each does different things. Furthermore, different character classes will value different ability scores. Fighters, for example, will tend to value strength and constitution; rangers and rogues will value dexterity; wizards will value intelligence, etc. Characters’ ability scores affect how well they do in combat, how they cast spells, how well they can persuade others, whether or not they can successfully climb a trees, etc – they affect pretty much anything you want to do in the game. And, due to how ability scores are allocated, characters will not have high scores on every ability, and the differential prioritizations and limited availability makes the distributions (and relationships among the scores) interesting to me.\nSo, let’s first check out the distributions.\n\nabs <- c(\"strength\", \"charisma\", \"dexterity\", \"intelligence\", \"wisdom\", \"constitution\")\n\nab_scores <- mons_df %>%\n  select(name, all_of(abs)) %>%\n  pivot_longer(cols = 2:ncol(.),\n               names_to = \"ability\",\n               values_to = \"score\")\n\nab_scores %>%\n  ggplot(aes(x = score, y = ability, fill = ability)) +\n  geom_density_ridges(alpha = .7) +\n  labs(\n    title = \"Monster Ability Score Distributions\",\n    y = NULL\n  ) +\n  theme(\n    legend.position = \"none\"\n  )\n\n\n\n\nIt’s interesting to me that the distributions all have different shapes, and especially that wisdom appears to the be only ability score that has a normal distribution for monsters.\nAnother interesting question to me is to what extent are these correlated. To check this out, we could use base R’s cor() function, but I want to try the {corrr} package, which provides some helpers for doing correlation analyses.\n\nabs_corrs <- mons_df %>%\n  select(all_of(abs)) %>%\n  correlate() %>%\n  rearrange() %>%\n  shave()\n\nabs_corrs %>%\n  fashion() %>%\n  gt() %>%\n  tab_header(\n    title = \"Ability Score Correlations\",\n    subtitle = \"D&D 5e Monsters\"\n  )\n\n\n\n\n\n  \n    \n      Ability Score Correlations\n    \n    \n      D&D 5e Monsters\n    \n  \n  \n    \n      term\n      constitution\n      strength\n      charisma\n      intelligence\n      wisdom\n      dexterity\n    \n  \n  \n    constitution\n\n\n\n\n\n\n    strength\n .86\n\n\n\n\n\n    charisma\n .60\n .52\n\n\n\n\n    intelligence\n .51\n .42\n .90\n\n\n\n    wisdom\n .45\n .42\n .74\n .65\n\n\n    dexterity\n-.19\n-.23\n .21\n .21\n .36\n\n  \n  \n  \n\n\n\n\nWe see some interesting stuff here:\n\nDexterity has weak to moderate correlations with everything.\nExcept for dexterity, constitution has moderate to large correlations with everything. This makes sense, since constitution relates to how many hit points creatures have, and so we’d expect cons to increase with level, and so regardless of what a monster’s primary ability is, higher level monsters will likely have high cons scores.\nThe strongest correlations are cons:strength and intelligence:charisma, which both seem reasonable. Int/charisma are useful for spellcasting and spell saving throws (and so are likely to travel together in monster stat blocks), and strength/cons are likely going to travel together in the form of big beefy melee combat type monsters.\n\nWe can also look at the same data using the rplot() function in {corrr}, although it’s not super easy to see:\n\nabs_corrs %>%\n  rplot()\n\n\n\n\nBeyond looking at the distributions and correlations of these ability scores, I think it’s also worth it to just look at the means. This will give us a sense of what the average monster in D&D is like, stats-wise.\n\nab_scores %>%\n  group_by(ability) %>%\n  summarize(avg = mean(score)) %>%\n  mutate(ability = fct_reorder(ability, avg)) %>%\n  ungroup() %>%\n  ggplot(aes(x = avg, y = ability)) +\n  geom_col(fill = herm) +\n  geom_text(aes(label = round(avg, 1), x = avg - .2), hjust = 1, color = \"white\") +\n  labs(\n    x = \"Average Ability Score\",\n    y = NULL,\n    title = \"Average Monster Ability Scores in D&D 5e\"\n  )\n\n\n\n\nSo, on average, monsters tend to be stronger and have higher constitutions, but have lower intelligence. Which makes sense if we look at the distributions again.\nFinally, we might want to look at which monster scores the highest on each ability score.\n\nab_scores %>%\n  select(ability, name, score) %>%\n  mutate(ability = str_to_title(ability)) %>%\n  group_by(ability) %>%\n  slice_max(order_by = score, n = 1) %>%\n  ungroup() %>%\n  gt() %>%\n  tab_header(\n    title = \"Highest Ability Scores\",\n    subtitle = \"...and the monsters that own them\"\n  ) %>%\n  cols_label(\n    name = \"Monster\",\n    ability = \"Ability\",\n    score = \"Score\"\n  ) %>%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels(columns = everything())\n  )\n\n\n\n\n\n  \n    \n      Highest Ability Scores\n    \n    \n      ...and the monsters that own them\n    \n  \n  \n    \n      Ability\n      Monster\n      Score\n    \n  \n  \n    Charisma\nSolar\n30\n    Constitution\nTarrasque\n30\n    Dexterity\nWill-o'-Wisp\n28\n    Intelligence\nSolar\n25\n    Strength\nAncient Gold Dragon\n30\n    Strength\nAncient Red Dragon\n30\n    Strength\nAncient Silver Dragon\n30\n    Strength\nKraken\n30\n    Strength\nTarrasque\n30\n    Wisdom\nSolar\n25\n  \n  \n  \n\n\n\n\nOne thing to keep in mind is that ability scores are capped at 30, so we see a handful of abilities that, according to the game rules, cannot be any higher (cons, strength, and charisma). We also see multiple monsters hitting the strength cap, including our old friend the Tarrasque. And we see the Solar represented in 3 categories (charisma, int, wisdom). I wasn’t familiar with the Solar before making this table, so I looked it up, and it seems like a sword-welding angel, which is pretty cool."
  },
  {
    "objectID": "posts/dungeons-and-dragons-part-2/index.html#other-questions",
    "href": "posts/dungeons-and-dragons-part-2/index.html#other-questions",
    "title": "Dungeons and Dragons - Part 2",
    "section": "Other Questions",
    "text": "Other Questions\nI’ll cap this post off by looking at a few odds and ends. This one might be a little silly, but since the game is called Dungeons and Dragons, it might be relevant to see how many monsters are actually dragons.\n\nmons_df %>%\n  mutate(is_dragon = if_else(type == \"dragon\", \"Dragon\", \"Not Dragon\")) %>%\n  count(is_dragon) %>%\n  ggplot(aes(x = n, y = is_dragon)) +\n  geom_col(fill = herm) +\n  geom_text(aes(label = n, x = n - 3), hjust = 1, color = \"white\", fontface = \"bold\") +\n  labs(\n    title = \"More Dungeons than Dragons\",\n    subtitle = \"Most monsters in D&D 5e are not dragons\",\n    y = NULL,\n    x = \"Monster Count\"\n  )\n\n\n\n\nGiven the above, we might be interested in seeing which monster types are the most common:\n\nmons_df %>%\n  count(type) %>%\n  ggplot(aes(x = n, y = fct_reorder(type, n))) +\n  geom_col(fill = herm) +\n  geom_text(aes(label = n, x = n - 1), hjust = 1, color = \"white\", fontface = \"bold\") +\n  labs(\n    title = \"Most Common Monster Types\",\n    subtitle = \"D&D 5e\",\n    y = NULL,\n    x = \"Monster Count\"\n  )\n\n\n\n\nWe can see here that beasts are the most common monster type, followed by humanoids and then dragons. I’m not really sure what’s going on with the “swarm of tiny beasts,” so let’s take a peek at those.\n\nmons_df %>%\n  filter(str_detect(type, \"swarm\")) %>%\n  pull(name)\n\n [1] \"Swarm of Bats\"             \"Swarm of Beetles\"         \n [3] \"Swarm of Centipedes\"       \"Swarm of Insects\"         \n [5] \"Swarm of Poisonous Snakes\" \"Swarm of Quippers\"        \n [7] \"Swarm of Rats\"             \"Swarm of Ravens\"          \n [9] \"Swarm of Spiders\"          \"Swarm of Wasps\"           \n\n\nI suppose that makes sense – there are stat blocks for swarms of bats, spiders, etc.\nThe last thing I’m going to check out here is the experience points (xp) distributions. My sense is that it’ll look similar to the distributions for challenge rating and hit points, since tougher monsters will award more experience for beating them. But still worth checking out.\n\nmons_df %>%\n  ggplot(aes(x = xp)) +\n  geom_histogram(fill = herm, color = herm, alpha = .8) +\n  labs(\n    title = \"Distribution of XP from Monsters\"\n  )\n\n\n\n\nSo this plot isn’t great since the Tarrasque is worth over 150k xp, which makes it harder to see the distribution at the lower end. There are also a few monsters that award ~50k xp, which again also makes it difficult to distinguish values at the lower end. I’ll filter put this on a log scale to make it easier to see the lower end.\n\nmons_df %>%\n  ggplot(aes(x = xp)) +\n  geom_histogram(fill = herm, color = herm, alpha = .8) +\n  labs(\n    title = \"Distribution of XP from Monsters\"\n  ) +\n  scale_x_log10()\n\n\n\n\nThis is a lot easier to read – we can see that the modal value for xp is maybe 500ish, although it’s also quite common for monsters to award 100xp or less. Monsters that award over 10k xp are pretty rare.\nAnd that’ll be it for this one. In the next blog post in this series, I’ll likely do some sort of clustering – probably latent profile analysis because I want to brush back up on it – to examine different groups of monsters based on their ability scores."
  },
  {
    "objectID": "posts/dungeons-and-dragons-part-3/index.html",
    "href": "posts/dungeons-and-dragons-part-3/index.html",
    "title": "Dungeons and Dragons - Part 3",
    "section": "",
    "text": "This post is the next installment in a mini-series I’m doing where I explore Dungeons and Dragons (D&D) data. In the first post, I showed how to wrangle JSON data from a D&D API, and in the second post, I explored monster statistics.\nIn this post, I’m going to use latent profile analysis (LPA) and monsters’ ability scores to try to classify monsters into different “classes.” For instance, we might suspect that there is a “class” of up-in-your-face monsters that have high strength and constitution but low intelligence, whereas there may be another class of spell casters that has high intelligence and charisma but low strength and constitution. LPA gives us a framework to estimate how many of these classes exist as well as which monsters fall into which classes.\nNote: I’m probably going to use the terms profile, class, group, and cluster somewhat interchangeably throughout this post (because I’m undisciplined), so just as a warning ahead of time – these all mean pretty much the same thing.\nBefore getting into the meat of this post, I want to give a shoutout to the excellent {tidyLPA]} package, which provides some functions to make doing LPA easier.\nSo let’s get into it, then."
  },
  {
    "objectID": "posts/dungeons-and-dragons-part-3/index.html#fit-lpa-select-best-model",
    "href": "posts/dungeons-and-dragons-part-3/index.html#fit-lpa-select-best-model",
    "title": "Dungeons and Dragons - Part 3",
    "section": "Fit LPA & Select Best Model",
    "text": "Fit LPA & Select Best Model\nThe next step is to estimate our LPA. This is very easy using {tidyLPA}’s estimate_profiles() function. Here’s what I’m doing in the step below:\n\nSetting the n_profiles option to 1:5, which will fit models with 1 profile, 2 profiles, …, 5 profiles.\nSetting the variances option to equal and varying. This will fit 1-5 profile models where the conditional variances of each indicator are constrained to be equal across profiles as well as 1-5 profile models where the conditional variances are allowed to vary.\nSetting covariances equal to 0. The means that indicator variables are conditionally independent.\nTelling the model to only use the centered ability score variables to estimate the profiles.\n\nAlso! always remember to set your seed.\n\nset.seed(0408)\nlpa_fits <- mons_centered %>%\n  estimate_profiles(1:5,\n                    variances = c(\"equal\", \"varying\"),\n                    covariances = c(\"zero\", \"zero\"),\n                    select_vars = str_subset(names(mons_centered), \"centered\"))\n\nNow that the model is fit, I want to select the best model. {tidyLPA} makes this pretty easy via the compare_solutions() function, which I’ll use later, but I also want to explore a few fit indices – Bayesian Information Criterion (BIC) and entropy. I’ve forgotten what BIC actually is, but I do know that it’s the fit index that people who know more about statistics than I do suggest we want to minimize in LPA. Entropy is a measure of how distinct classes are from one another and ranges from 0-1, where values closer to 1 represent more separation between classes (which is what we want). I’m going to pull these out and plot them.\n\nmods <- names(lpa_fits)\n#recall that model 1 corresponds to equal variances; model 2 corresponds to varying variances\n\n#getting some fit indices\nbics <- map_dbl(1:10, ~pluck(lpa_fits, .x, \"fit\", \"BIC\"))\nentrops <- map_dbl(1:10, ~pluck(lpa_fits, .x, \"fit\", \"Entropy\"))\n\nfit_indices <- bind_cols(mods, bics, entrops) %>%\n  set_names(c(\"model\", \"bic\", \"entrop\")) %>%\n  pivot_longer(cols = c(\"bic\", \"entrop\"),\n               names_to = \"metric\",\n               values_to = \"val\")\n\nfit_indices %>%\n  ggplot(aes(x = val, y = reorder_within(model, val, metric), fill = metric)) +\n  geom_col() +\n  geom_text(aes(label = if_else(val > 1, round(val, 0), round(val, 3)), x = val - .01), hjust = 1, color = \"white\") +\n  facet_wrap(vars(metric), scales = \"free\") +\n  scale_y_reordered() +\n  labs(\n    y = \"Model\",\n    x = \"Value\",\n    title = \"Selected Fit Indices\"\n  ) +\n  theme(\n    legend.position = \"none\"\n  )\n\n\n\n\nA couple things to keep in mind here:\n\nModels with 1 class will always have an entropy of 1 since there’s no overlap between classes.\nModel 1, Class 5 (equal variances, 5 profiles) threw a warning indicating that < 1% of observations were classified into one profile, so we probably don’t want to use that one.\n\nWe can see in the above plot that Model 2, Class 5 (varying variances, 5 profiles) has the lowest BIC (other than the model we want to ignore anyway) and the highest entropy (other than the 1-profile models). So that’s the one we likely want to go with, but let’s see what compare_solutions() tells me.\n\nlpa_fits %>%\n  compare_solutions()\n\nCompare tidyLPA solutions:\n\n Model Classes BIC      \n 1     1       10599.528\n 1     2       10366.807\n 1     3       10237.207\n 1     4       10145.513\n 1     5       10127.963\n 2     1       10599.528\n 2     2       10383.456\n 2     3       10230.918\n 2     4       10160.966\n 2     5       10127.499\n\nBest model according to BIC is Model 2 with 5 classes.\n\nAn analytic hierarchy process, based on the fit indices AIC, AWE, BIC, CLC, and KIC (Akogul & Erisoglu, 2017), suggests the best solution is Model 2 with 5 classes."
  },
  {
    "objectID": "posts/dungeons-and-dragons-part-3/index.html#examine-profiles",
    "href": "posts/dungeons-and-dragons-part-3/index.html#examine-profiles",
    "title": "Dungeons and Dragons - Part 3",
    "section": "Examine Profiles",
    "text": "Examine Profiles\nNow that we’ve selected a model specification, let’s look at what each profile looks like. The plot below shows the average bin-centered ability scores for each profile.\n\nprof_estimates <- get_estimates(lpa_fits) %>%\n  filter(Model == 2,\n         Classes == 5)\n\nprof_estimates %>%\n  filter(Category == \"Means\") %>%\n  mutate(Class = str_replace_all(Class, \"^\", \"Profile \"),\n         Parameter = str_remove_all(Parameter, \"_centered\")) %>%\n  ggplot(aes(x = Estimate, y = Parameter)) +\n  geom_col(aes(fill = if_else(Estimate > 0, TRUE, FALSE))) +\n  facet_wrap(vars(Class)) +\n  labs(\n    y = NULL,\n    x = \"CR-Centered Ability Score\",\n    title = \"Ability Score Means by Profile\",\n    caption = \"5 profiles, varying variances\"\n  ) +\n  theme(\n    legend.position = \"none\"\n  )\n\n\n\n\nProfiles 1 and 4 aren’t terribly interesting – they represent monsters that have all ability scores either higher or lower than the bin average. This is probably an artifact of the challenge rating variation within each bin (especially the higher ones). Profiles 1, 3, and 5 are interesting, though.\n\nProfile 2 seems to represent spell-casting monsters with lower strength and constitution but higher intelligence, dexterity, and charisma.\nLikewise, Profile 3 looks like a different type of spellcaster, with very high intelligence and charisma.\nProfile 5 is more of a fighter-type monster, with higher strength and constitution but low intelligence and charisma.\n\nNext, I’ll check how many monsters ended up in each class. I didn’t get a warning for this model suggesting that a class had dangerously low membership, but it’s still good practice to check this.\n\nclass_assigns <- get_data(lpa_fits) %>%\n  filter(model_number == 2,\n         classes_number == 5) %>%\n  group_by(id) %>%\n  filter(Probability == max(Probability)) %>%\n  ungroup() %>%\n  select(name, Class, Probability)\n\nclass_assigns %>%\n  count(Class)\n\n# A tibble: 5 × 2\n  Class     n\n  <dbl> <int>\n1     1    68\n2     2    69\n3     3    32\n4     4    45\n5     5   120\n\n\nThis feels reasonable to me. The smallest class (Class 3) still comprises ~10% of the total monsters in the dataset.\nFinally, let’s take a quick look at one example monster from each class as well as the estimated probability that the mosnter belongs to that class (note that the assigned/estimated class is the class for each monster with the highest probability.)\n\nset.seed(0409)\n\nclass_assigns %>%\n  group_by(Class) %>%\n  sample_n(size = 1) %>%\n  ungroup() %>%\n  select(Class, name, Probability) %>%\n  gt() %>%\n  tab_header(\n    title = \"Example Monster for Each Class\"\n  ) %>%\n  cols_label(\n    Class = \"Esimated Class\",\n    name = \"Monster\",\n    Probability = \"Estimated Class Prob\"\n  ) %>%\n  fmt_percent(\n    columns = vars(Probability)\n  )\n\n\n\n\n\n  \n    \n      Example Monster for Each Class\n    \n    \n  \n  \n    \n      Esimated Class\n      Monster\n      Estimated Class Prob\n    \n  \n  \n    1\nGrimlock\n96.19%\n    2\nDrider\n98.36%\n    3\nVeteran\n99.14%\n    4\nScorpion\n64.04%\n    5\nSwarm of Rats\n89.60%\n  \n  \n  \n\n\n\n\nThat’s it for now! Hopefully that was interesting to people who like D&D and helpful for anyone interested in LPA. I’ll probably do one more post in this series, and likely one that gets into some predictive modeling."
  },
  {
    "objectID": "posts/dungeons-and-dragons-part-4/index.html",
    "href": "posts/dungeons-and-dragons-part-4/index.html",
    "title": "Dungeons and Dragons - Part 4",
    "section": "",
    "text": "Before getting into this, I want to give a shoutout to Julia Silge’s recent-ish blog post that gave me the idea (and some of the code) to explore model diagnostics via {tidymodels}. So let’s get going!\n\nSetup\n\nlibrary(tidyverse)\nlibrary(eemisc) #ggplot theme\nlibrary(jsonlite) #work with json data\nlibrary(harrypotter) #colors\nlibrary(tidymodels)\nlibrary(finetune)\nlibrary(vip)\nlibrary(tidytext)\n\nherm <- harrypotter::hp(n = 1, option = \"HermioneGranger\")\n\nopts <- options(\n  ggplot2.discrete.fill = list(\n    harrypotter::hp(n = 3, option = \"HermioneGranger\"),\n    harrypotter::hp(n = 7, option = \"Always\")\n  )\n)\n\ntheme_set(theme_ee())\n\ndnd_base <- \"https://www.dnd5eapi.co/api/monsters/\"\n\n#getting data from api -- see 1st d&d post\n#for process explanation\n\nfetch_monster <- function(monster) {\n  dnd_url <- \"https://www.dnd5eapi.co/api/monsters/\"\n  \n  ret <- fromJSON(paste0(dnd_url, monster)) %>%\n    enframe() %>%\n    pivot_wider(names_from = name,\n                values_from = value)\n  \n  return(ret)\n}\n\ncompare_lens <- function(x, size = 1) {\n  all(map_lgl(x, ~length(unlist(.x)) == size))\n}\ncond_unlist <- function(x) {\n  if (compare_lens(x) == TRUE) {\n    unlist(x)\n  } else {\n    x\n  }\n}\n\nmons <- fromJSON(dnd_base)$results %>%\n  pull(index)\n\nmonster_lists <- purrr::map(mons, fetch_monster)\n\nmons_bind <- bind_rows(monster_lists)\n\nmons_df <- mons_bind %>%\n  mutate(across(.cols = everything(), ~cond_unlist(x = .x)))\n\nLet’s take a look at our data.\n\nglimpse(mons_df)\n\nRows: 334\nColumns: 31\n$ index                  <chr> \"aboleth\", \"acolyte\", \"adult-black-dragon\", \"ad…\n$ name                   <chr> \"Aboleth\", \"Acolyte\", \"Adult Black Dragon\", \"Ad…\n$ size                   <chr> \"Large\", \"Medium\", \"Huge\", \"Huge\", \"Huge\", \"Hug…\n$ type                   <chr> \"aberration\", \"humanoid\", \"dragon\", \"dragon\", \"…\n$ alignment              <chr> \"lawful evil\", \"any alignment\", \"chaotic evil\",…\n$ armor_class            <int> 17, 10, 19, 19, 18, 19, 18, 19, 19, 19, 19, 18,…\n$ hit_points             <int> 135, 9, 195, 225, 172, 212, 184, 256, 207, 256,…\n$ hit_dice               <chr> \"18d10\", \"2d8\", \"17d12\", \"18d12\", \"15d12\", \"17d…\n$ speed                  <list> [\"10 ft.\", \"40 ft.\"], [\"30 ft.\"], [\"40 ft.\", \"…\n$ strength               <int> 21, 10, 23, 25, 23, 25, 23, 27, 23, 27, 27, 22,…\n$ dexterity              <int> 9, 10, 14, 10, 10, 10, 12, 14, 12, 10, 10, 10, …\n$ constitution           <int> 15, 10, 21, 23, 21, 23, 21, 25, 21, 25, 25, 22,…\n$ intelligence           <int> 18, 10, 14, 16, 14, 16, 18, 16, 18, 16, 16, 8, …\n$ wisdom                 <int> 15, 14, 13, 15, 13, 15, 15, 15, 15, 13, 13, 12,…\n$ charisma               <int> 18, 11, 17, 19, 17, 19, 17, 24, 17, 21, 21, 12,…\n$ proficiencies          <list> [<data.frame[5 x 2]>], [<data.frame[2 x 2]>], …\n$ damage_vulnerabilities <list> [], [], [], [], [], [], [], [], [], [], [], []…\n$ damage_resistances     <list> [], [], [], [], [], [], [], [], [], [], [], []…\n$ damage_immunities      <list> [], [], \"acid\", \"lightning\", \"fire\", \"lightnin…\n$ condition_immunities   <list> [], [], [], [], [], [], [], [], [<data.frame[1…\n$ senses                 <list> [\"120 ft.\", 20], [12], [\"60 ft.\", \"120 ft.\", 2…\n$ languages              <chr> \"Deep Speech, telepathy 120 ft.\", \"any one lang…\n$ challenge_rating       <dbl> 10.00, 0.25, 14.00, 16.00, 13.00, 15.00, 14.00,…\n$ xp                     <int> 5900, 50, 11500, 15000, 10000, 13000, 11500, 18…\n$ special_abilities      <list> [<data.frame[3 x 3]>], [<data.frame[1 x 3]>], …\n$ actions                <list> [<data.frame[4 x 7]>], [<data.frame[1 x 4]>], …\n$ legendary_actions      <list> [<data.frame[3 x 4]>], [], [<data.frame[3 x 4]…\n$ url                    <chr> \"/api/monsters/aboleth\", \"/api/monsters/acolyte…\n$ subtype                <list> <NULL>, \"any race\", <NULL>, <NULL>, <NULL>, <N…\n$ reactions              <list> <NULL>, <NULL>, <NULL>, <NULL>, <NULL>, <NULL>…\n$ forms                  <list> <NULL>, <NULL>, <NULL>, <NULL>, <NULL>, <NULL>…\n\n\nThere’s a ton of data here, and a lot of it is still in deep-list-land. If this were a “real” project (i.e. not a blog post and something with stakes tied to it), I’d probably going digging more through these lists to search for useful features. But since this is just a blog post, I’m largely going to focus on the easy-to-use data (i.e. stuff that’s already a good-old atomic vector).\n\n\nFeature Engineering\nThat said, one feature I do want to add is whether or not the monster is a spellcaster. And because I’ve dug around in this data a little bit before (and because I play D&D), I know this is contained within the “special abilities” list-column. So, I’m going to enlist some help from {purrr}'s map_int() and pluck() to dig into this column, identify monsters that have a can cast spells (they’ll have an ability called either “Spellcasting” or “Innate Spellcasting”), and then create a binary yes/no feature.\nBeyond that, I’m going to keep a handful of other potentially useful features: - size (a nominal feature ranging from “tiny” to “gargantuan”), - type (a nominal feature indicating whether the monster is a humanoid, beast, dragon, etc), - armor class (a numeric feature indicating how much armor a monster has), and - all of the ability scores (strength through intelligence; all numeric)\n\nmons_df_small <- mons_df %>%\n  mutate(spellcaster = map_int(seq_along(1:nrow(mons_df)), ~pluck(mons_df$special_abilities, .x, \"name\") %>%\n                                 paste(collapse = \", \") %>%\n                                 str_detect(\"Spellcast\"))) %>%\n  select(index, size, type, armor_class, strength, dexterity, constitution, wisdom, charisma, intelligence, spellcaster, challenge_rating)\n\n\n\nSplitting Data\nNow I can get into the {tidymodels} flow of splitting data, specifying a recipe, specifying a model, tuning the model, etc. We’ll use bootstrapping here rather than cross validation to split our data because we have a pretty small sample size in the training set (268 obs).\n\nset.seed(0408)\nmons_split <- initial_split(mons_df_small, strata = challenge_rating, prop = 4/5)\ntrn <- training(mons_split)\ntst <- testing(mons_split)\n\n#and also getting our folds\nbooties <- bootstraps(trn)\n\n\n\nPreprocessing with Recipes\nI’m going to do some pretty minimal preprocessing here. There’s more I could do (and I’ll revisit some later, actually), but for now I’m just going to:\n\nTell the model not to use index, which is an id column, in the model,\nCreate an “other” category for type (since there are many types, some with low counts),\nScale all of the numeric variables, and\nDummy-out the size and type variables.\n\n\nmons_rec <- recipe(challenge_rating ~ ., data = trn) %>%\n  update_role(index, new_role = \"id_var\") %>%\n  step_other(type) %>%\n  step_scale(armor_class, strength, dexterity, constitution, wisdom, charisma, intelligence) %>%\n  step_dummy(size, type)\n\n\n\nSetting Model Specifications\nNext, I’ll specify the model I want to fit. Again, there are lots of options here, and if I wanted the best-performing model, I might use xgboost or catboost or something, but I’m just going to stick with a linear model here because I think it will do decently well and they’re faster. More specifically, I’m going to use a lasso model to regularize the regression and potentially do some feature selection for me.\n\nlasso_spec <- linear_reg(penalty = tune(), mixture = 1) %>%\n  set_engine(\"glmnet\")\n\n#and combining into a workflow\nlasso_wf <- workflow() %>%\n  add_recipe(mons_rec) %>%\n  add_model(lasso_spec)\n\n\n\nFitting Model\nNow we fit the model. Even though this should be quick to fit, I’m going to use tune_race_anova() from the {finetune} package (rather than, say, tune_grid()) to speed up the process a little bit (see Max Kuhn’s video from rstudioconf 2021 for more details about this).\n\ndoParallel::registerDoParallel()\nset.seed(0408)\n\nlasso_res <- tune_race_anova(\n  lasso_wf,\n  resamples = booties\n)\n\n\n\nModel Selection\nNow that the models are fit, I’ll look at the accuracy metrics real quick via autoplot().\n\nautoplot(lasso_res)\n\n\n\n\nIt looks like the best model here has an R-squared of ~.85, which is really good (well, I’m used to modeling education data, where an R-squared of .85 is obscenely high, but I suppose other people’s mileage may vary). I’m going to select the simplest model here that is within 1 standard error of the numerically best model, in the hopes that this will give me some feature selection as well. And once I select that, I’m going to finalize the workflow and use last_fit() to train the model with the selected parameters on the full training set and then evaluate it on the test set.\n\nparams <- select_by_one_std_err(lasso_res, metric = \"rmse\", penalty)\n\nlasso_fin_wf <- finalize_workflow(lasso_wf, params)\n\n#and doing our last fit\nlasso_fin_fit <- last_fit(lasso_fin_wf, mons_split)\n\nFrom there, we can check out the final model’s performance on the test set.\n\n#check out final test set performance\ncollect_metrics(lasso_fin_fit)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       2.58  Preprocessor1_Model1\n2 rsq     standard       0.818 Preprocessor1_Model1\n\n\nOur R-squared on the test set is .871, which is even better than we did on our bootstraps earlier. Not bad!\n\n\nDiagnosing Model\nWe could stop here, but I think it’s worthwhile to dig into our model a bit more to see if anything stands out/if there’s a way we could improve it. To do this, I’m going to look at a few plots:\n\nthe predicted values vs the actual values, and\nthe predicted values vs the residuals\n\nFirst, let’s look at predicted values vs the actual challenge ratings:\n\ncollect_predictions(lasso_fin_fit) %>%\n  ggplot(aes(x = challenge_rating, y = .pred)) +\n  geom_abline(lty = 2) +\n  geom_point(color = herm, alpha = .4)\n\n\n\n\nSo, there are a number of things that stand out to me. First, it’s clearly not a bad model, but there are some areas where the model is missing by quite a bit. For instance, there’s quite a bit of variability in predictions for low-CR monsters, and even some negative predictions (which isn’t possible).\nLet’s also take a look at the residuals vs the predictions.\n\naugment(lasso_fin_fit) %>%\n  ggplot(aes(x = .pred, y = .resid)) +\n  geom_point(color = herm, alpha = .4) +\n  geom_smooth(color = \"black\")\n\n\n\n\nWhat we’d want to see here is basically no pattern, and a constant variance in the residuals, which isn’t quite what we get here (although this is going to be somewhat harder to see with a small sample, since this is just plotting the 64-observation test data).\nAgain – this isn’t a terrible model, but there are a few things we could do to improve it. One would be to drop the Tarrasque observation, since it’s an extreme CR (it has a CR of 30, which is well beyond any other monster). It doesn’t show up in the plot above, but I know from previous data exploration that it’s different from other monsters.\nAnother approach is to log-transform challenge_rating (our DV), since I know from previous exploration that it has a strong right skew. This might help with unequal variances in the error terms.\n(n.b. that there are other approaches we could take, too, including fitting different type of model or doing some more feature engineering).\n\n\nRefitting with a Log-Transformed DV\nI won’t walk through everything here, but I’m basically redoing all of the previous steps, but adding a log transformation to challenge_rating.\n\nmons_rec2 <- recipe(challenge_rating ~ ., data = trn) %>%\n  update_role(index, new_role = \"id_var\") %>%\n  step_log(all_outcomes(), offset = .1) %>%\n  step_other(type) %>%\n  step_scale(armor_class, strength, dexterity, constitution, wisdom, charisma, intelligence) %>%\n  step_dummy(size, type)\n\nlasso_wf2 <- workflow() %>%\n  add_recipe(mons_rec2) %>%\n  add_model(lasso_spec)\n\n#fitting model\ndoParallel::registerDoParallel()\nset.seed(0408)\n\nlasso_res2 <- tune_race_anova(\n  lasso_wf2,\n  resamples = booties\n)\n\nparams2 <- select_by_one_std_err(lasso_res2, metric = \"rmse\", penalty)\n\nlasso_fin_wf2 <- finalize_workflow(lasso_wf2, params2)\n\n#and doing our last fit\nlasso_fin_fit2 <- last_fit(lasso_fin_wf2, mons_split)\n\ncollect_metrics(lasso_fin_fit2)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.739 Preprocessor1_Model1\n2 rsq     standard       0.796 Preprocessor1_Model1\n\n\nWe see that this model gives us basically the same R-squared. The RMSE isn’t directly comparable since we’ve log-transformed the outcome. But let’s take a look at our predictions. To do that, I first need to recalculate the predictions and residuals to account for the log scale here.\n\nlasso_fit2_aug <- augment(lasso_fin_fit2) %>%\n  mutate(pred = exp(.pred),\n         resid = challenge_rating - pred)\n\nlasso_fit2_aug %>%\n  ggplot(aes(x = challenge_rating, y = pred)) +\n  geom_point(color = herm, alpha = .4) +\n  geom_abline(lty = 2)\n\n\n\n\nOk, so, this model does a lot better for very low CR monsters than the previous model did. And recall that most monsters are low CR. However, it seems to perform worse for very high CR monsters – we can see that it’s predicting a CR of over 35 for a monster with an actual CR of 24, which is a pretty big miss.\nWe can see something similar when plotting our residuals vs predictions. Through, say, CR 10, the model seems decent, but not so much after that.\n\nlasso_fit2_aug %>%\n  ggplot(aes(x = pred, y = resid)) +\n  geom_point(color = herm, alpha = .4) +\n  geom_smooth(color = \"black\")\n\n\n\n\nSo which of these is the better model? Neither is ideal, obviously, but it depends on what you want to do. The first model seems to be more stable (but not great) across all possible CR values (although let’s not forget that it gave us some negative predictions, which isn’t good). The second model is much better at predicting low CR monsters but much worse at predicting high CR monsters. I sort of like the 2nd one better since low CR monsters are much more common.\n\n\nInterpreting Coefficients\nFinally, let’s interpret the coefficients of this second model. Again, bear in mind that these are the coefficients of a model that does not do a good job at predicting high CR monsters. I’m going to facet these coefficients out so that things in the same facet are roughly comparable.\n\nlasso_coefs <- pull_workflow_fit(lasso_fin_fit2$.workflow[[1]]) %>%\n  vi()\n\nlasso_coefs %>%\n  mutate(Importance = if_else(Sign == \"NEG\", -1*Importance, Importance)) %>%\n  mutate(coef_type = case_when(\n    str_detect(Variable, \"type\") ~ \"Monster Type\",\n    str_detect(Variable, \"size\") ~ \"Monster Size\",\n    TRUE ~ \"Other\"\n  )) %>%\n  ggplot(aes(y = reorder_within(Variable, Importance, coef_type), x = Importance)) +\n  geom_col(aes(fill = Importance > 0)) +\n  facet_wrap(vars(coef_type), scales = \"free\") +\n  scale_y_reordered() +\n  labs(\n    y = NULL,\n    x = \"Beta\"\n  ) +\n  theme(\n    legend.position = \"none\",\n    plot.margin = margin(t = 14, b = 14, l = 7, r = 7)\n  )\n\n\n\n\nI’d take the “size” stuff here with a grain of salt, since the reference category is “gargantuan” and those are the monsters it does the worst with. The “type” coefficients make sense to me – the reference category is “beast,” and I’d expect those to generally have lower CRs than like fiends (demons & whatnot), monstrosities, etc. And of the coefficients in “other,” it’s no surprise that constitution is the strongest predictor – regardless of how a monster fights or what you expect them to do, harder monsters will have more health. We also see that our spellcaster binary feature (which is not on the same scale as the others in this facet) has a positive effect.\nThat’s going to be it for now, and probably the end of this little D&D series of posts. There’s a lot more that could be done with this data – both with the monster data and with other data available through the API – so who knows, I may pick it back up at some point.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-nc/4.0/CitationBibTeX citation:@online{ekholm2021,\n  author = {Eric Ekholm},\n  title = {Dungeons and {Dragons} - {Part} 4},\n  date = {2021-03-10},\n  url = {https://www.ericekholm.com/posts/dungeons-and-dragons-part-4},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEric Ekholm. 2021. “Dungeons and Dragons - Part 4.” March\n10, 2021. https://www.ericekholm.com/posts/dungeons-and-dragons-part-4."
  },
  {
    "objectID": "posts/fitting-a-multiple-regression-with-torch/index.html",
    "href": "posts/fitting-a-multiple-regression-with-torch/index.html",
    "title": "Fitting a Multiple Regression with Torch",
    "section": "",
    "text": "In this post, I want to play around with the {torch} package a little bit by fitting a multiple regression model “by hand” (sort of) using torch and the Adam optimizer.\nA few warnings/disclaimers right up front:\nAll of that said, if you’re still with me, let’s dive in."
  },
  {
    "objectID": "posts/fitting-a-multiple-regression-with-torch/index.html#creating-a-dataset",
    "href": "posts/fitting-a-multiple-regression-with-torch/index.html#creating-a-dataset",
    "title": "Fitting a Multiple Regression with Torch",
    "section": "Creating a Dataset",
    "text": "Creating a Dataset\nRight, so, now we can get into the torch-y stuff. The first step is to use the dataset() constructor to build a dataset. According to the torch documentation, this requires following a few conventions. More specifically, we need to establish an initialize() function, a .getitem() function, and a .length() function.\nBasically, these do the following:\n\ninitialize() creates x (predictor) and y (outcome) tensors from the data;\n.getitem() provides a way to return the x and y values for an item when provided an index (or multiple indices) by the user;\n.length() tells us how many observations we have in the data\n\nWe can also define helper functions within dataset() as well (e.g. preprocessors for our data). I’m not going to do that here (since we’ve already lightly preprocessed our data), but I could if I wanted.\n\n#initializing dataset\n\nultra_dataset <- dataset(\n  \n  name = \"ultra_dataset\",\n  \n  initialize = function(df) {\n    self$x <- df %>%\n      select(-time_in_seconds) %>%\n      as.matrix() %>%\n      torch_tensor()\n    \n    self$y <- torch_tensor(df$time_in_seconds)\n    \n  },\n    \n    .getitem = function(i) {\n      x <- self$x[i, ]\n      y <- self$y[i]\n      \n      list(x, y)\n    },\n    \n    .length = function() {\n      self$y$size()[[1]]\n    }\n)\n\nLet’s see what this looks like. We’ll create a tensor dataset from the full ultra_normed data and then return its length:\n\nultra_tensor_df <- ultra_dataset(ultra_normed)\n\n\nultra_len <- ultra_tensor_df$.length()\n#note that this is the same as: length(ultra_tensor_df)\n\nultra_len\n\n[1] 1237\n\n\nWe can also pull out a single observation if we want, and the result will give us the values in the X tensor and the y tensor:\n\nultra_tensor_df$.getitem(1)\n\n[[1]]\ntorch_tensor\n-0.3624\n 0.2811\n-0.2866\n[ CPUFloatType{3} ]\n\n[[2]]\ntorch_tensor\n-0.809365\n[ CPUFloatType{} ]\n\n#note that 1 here refers to the index of the item\n\nNext, let’s make train and validation datasets.\n\nset.seed(0408)\ntrain_ids <- sample(1:ultra_len, floor(.8*ultra_len))\nvalid_ids <- setdiff(1:ultra_len, train_ids)\n\ntrn <- ultra_dataset(ultra_normed[train_ids, ])\nvld <- ultra_dataset(ultra_normed[valid_ids, ])\n\nThis would be the point where we could also define a dataloader to train on batches of the data, but I’m not going to do that here because we can just train on the entire dataset at once."
  },
  {
    "objectID": "posts/fitting-a-multiple-regression-with-torch/index.html#defining-a-model",
    "href": "posts/fitting-a-multiple-regression-with-torch/index.html#defining-a-model",
    "title": "Fitting a Multiple Regression with Torch",
    "section": "Defining a Model",
    "text": "Defining a Model\nNow, let’s define our model. Again, for our learning purposes today, this is just going to be a plain old multiple regression model. To implement this in {torch}, we can define the model as follows:\n\nlin_mod <- function(x, w, b) {\n  torch_mm(w, x) + b\n}\n\nIn this model, we’re taking a vector of weights (or slopes), w, multiplying it by our input matrix, x, and adding our bias (or intercept). The torch_mm() function lets us perform this matrix multiplication.\nNow that we’ve defined this model, let’s create our w and b parameters. Since this is a linear regression, each predictor in our model will have a single weight associated with it, and we’ll have a single intercept for the model. We’ll just use 1 as the starting value for our w parameters and 0 as the starting value for our b parameter.\n\n#defining parameters\nnum_feats <- 3\n\nw <- torch_ones(c(1, num_feats))\nb <- torch_zeros(1)\n\nNow we can do a quick test to make sure everything fits together. We’re not actually training our model at this point, but I want to just run a small sample of our training data through the model (with the parameter starting values) to make sure we don’t get any errors.\nNote that I need to transpose the X matrix for the multiplication to work.\n\naa <- trn$.getitem(1:10)\n\naa_x <- torch_transpose(aa[[1]], 1, 2)\n\nt_out <- lin_mod(aa_x, w, b)\n\nt_out\n\ntorch_tensor\n-0.2315 -0.4256 -0.7237 -0.2182 -0.2279 -0.2015 -0.2869 -0.2124 -0.5832 -0.2300\n[ CPUFloatType{1,10} ]\n\n\nGreat! This gives us a single output for each of our input observations, which is what we want."
  },
  {
    "objectID": "posts/fitting-a-multiple-regression-with-torch/index.html#training-the-model",
    "href": "posts/fitting-a-multiple-regression-with-torch/index.html#training-the-model",
    "title": "Fitting a Multiple Regression with Torch",
    "section": "Training the Model",
    "text": "Training the Model\nNow that we have a model and can feed data into the model, let’s train it.\nTraining the model involves using gradient descent, an optimizer, a loss function, and backpropagation to slowly tweak our parameters until they reach their optimal values (i.e. those that minimize loss). I’m not going to do a super deep dive into what all of that means, but basically in our training loop we’re going to:\n\nRun the data through the model and get predictions;\nMeasure how good our predictions are (via the loss function);\nCompute the gradient of the loss with respect to the parameters (via the backward() method);\nTell our optimizer to update the parameters (via optimizer$step());\nRepeat a bunch of times\n\nThat’s basically what the code below does. A few little extra things to point out, thought:\n\nIn addition to training the model on the training data, I’m also getting predictions on the validation data during each iteration of the training process. This won’t influence the training at all, but it’ll give us a look at how the model does on a holdout set of data throughout the entire process.\nThe torch_squeeze() function just removes an unnecessary dimension from the predictions tensors.\nI’ve also created lists to track training loss, validation loss, and parameter values throughout the fitting, and these get recorded on each pass through the training loop.\n\n\n#recreate our parameters with the requires_grad attribute\nw <- torch_zeros(c(1, num_feats), requires_grad = TRUE)\nb <- torch_zeros(1, requires_grad = TRUE)\n\n#put the parameters in a list\nparams <- list(w, b)\n\n#define our optimizer\noptimizer <- optim_adam(params, lr = .1)\n\n#create lists to track values during the training\nloss_tracking <- list()\nparams_tracking <- list()\nvld_loss_tracking <- list()\n\n#training loop\nfor (i in 1:1000) {\n  \n  optimizer$zero_grad()\n  \n  x <- torch_transpose(trn$x, 1, 2)\n  vld_x <- torch_transpose(vld$x, 1, 2)\n  \n  preds <- lin_mod(x, w, b)\n  vld_preds <- lin_mod(vld_x, w, b)\n  \n  preds <- torch_squeeze(preds)\n  vld_preds <- torch_squeeze(vld_preds)\n  \n  current_loss <- nnf_mse_loss(preds, trn$y)\n  vld_loss <- nnf_mse_loss(vld_preds, vld$y)\n  \n  loss_tracking[i] <- current_loss$item()\n  vld_loss_tracking[i] <- vld_loss$item()\n  params_tracking[i] <- list(c(as.numeric(params[[1]]), as.numeric(params[[2]])))\n  \n  current_loss$backward()\n  \n  optimizer$step()\n  \n}"
  },
  {
    "objectID": "posts/fitting-a-multiple-regression-with-torch/index.html#investigating-our-results",
    "href": "posts/fitting-a-multiple-regression-with-torch/index.html#investigating-our-results",
    "title": "Fitting a Multiple Regression with Torch",
    "section": "Investigating our Results",
    "text": "Investigating our Results\nCool stuff – our model has finished training now. Let’s take a look at our final parameter values. In a little while, we’ll also compare these to values we get from fitting a multiple regression using the lm() function.\n\nbetas <- tibble(\n  term = c(names(ultra_normed)[2:4], \"intercept\"),\n  size = params_tracking[[1000]]\n)\n\nbetas\n\n# A tibble: 4 × 2\n  term              size\n  <chr>            <dbl>\n1 distance       -0.154 \n2 elevation_gain  0.906 \n3 elevation_loss  0.314 \n4 intercept       0.0144\n\n\nNext, let’s take a look at how the parameter values (minus the intercept) change throughout the training loop/fitting process.\n\ndescent_tibble <- function(i, inp) {\n  tibble(\n    iter = i,\n    distance = inp[[i]][1],\n    elevation_gain = inp[[i]][2],\n    elevation_loss = inp[[i]][3]\n  )\n}\n\nparams_fitting_tbl <- map_dfr(1:1000, ~descent_tibble(.x, params_tracking)) %>%\n  pivot_longer(cols = -iter)\n\nparams_fitting_tbl %>%\n  ggplot(aes(x = iter, y = value, color = name)) +\n  geom_line() +\n  scale_color_hp_d(option = \"HermioneGranger\")\n\n\n\n\nWe probably could have trained for fewer iterations, but it’s a small dataset and a simple model, so whatever.\nNow, let’s see what the coefficients of a “standard” multiple regression (fit using lm()) look like. This will serve as our “ground truth” and will tell us if our gradient descent fitting process arrived at the “right” coefficient values:\n\nmod_res <- lm(time_in_seconds ~ distance + elevation_gain + elevation_loss, data = ultra_normed[train_ids, ])\n\nmod_res\n\n\nCall:\nlm(formula = time_in_seconds ~ distance + elevation_gain + elevation_loss, \n    data = ultra_normed[train_ids, ])\n\nCoefficients:\n   (Intercept)        distance  elevation_gain  elevation_loss  \n       0.01438        -0.15403         0.90636         0.31391  \n\n\nGood stuff! If we look back up at the coefficients from our torch model, we can see that they’re (nearly) identical to those from this lm() model – which is what we want.\nAs a final step, let’s look at the loss of the model throughout the training process on both the training set and the validation set. This will give us a sense of how our model “learns” throughout the process.\nAs sort of an aside – we’d typically look at these metrics as a way to examine overfitting, which is a big problem for neural networks and more complex models. However, we’re not running a complex model. Linear models pretty much can’t overfit, so this is a less useful diagnostic here. But let’s take a look anyway.\n\n#checking out loss during training\nloss_metrics <- tibble(\n  iter = 1:1000,\n  trn_loss = unlist(loss_tracking),\n  vld_loss = unlist(vld_loss_tracking)\n) %>%\n  pivot_longer(\n    cols = -iter\n  )\n\nloss_metrics %>%\n  ggplot(aes(x = iter, y = value, color = name)) +\n  geom_line() +\n  scale_color_hp_d(option = \"HermioneGranger\")\n\n\n\n\nRight, so this is pretty much what we’d expect. Both losses drop in the first few iterations and then level off. The fact that both losses flatline indicates that we’re not overfitting, which again is what we expect with a linear model. We also expect our validation loss to be higher than the training loss, because the model hasn’t seen this data ever."
  },
  {
    "objectID": "posts/fitting-a-multiple-regression-with-torch/index.html#conclusion",
    "href": "posts/fitting-a-multiple-regression-with-torch/index.html#conclusion",
    "title": "Fitting a Multiple Regression with Torch",
    "section": "Conclusion",
    "text": "Conclusion\nThat’s it for now. We’ve learned how to write a ton of code to accomplish something we can do in a single-liner call to lm() :)\nI’m planning on digging into {torch} more and potentially writing a few more blogs once I get into actual neural networks with image and/or text data, but that’s for another day."
  },
  {
    "objectID": "posts/function-writing-metacognition/index.html",
    "href": "posts/function-writing-metacognition/index.html",
    "title": "Function Writing Metacognition",
    "section": "",
    "text": "If you’re a sane and respectable person, you keep percentages in your data formatted as decimals (e.g. 50% as .5, 71% as .71). However, you may also find that you need to present these numbers in reports/visuals/tables in a more readable way (i.e. as “50%”). If you’re like me, you’ll often find yourself creating additional columns that take your (correct) percent-as-decimal columns and turn them into strings. The other day, I realized that I was doing this a lot, and so I wrote a very simple function to handle this for me, which I added to my personal/miscellaneous R package, {eemisc}. Here’s that function:\nVery simple, very straightforward, but it’ll save me a little bit of typing. One common (for me) use case is:\nWhich directly prints our string-formatted percent on the bar.\nWhile writing this function, though, I made a few tweaks to the “base” version above, and I thought this would be a decent opportunity to write a metacognitive reflection on the process of developing this function. Hopefully this is helpful for people just starting out with writing R functions."
  },
  {
    "objectID": "posts/function-writing-metacognition/index.html#base-function",
    "href": "posts/function-writing-metacognition/index.html#base-function",
    "title": "Function Writing Metacognition",
    "section": "Base Function",
    "text": "Base Function\nRight, so, the point of this function is to take a percent (as a decimal) and turn it into a string. The base version of that function is the same as we presented above:\n\npct_to_string <- function(x) {\n    paste0(round(100 * x, digits = 1), \"%\")\n}\n\nAnd there’s nothing necessarily wrong with this. It works like we’d expect it to:\n\npct_to_string(.1)\n\n[1] \"10%\"\n\n\n\npct_to_string(.111)\n\n[1] \"11.1%\""
  },
  {
    "objectID": "posts/function-writing-metacognition/index.html#digits-argument",
    "href": "posts/function-writing-metacognition/index.html#digits-argument",
    "title": "Function Writing Metacognition",
    "section": "Digits Argument",
    "text": "Digits Argument\nThe vast majority of the time, I’ll want to present these string percentages with a single decimal place (e.g. “11.1%”). It’s pretty rare – at least in the contexts I work in – for the hundredths place in a percentage to matter much, and including it detracts more than it helps. If two “groups” score 11.12% and 11.14%, these are functionally identical in my mind.\nThat said, there may be some cases where I do want to include a hundredths decimal point. More likely, there may be cases where I don’t want to include any decimal points. In the base version of pct_to_string(), I hardcoded the function to provide 1 decimal place (digits = 1). But to allow for some flexibility, I want to make digits an argument. Since I’ll be setting it to 1 99.9% (see what I did there) of the time, I’ll just set 1 as the default. So now our function looks like this:\n\npct_to_string <- function(x, digits = 1) {\n    paste0(round(100 * x, digits = digits), \"%\")\n}\n\nAnd we can see this in action like so:\n\n#not specifying a digits argument will use the default of 1\npct_to_string(.1111)\n\n[1] \"11.1%\"\n\n\n\n#but we can specify a different number of digits if we want\npct_to_string(.1111, digits = 2)\n\n[1] \"11.11%\""
  },
  {
    "objectID": "posts/function-writing-metacognition/index.html#checking-bounds",
    "href": "posts/function-writing-metacognition/index.html#checking-bounds",
    "title": "Function Writing Metacognition",
    "section": "Checking Bounds",
    "text": "Checking Bounds\nMost often, the percent data I’m working with is bounded between 0 and 1 (0% - 100%). For instance, if I’m looking at the pass rates for standardized tests, I’m doing something wrong if I have a number greater than 1 or less than 0.\nAnother note is that, although I’m pretty consistent (insistent? both?) about formatting my percents as decimals, I sometimes pull data from sources where this isn’t the case, and it comes in as, e.g., 80.5 (rather than .805). The Virginia Department of Education tends to format their data this way.\nGiven both of these tidbits, I want to add an argument to pct_to_string() that checks if values of x are between 0 and 1. In my case, this is mostly to help catch mistakes before I make them. For instance, I want it to stop me if I try to multiply 80.5 * 100 because I didn’t realize the input was 80.5 and not .805. Additionally, because it’s so common that my percents at between 0 and 1, I want it to stop me if I’m working outside of this range.\nTo accomplish this, I’ll add a check_bounds argument to pct_to_string(). I want this to be a logical argument that, if set to TRUE, will stop the function from running if any values of x are less than 0 or greater than 1.\n\npct_to_string <- function(x, digits = 1, check_bounds = TRUE) {\n\n    if (check_bounds == TRUE & !(min(x) >= 0 & max(x) <= 1)) {\n        stop(\"all elements of `x` must be between 0 and 1. If you are intentionally using a percentage outside of these bounds, set `check_bounds = FALSE`\")\n    }\n\n    paste0(round(100 * x, digits = digits), \"%\")\n}\n\nSo let’s see how this works, now:\n\na <- seq(0, 1, .1)\n\n#this should work\npct_to_string(a)\n\n [1] \"0%\"   \"10%\"  \"20%\"  \"30%\"  \"40%\"  \"50%\"  \"60%\"  \"70%\"  \"80%\"  \"90%\" \n[11] \"100%\"\n\n\nNote that the below will throw an error, so I’m going to capture it using safely() from the {purrr} package and then return the error\n\nb <- seq(0, 1.1, .1)\n\nsafe_pts <- safely(pct_to_string)\n\nsafe_pts(b)$error\n\n<simpleError in .f(...): all elements of `x` must be between 0 and 1. If you are intentionally using a percentage outside of these bounds, set `check_bounds = FALSE`>\n\n\nOne note is that you could write this function to throw a warning or a message rather than an error depending on your needs. For my personal use cases, I think it makes more sense to throw an error rather than a warning, but your mileage my vary."
  },
  {
    "objectID": "posts/function-writing-metacognition/index.html#input-checks",
    "href": "posts/function-writing-metacognition/index.html#input-checks",
    "title": "Function Writing Metacognition",
    "section": "Input Checks",
    "text": "Input Checks\nA final thing I want to do is add a few statements to check that the input values I’m providing are valid. If this is a function that really is just for me, I might not do this (mostly out of laziness), but I’m also going to add it to a package that other people at my work use, so I think it makes sense to include these.\nBasically, these will just throw an error if you try to pass an invalid value to one of the function arguments. Like with the check_bounds piece earlier, this entials using an if statement to evaluate some parameter values, and then, if these are TRUE, to stop the function and instead return an error message. I also want to make sure that the error messages are actually helpful. We can add these like so:\n\npct_to_string <- function(x, digits = 1, check_bounds = TRUE) {\n    if (!is.numeric(x)) {\n        stop(\"`x` must be numeric\")\n    }\n\n    if (!is.integer(digits) | digits < 0) {\n        stop(\"`digits` must be a non-negative integer\")\n    }\n\n    if (!is.logical(check_bounds)) {\n        stop(\"`check_bounds` must be TRUE or FALSE\")\n    }\n\n    if (check_bounds == TRUE & !(min(x) >= 0 & max(x) <= 1)) {\n        stop(\"all elements of `x` must be between 0 and 1. If you are intentionally using a percentage outside of these bounds, set `check_bounds = FALSE`\")\n    }\n\n    paste0(round(100 * x, digits = digits), \"%\")\n}\n\n# and wrapping this with safely() again to show errors\nsafe_pts <- safely(pct_to_string)\n\nAnd we can see what happens if we pass in invalid values:\n\nsafe_pts(.1, digits = \"a\")$eror\n\nNULL\n\n\n\nsafe_pts(x = \"a\", digits = 0)$error\n\n<simpleError in .f(...): `x` must be numeric>\n\n\netc. etc.\nTechnically, you don’t need some of these checks. If you try to pass a non-numeric value (x) to round(), you’ll get an error. Likewise if you give it an invalid value for its digits argument:\n\n# first creating a safe function to catch error\nsafe_round <- safely(round)\n\nsafe_round(1.1, digits = \"a\")$error\n\n<simpleError in .Primitive(\"round\")(x, digits): non-numeric argument to mathematical function>\n\n\n\nsafe_round(\"a\", digits = 1)$error\n\n<simpleError in .Primitive(\"round\")(x, digits): non-numeric argument to mathematical function>\n\n\nBut these error messages aren’t quite as helpful as the ones we’ve written."
  },
  {
    "objectID": "posts/function-writing-metacognition/index.html#wrapping-up",
    "href": "posts/function-writing-metacognition/index.html#wrapping-up",
    "title": "Function Writing Metacognition",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nThat’s pretty much it – my thought process for creating a pretty simple function to convert percents to strings, as well as how I would build out this function. Hopefully this metacognitive activity was useful for people who are just starting out writing their own R functions!"
  },
  {
    "objectID": "posts/improving-ggplots-with-text-color/index.html",
    "href": "posts/improving-ggplots-with-text-color/index.html",
    "title": "Improving ggplots with Text Color",
    "section": "",
    "text": "Here’s my Bechdel Test #TidyTuesday plot as an example:\n\nYou’ll notice that, instead of having a legend off to the right or the bottom or wherever it is you typically place your legends, I’ve changed the font color of the words “Pass” and “Fail” in the title to contain this same information. We can do this using Claus Wilke’s invaluable {ggtext} package, which provides improved rendering for ggplot2.\nLet’s walk through how (and when) to do this, then!\nDisclaimer: I’m not a data viz expert, and there are tons of people who create visualizations for #TidyTuesday that regularly blow anything I make out of the water (see CedScherer or geokaramanis, for example). That said, what I want to walk through here is a useful, easy-to-implement trick that I use for a lot of plots I make for my job and doesn’t require you to be a ggplot wizard.\n\nSetup\nFirst, let’s load the packages we’ll need. I’m going to load:\n\n{tidyverse}, for {ggplot2} and other data wrangling tools;\n{ggtext}, for text rendering, and especially the element_markdown() function;\n{harrypotter}, for the colors I’m going to use.\n\nI’m also going to use {ggbeeswarm} to make a beeswarm plot, but if the plot you’re interested in making isn’t a beeswarm plot, then obviously you don’t need this. And, finally, I’ll use my personal miscellaneous package, {eemisc} in the very final example, since it has some custom ggplot theming in it that I like better than theme_minimal(), which I’ll use in the previous examples.\nIn this setup step, I’m also going to read in the Bechdel Test data from the #TidyTuesday Github repo and do some very light data processing to return the decade each movie was released in (rather than the year).\n\n#load packages\nlibrary(tidyverse)\nlibrary(ggtext)\nlibrary(eemisc)\nlibrary(harrypotter)\nlibrary(ggbeeswarm)\n\n#read in data\nmovies <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-03-09/movies.csv')\n\n#return decade a movie is released in\nmovies <- movies %>%\n  mutate(decade = year - (year %% 10))\n\n#getting the hex codes I want to use -- not strictly necessary to store these; I could just look them up\nluna <- hp(n = 2, option = \"LunaLovegood\")\n\n\n\nCreate the Initial Plot\nNow we’re ready to make a plot. First, I’ll make a “typical” plot – one with a legend describing the values that each color represents. I’ll make a beeswarm plot (similar to a violin plot, but with individual points rather than a density curve) using geom_quasirandom() to examine the IMDB rating distributions for movies that pass and fail the Bechdel test, by decade.\n\np_legend <- movies %>%\n  ggplot(aes(x = as_factor(decade), y = imdb_rating, color = binary, group = as_factor(binary))) +\n  geom_quasirandom(dodge.width = .7, width = .15, alpha = .6) +\n  scale_color_hp_d(option = \"LunaLovegood\", name = \"Bechdel Status\") +\n  theme_minimal() +\n  labs(\n    y = \"IMDB Rating\",\n    x = \"Decade\",\n    title = \"Movies that Pass and Fail the Bechdel Test Have Similar IMDB Rating\\nDistributions\",\n    caption = \"Data: FiveThirtyEight & IMDB | Viz: Eric Ekholm (@ekholm_e)\"\n  ) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title.position = \"plot\",\n    plot.caption.position = \"plot\"\n  )\n\np_legend\n\n\n\n\nSo, this isn’t a bad plot. It’s easy to read and the takeaway is fairly obvious. There are several tweaks we could make to improve it, though, and the one that stands out to me is to do something about the legend. It’s eating up a lot of space on the right-hand side of the graph to present a relatively small amount of information (albeit important information).\nOne approach might be to put the legend at the bottom of the plot. It will take up less space this way (and give the plot more space to expand horizontally), but it will still take up some space, and there’s another alternative…\n\n\nCreate a Legend-less Plot\nA better approach would be to use {ggtext} to color the text of the words “Pass” and “Fail” in the title to correspond to the colors of the points in the plot. I prefer this approach for a few reasons:\n\nPeople’s eyes will be drawn to the title anyway, so it makes reading the plot more efficient by allowing the title to pull double duty;\nIt frees up horizontal (or vertical) space in the plot itself, and;\nAs sort of an added bonus, it makes you think a little more carefully about your plot title.\n\nFortunately, {ggtext} lets us change the color text and labels in ggplots via the element_markdown() function, which we can use within the ggplot theme(). We can just pass some HTML directly into the title (or axis, or caption, or w/e) text and it will render appropriately.\nMore specifically, we can put whichever word(s) we want to change in the title within a <span> tag and then specify the style we want to apply within the tag. We then tell theme() that the plot title should be considered markdown rather than text (by setting plot.title = element_markdown()), like so:\n\np_color <- movies %>%\n  ggplot(aes(x = as_factor(decade), y = imdb_rating, color = binary, group = as_factor(binary))) +\n  geom_quasirandom(dodge.width = .7, width = .15, alpha = .6) +\n  scale_color_hp_d(option = \"LunaLovegood\", name = \"Bechdel Status\") +\n  theme_minimal() +\n  labs(\n    y = \"IMDB Rating\",\n    x = \"Decade\",\n    title = \"Movies that <span style='color:#830042'>Pass</span> and\n    <span style='color:#084d49'>Fail</span> the Bechdel Test Have Similar IMDB Rating<br>Distributions\",\n    caption = \"Data: FiveThirtyEight & IMDB | Viz: Eric Ekholm (@ekholm_e)\"\n  ) +\n    theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_markdown(), #telling ggplot to interpret the title as markdown\n    legend.position = \"none\", #remove the legend\n    plot.title.position = \"plot\",\n    plot.caption.position = \"plot\"\n  )\n\np_color\n\n\n\n\nThis plot contains the same information, but the actual plot itself has a lot more breathing room now that we’ve gotten rid of the legend. It also requires our eyes to move to fewer places since the title carries now carries the information that was in the legend.\nAgain, it’s the styling specified in the <span> tag within the title text (above) coupled with setting plot.title = element_markdown() within the theme() that allows this. You could also specify other styling (bold, italics, sizes, etc) as necessary, but I won’t focus on that here.\n\n\nSome Caveats\nAlthough I think this is a great trick to improve a lot of different plots, it’s not something I use for everything, and there are definitely cases where it’s not appropriate. Most notably, I wouldn’t use this approach if I had more than 3 or 4 groups/classes represented by colors. This isn’t necessarily a hard and fast rule, but I’ve found that if I have a lot of classes, I have a hard time writing a sensible title that incorporates the name of each class (so that I can color-code the text). If you end up making a list and coloring the text of that list, you might as well just use a legend. I’m also not sure I’d use this approach if I had two colors that were fairly similar. It’s easier to distinguish colors that are adjacent to one another, and so if your words containing color aren’t adjacent in your title, it might be hard to tell them apart if they’re similar colors.\n\n\nSome Extra Styling\nLike I mentioned at the outset, I have a custom ggplot theme in my personal R package ({eemisc}) that tweaks the above plot in a few ways to make it look nicer (in my opinion). Additionally, it assumes titles and subtitles are element_markdown() already, so it saves that line of code. Using my theme, we can revise the previous plot to the following:\n\np_final <- movies %>%\n  ggplot(aes(x = as_factor(decade), y = imdb_rating, color = binary, group = as_factor(binary))) +\n  geom_quasirandom(dodge.width = .7, width = .15, alpha = .6) +\n  scale_color_hp_d(option = \"LunaLovegood\") +\n  labs(\n    y = \"IMDB Rating\",\n    x = \"Decade\",\n    title = \"Movies that <span style='color:#830042'>Pass</span> and\n    <span style='color:#084d49'>Fail</span> the Bechdel Test Have<br>Similar IMDB Rating Distributions\",\n    caption = \"Data: FiveThirtyEight & IMDB | Viz: Eric Ekholm (@ekholm_e)\"\n  ) +\n  theme_ee(size = 10) +\n  theme(\n    legend.position = \"none\",\n    panel.grid.minor = element_blank()\n  )\n\np_final\n\n\n\n\nHope this is helpful, and happy plotting, all!\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-nc/4.0/CitationBibTeX citation:@online{ekholm2021,\n  author = {Eric Ekholm},\n  title = {Improving Ggplots with {Text} {Color}},\n  date = {2021-03-24},\n  url = {https://www.ericekholm.com/posts/improving-ggplots-with-text-color},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEric Ekholm. 2021. “Improving Ggplots with Text Color.”\nMarch 24, 2021. https://www.ericekholm.com/posts/improving-ggplots-with-text-color."
  },
  {
    "objectID": "posts/its-a-me-linear-regression/index.html",
    "href": "posts/its-a-me-linear-regression/index.html",
    "title": "It’s-a Me, Linear Regression",
    "section": "",
    "text": "Anyway, when I initially looked at the dataset, I thought I’d approach it by trying to fit a model to predict whether or not a driver took advantage of a shortcut or not when they set a record, but alas, I waited until later in the week and got scooped by Julia Silge (check out her analysis here). Which is probably for the best, because she did a better job than I would have.\nThat said, when I dug into the data, I did stumble across some interesting patterns in the progression of records over time, so I want to show how we can model these progressions using some simple feature engineering and a relatively straightforward mixed-effects model.\n\nSetup\nFirst, we’ll load our packages, set some global options, and get our data.\n\nlibrary(tidyverse)\nlibrary(eemisc)\nlibrary(harrypotter) #colors\nlibrary(nlme) #for mixed-effects models\nlibrary(broom.mixed) #functions for tidying mixed effects models\n\nherm <- harrypotter::hp(n = 1, option = \"HermioneGranger\")\nherm2 <- harrypotter::hp(n = 2, option = \"HermioneGranger\")[2]\n\nopts <- options(\n  ggplot2.discrete.fill = list(\n    harrypotter::hp(n = 3, option = \"HermioneGranger\"),\n    harrypotter::hp(n = 7, option = \"HermioneGranger\")\n  )\n)\n\ntheme_set(theme_ee())\n\nrecords <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-05-25/records.csv')\n\n\n\nExplore Data\nSo, what I’m interested in here is how the world record for each track progresses over time. To make sure all of our comparisons are “apples to apples,” I’m going to limit this to single-lap, no-shortcut records.\nLet’s randomly choose 4 tracks and look at these records over time.\n\nset.seed(0408)\nsamp_tracks <- sample(unique(records$track), size = 4)\n\n\nrecords %>%\n  filter(track %in% samp_tracks,\n         type == \"Single Lap\",\n         shortcut == \"No\") %>%\n  ggplot(aes(x = date, y = time, color = track)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(vars(track))\n\n\n\n\nIt’s a little bit hard to tell what’s going on here, especially since the tracks are different lengths and seem to have different record asymptotes. Another issue is that record-setting seems to be clustered by dates. A lot of records are set in a cluster, then there’s a drought for several years where the record isn’t broken. In some analyses this may be meaningful, but I care less about the actual date a record was set on and more about where it is in the sequence of records for that track. So, it might be more straightforward to just assign a running count of records for each track:\n\nrecords %>%\n  filter(track %in% samp_tracks,\n         type == \"Single Lap\",\n         shortcut == \"No\") %>%\n  group_by(track) %>%\n  mutate(record_num = row_number()) %>%\n  ungroup() %>%\n  ggplot(aes(x = record_num, y = time, color = track)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(vars(track))\n\n\n\n\nThis puts all of our X-axes on the same scale, and it also removes a lot of the white space where we had no records being broken (again, this information might be useful for a different analysis).\nWe might also want to consider how we’re representing the lap time here. Each track is a different length, and each track has its own unique obstacles. We can see here that Wario Stadium is a much longer track than, say, Sherbet Land. By extension, a 1 second decrease in time on Sherbet Land means a lot more than a 1 second decrease in time on Wario Stadium.\nStandardizing our measure of time – and our measure of improvement over time – will help us out here. What I’m going to do is, for each record (and specific to each track), calculate how much better (as a percent) it was than the first world record on that track. This will give us a standard way to compare the progress of each world record across all of the tracks.\nLet’s graph this to see what they look like.\n\nrecords_scaled <- records %>%\n  filter(type == \"Single Lap\",\n         shortcut == \"No\") %>%\n  group_by(track) %>%\n  mutate(init_wr = max(time),\n         pct_better = 1 - time/init_wr,\n         record_num = row_number()) %>%\n  ungroup()\n\nrecords_scaled %>%\n  ggplot(aes(x = record_num, y = pct_better)) +\n  geom_point(color = herm) +\n  geom_line(color = herm) +\n  labs(\n    x = \"Record Number\",\n    y = \"Pct Improvement over Initial\"\n  ) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  facet_wrap(vars(track)) +\n  theme_minimal()\n\n\n\n\nThese are a lot easier to compare, and we can see a pretty definitive pattern across all of the tracks. There are some sharp improvements in the early record-setting runs, but then these improvements attenuate over time, and records are getting only a teeny bit faster each time they’re broken.\nAnd this is sort of what we’d expect, particularly given a closed system like Mario Kart 64. The game isn’t changing, and people will hit a threshold in terms of how much better they can get, so it makes sense that these records are flattening.\nAnother interesting feature of the above graphs is that they (strongly) resemble logarithmic curves. We can plot these below to illustrate the similarity:\n\nrecords_scaled %>%\n  ggplot(aes(x = record_num, y = pct_better)) +\n  geom_point(color = herm) +\n  geom_line(color = herm) +\n  facet_wrap(vars(track)) +\n    labs(\n    x = \"Record Number\",\n    y = \"Pct Improvement over Initial\"\n  ) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  stat_function(fun = function(x) .1*log(x, base = 10), geom = \"line\", color = herm2) +\n  theme_minimal()\n\n\n\n\nWe can see that the general shape of the logarithmic curve matches the general shape of the track records here. I multiplied the curves by an arbitrary constant just to plot them, so of course we don’t expect them to match perfectly. That said, this does give us a clue that, given these feature engineering choices, we can model the data using a logarithmic curve.\n\n\nBuilding A Model\nThere are a few paths we could take moving forward to model the data. Two in particular stand out to me:\n\nWe could fit a model separately for each track, where percent_better is regressed on the log of record_num.\nWe could fit a multilevel model where percent_better is regressed on the log of record_num and specify a random intercept and a random slope by track. This is the option I’m going to take.\n\nTo give a very quick and insufficient nod to multilevel models (MLMs), they are useful for modeling clustered data (data where there are known dependencies between observations). The prototypical example of this is looking at people over time. Imagine you have a dataset of 1,000 observations where each of the 50 people in the dataset contributes 20 observations. When modeling this, you’d want to account for the fact that within-person observations are not independent. The one I encounter a lot in education is students clustered within classrooms. Different application, but same principle. For more on MLMs, Raudenbush & Bryk (2002) is a great resource, as is John Fox’s Applied Regression Analysis, which as a chapter on MLMs. My friend and colleague Mike Broda also has made public some content from his multilevel modeling (and multivariate statistics) course in R as well.\nAnyway, moving along! We basically have clustered data here: records are clustered within (read: dependent upon) each track. What an MLM allows us to do is fit a single model to the entire dataset while also allowing some of the parameters to vary by track. More specifically, we can allow the intercept to vary (which we don’t actually need here, since we’ve standardized our intercepts, but it’s just as easy to allow it), and we can allow the slope to vary. Varying the slope will let us estimate a different progression of world record runs for each track, which we can see that we need from the plots above.\nI’m using the {nlme} package to fit this model. And the model I’m fitting can be read as follows:\n\nI want a “regular” fixed-effects model where pct_better is regressed on the log of record_num.\nI also want to allow the coefficient of the log of record_num to vary depending on which track the record was set on.\n\n\nmod <- lme(fixed = pct_better ~ log10(record_num),\n           random = ~ log10(record_num) | track,\n           data = records_scaled)\n\nAnd let’s take a look at the results (thanks to {broom.mixed}):\n\ntidy(mod)\n\n# A tibble: 6 × 8\n  effect   group    term            estimate std.error    df statistic   p.value\n  <chr>    <chr>    <chr>              <dbl>     <dbl> <dbl>     <dbl>     <dbl>\n1 fixed    <NA>     (Intercept)      0.0162    0.00505   608      3.20  1.44e- 3\n2 fixed    <NA>     log10(record_n…  0.0518    0.00396   608     13.1   1.46e-34\n3 ran_pars track    sd_(Intercept)   0.0199   NA          NA     NA    NA       \n4 ran_pars track    cor_log10(reco…  0.768    NA          NA     NA    NA       \n5 ran_pars track    sd_log10(recor…  0.0156   NA          NA     NA    NA       \n6 ran_pars Residual sd_Observation   0.00651  NA          NA     NA    NA       \n\n\nFor what it’s worth, we see that both of the fixed effects are statistically significant. For reasons that I don’t quite remember off the top of my head, there’s some debate about doing hypothesis tests on random effects, so these aren’t included here (I think other packages will run these tests if you really want them). The main thing I focus on here, though, is that there’s a seemingly non-negligible amount of variance in the coefficient for record_num (see the term sd_log10(recordnum)). The mean coefficient is .051, and the standard deviation of the coefficient values is .015, which seems meaningful to me.\n\n\nPlotting Our Results\nTo get a better sense of what this model is doing, as well as to graphically examine how well it does, we can use the augment() function from {broom.mixed}. Let’s plot our fitted values against our actual pct_better values.:\n\naug <- mod %>%\n  augment()\n\naug %>%\n  filter(pct_better != 0) %>%\n  ggplot(aes(x = pct_better, y = .fitted)) +\n  geom_point(color = herm) +\n  theme_minimal()\n\n\n\n\nWe’re expecting an a-b line here, so this is good.\nFinally, though, what if we plot our actual values and our fitted values against record_num to see how well our model predictions compare to the real values, and let’s look at this by track:\n\naug %>%\n  select(track, .fitted, pct_better, record_num) %>%\n  pivot_longer(cols = c(\".fitted\", \"pct_better\"),\n               names_to = \"type\",\n               values_to = \"val\") %>%\n  ggplot(aes(x = record_num, y = val, color = type)) +\n  geom_point(alpha = .4) +\n  facet_wrap(vars(track)) +\n  theme_minimal()\n\n\n\n\nThe fitted values pretty much entirely overlap with the actual values (with the exception of Yoshi Valley and maybe sort of DK Jungle Parkway), which means we have a pretty solid model, here!\nIf we wanted, it would be fairly straightforward to translate these predictions back into seconds, but I’m going to call it done right here. Hopefully this post illustrates that with a little bit of feature engineering, you can build a really good model without having to load {xgboost} or {keras}. And hopefully it encourages people to dig more into MLMs!\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-nc/4.0/CitationBibTeX citation:@online{ekholm2021,\n  author = {Eric Ekholm},\n  title = {It’s-a {Me,} {Linear} {Regression}},\n  date = {2021-05-30},\n  url = {https://www.ericekholm.com/posts/its-a-me-linear-regression},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEric Ekholm. 2021. “It’s-a Me, Linear Regression.” May 30,\n2021. https://www.ericekholm.com/posts/its-a-me-linear-regression."
  },
  {
    "objectID": "posts/mle-learning-julia/index.html",
    "href": "posts/mle-learning-julia/index.html",
    "title": "MLE Learning Out Loud",
    "section": "",
    "text": "Maury Povich as a metaphor for maximum likelihood estimation\nSo this obviously isn’t 100% mathematically rigorous, but based on my understanding of maximum likelihood estimation (MLE), I think it’s kind of like the Maury Povich show…\nBack when I was in high school, some of my friends and I used to eat lunch in our track coach’s classroom and watch the Maury Povich show. For those of you that haven’t every watched Maury, it’s an…interesting study of human behavior…and worth checking out. But basically it’s like Jerry Springer or any of these other daytime drama-fests, covering everything from infidelity to obesity to insane behavior and everything in between. But Maury’s specialty was paternity tests.\nAlthough the details of the paternity test episodes differed slightly, a common pattern was that a pregnant woman along with multiple men would come on the show, and each of the men would take a paternity test. Maury would ask the men and the women to describe how confident they were in the results of the test, and the men would usually offer up something like:\n“I am a thousand percent sure I am not the father.”\nWhich would then elicit the next man to say:\n“Well I am one million percent sure I’m not the father!”\nWhich would in turn elicit animated reactions from the audience, the mother, and the other potential father(s) on the stage.\nSo how’s this like maximum likelihood estimation?\nSo my understanding of the logic of maximum likelihood estimation (MLE) is that, given a set of data, we can estimate the likelihood of a distribution parameterized by a given set of parameters. Imagine we have a bunch of measures of adult heights, and we assume that height is normally distributed. We know that a normal distribution is defined by its mean and its standard deviation. And so using our set of data, we can estimate the likelihood of any combination of mean and standard deviation (i.e. any set of parameters) given this data. And the parameters with the maximum likelihood are the “best” given our set of data. We’ll walk through this with examples later.\nWhat matters here though is that the actual number describing the likelihood (or the log-likelihood, more likely) doesn’t really matter. It’s not arbitrary, but it’ll differ depending upon how many observations are in your dataset, the distribution you’re using, etc. The values of the (log)likelihood relative to one another are what matters. And in this respect I’m reminded of Maury’s paternity tests.\nIt doesn’t matter if a guest on the show says he’s 100% sure the baby isn’t his. If the guy next to him says he’s 110% sure the baby’s not his, then he’s more certain than the first guy. Likewise, if the first guy says he’s one million percent sure the baby isn’t his, he still “loses” if the guy next to him says he’s 2 million percent sure. The actual number doesn’t matter – what matters is the estimate relative to the other estimates.\n\n\nSome Examples\nI’m not 100% sure the Maury analogy actually holds up, but whatever, let’s work through some examples\nFirst we’ll load some necessary packages.\n\nusing Distributions\nusing CairoMakie\nusing Random\nusing Optim\nusing GLM\nusing DataFrames\n\n\n\nCase 1: Fitting a Normal Distribution\nThis is the simplest case. First, we’re going to generate some sample data, s, from a normal distribution with \\(\\mu = 0\\) and \\(\\sigma = 1\\)\n\nRandom.seed!(0408)\ns = rand(Normal(), 10000)\n\n10000-element Vector{Float64}:\n -1.4055556573814212\n  0.8813161144909877\n  0.4695240597638853\n  1.0596565592604608\n -1.1909245261358548\n -1.4819187811057175\n -0.40408041211016915\n -0.37805385034816524\n -1.5132047920081557\n  2.2528479354589197\n -1.6595728371412546\n  1.321172026499611\n -1.5741912720732054\n  ⋮\n -0.6706076665047674\n  1.313413766916552\n -0.5776340358208154\n  2.2968511578121857\n  0.6020915294889897\n  0.19216658269979192\n  0.8936776607551574\n -0.5898756308872724\n  0.2424739897566387\n  0.7926169568329148\n -0.46603730352631795\n -0.6572491362891565\n\n\nThen we’ll generate a bunch of normal distributions with various means and standard deviations\n\nμs = collect(-2.0:2.0)\nσs = [0.5:0.5:2;]\n\nds = []\n\nfor i = μs, j = σs\n    d = Normal(i, j)\n    push!(ds, d)\nend\n\nSo our task now is going to be to determine the likelihood of each distribution (defined with a given set a parameters) given our data, s, that we’ve drawn from a normal distribution with \\(\\mu = 0\\) and \\(\\sigma = 1\\)\nTo do this, we use the probability density function (pdf) of our normal distribution to determine the likelihood of the parameters for any given observation. Fortunately, Julia (and other languages) have tools that can help us do this without having to write out the entire equation by hand. That said, here’s the equation – even though I’m not going to call it directly, it’s probably useful to see it.\n\\[f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma}} \\exp[-\\frac{(x - \\mu)^2}{2\\sigma^2}]\\]\nLet’s take a look at the first observation and the first distribution we defined:\nThe first value in our sample is:\n\ns[1]\n\n-1.4055556573814212\n\n\nAnd the first distribution we’ll look at is\n\nds[1]\n\nNormal{Float64}(μ=-2.0, σ=0.5)\n\n\nAnd if we look at the pdf of this, we get:\n\npdf(ds[1], s[1])\n\n0.39356088133821826\n\n\nI’m not a statistician (hence these learning posts), but my understanding of this is that it generally represents the “fit” of the distribution (and its parameters) to the given sample/data point. These values will be bound between 0 and 1, since they’re likelihoods, with higher values indicating better fit/higher likelihood.\nThe next step is to convert this to a log scale, since logging allows us to sum things rather than multiply them (which we’re gonna do soon).\n\nlogpdf(ds[1], s[1])\n#same as log(pdf(ds[1], s[]1))\n\n-0.9325195055871961\n\n\nSo this gives us the log likelihood of a given data point. But now we need to do this for all of the data points in our sample to determine the “fit”/likelihood of the distribution to our whole sample.\n\nsum(logpdf.(ds[1], s))\n\n-103363.07786213113\n\n\nApparently Distributions.jl gives us a helper for this via loglikelihood, so the above is the same as:\n\nloglikelihood(ds[1], s)\n\n-103363.07786213112\n\n\nSo this gives us the (log)likelihood of a distribution (normal, in this case, defined by parameters \\(\\mu\\) and \\(\\sigma\\)) given our sample. That is, the relatively plausibility of the parameters given our data. The goal then is to pick the best distribution/parameters, which we can do by maximizing the likelihood. In Maury terms, we want to find guy who’s most sure that the baby isn’t his.\nOr, apparently, it’s more common to minimize the negative loglikelihood, which is the same thing (and called logloss, I guess).\nSo let’s do this for all of the distributions we specified earlier\n\nlls = []\n\nfor i in ds\n    res = -loglikelihood(i, s)\n    push!(lls, res)\nend\n\nlls = Float64.(lls)\n\n20-element Vector{Float64}:\n 103363.07786213112\n  34465.6764159677\n  24477.94356153769\n  22439.92990862643\n  42816.83247490566\n  19329.115069161333\n  17750.58296295708\n  18655.789571924823\n  22270.587087680186\n  14192.553722354956\n  15467.666808820915\n  17371.649235223234\n  41724.3417004547\n  19055.992375548587\n  17629.195099129196\n  18587.50889852165\n 101178.09631322921\n  33919.43102874222\n  24235.16783388192\n  22303.36856182006\n\n\nAnd then we can plot the loglikelihoods we get:\n\nind = collect(1.0:length(ds))\n\nlines(ind, lls)\n\n\n\n\nNotice that our negative log likelihood is minimized in the 10th distribution, so let’s take a look at what that is\n\nds[10]\n\nNormal{Float64}(μ=0.0, σ=1.0)\n\n\nThis makes sense! This was the distribution that we drew our samples from!\nIf we want to do this without looking at a plot, we can apparently do this:\n\n#get the index of the minimum value in lls\nmin_ll = findall(lls .== minimum(lls))\n\n#get the distribution at this index\nds[min_ll]\n\n1-element Vector{Any}:\n Normal{Float64}(μ=0.0, σ=1.0)\n\n\nSo this tells us that – of the distributions we tested! – the most likely distribution given our data is a normal distribution with mean of 0 and standard deviation of 1. This doesn’t necessarily mean that this \\(\\mu = 0\\) and \\(\\sigma = 1\\) are the optimal parameters. There could be better parameters that we didn’t test, and so in the future we’d want to probably use some sort of optimizing functions that can do all of the math for us.\n\n\nCase 2: Simple Linear Regression\nSo now let’s move on a bit and try a simple linear regression. First we’ll just generate some fake data and a “ground truth” function\n\n#generate some x values\nx = collect(0:.1:10)\n\n#generate error\nϵ = rand(Normal(0, 1), length(x))\n\n#define a function relating x to y\nf(x) = 0.5 + 2*x\n\n#generate y as f(x) plus error\ny = f.(x) .+ ϵ\n\n101-element Vector{Float64}:\n  2.4461255238293758\n  1.2496723713219855\n  1.5014319678963934\n  1.8228982131749496\n -0.4320716243406362\n  0.9628100854937409\n  2.475384749019799\n  2.047025196204242\n  1.7487030877341891\n  1.4865883008076408\n  0.405749179591091\n  2.5585877608457355\n  2.956751811280712\n  ⋮\n 19.232878652117428\n 17.53450559237765\n 19.290332010598092\n 18.144129060042054\n 18.413568163778812\n 17.89449730367819\n 19.175282300038607\n 20.826640516579364\n 19.21519350783753\n 19.070233221768582\n 21.072296712369102\n 19.469128284822276\n\n\nAnd then we can plot the x and y values we just created:\n\nCairoMakie.scatter(x, y)\n\n\n\n\nAnother way to think about the above is that we expect a linear relationship between x and y in the form of\n\\(y = \\alpha + \\beta x + \\epsilon\\)\nWe need to estimate alpha and beta in a way that optimally fits this line, and we can do this with maximum likelihood. We can take advantage of the fact that linear regression assumes that residuals are normally distributed with an expected value (mean) of 0, since this will provide as with a distribution we can try to parameterize optimally. Since the residuals are dependent upon the predicted values of y, and since the predicted values of y are dependent on the intercept (\\(\\alpha\\)) and the coefficient (\\(\\beta\\)), this will give us a way to estimate the terms in the regression line.\n\\(\\sigma\\) is not super important to us, but we still need to estimate it. We can estimate the loglikelihood of a given set of parameters using the function below.\n\nfunction max_ll_reg(x, y, params)\n\n    α = params[1]\n    β = params[2]\n    σ = params[3]\n\n    ŷ = α .+ x.*β\n\n    resids = y .- ŷ\n\n    d = Normal(0, σ)\n\n    ll = -loglikelihood(d, resids)\n    \n    return ll\n\nend\n\nmax_ll_reg (generic function with 1 method)\n\n\nAnd let’s see how this works by passing in some generic values – .5 as the intercept, 2 as the beta coefficient, and 1 as the error variance.\n\nyy = max_ll_reg(x, y, [.5, 2, 1])\n\n137.00423917573127\n\n\nThe next step then is to optimize this. We pass some starting values and our max_ll_reg function into an optimizer, tell it to find the optimal values for the parameters (\\(\\alpha\\), \\(\\beta\\), and \\(\\sigma\\)), and then the magical optimizing algorithm written by people much smarter than me will give us our coefficients.\n\nres = optimize(params -> max_ll_reg(x, y, params), [0.0, 1.0, 1.0])\n\n * Status: success\n\n * Candidate solution\n    Final objective value:     1.361561e+02\n\n * Found with\n    Algorithm:     Nelder-Mead\n\n * Convergence measures\n    √(Σ(yᵢ-ȳ)²)/n ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   0  (vs limit Inf)\n    Iterations:    130\n    f(x) calls:    234\n\n\nAnd then this will give us the maximum likelihood solution for our regression equation.\n\nOptim.minimizer(res)\n\n3-element Vector{Float64}:\n 0.6262632240571052\n 1.9908770881500015\n 0.9315933450872507\n\n\nWe can check this by fitting the model with the GLM package\n\ndata = DataFrame(X = x, Y = y)\n\nols_res = lm(@formula(Y ~ X), data)\n\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}}}}, Matrix{Float64}}\n\nY ~ 1 + X\n\nCoefficients:\n────────────────────────────────────────────────────────────────────────\n                Coef.  Std. Error      t  Pr(>|t|)  Lower 95%  Upper 95%\n────────────────────────────────────────────────────────────────────────\n(Intercept)  0.626272   0.185875    3.37    0.0011   0.257455   0.995089\nX            1.99088    0.0321144  61.99    <1e-80   1.92715    2.0546\n────────────────────────────────────────────────────────────────────────\n\n\net voila, we get the same \\(\\alpha\\) and \\(\\beta\\)! The coefficients aren’t exactly the same as the ones we specified when generating the data, but that’s because of the error we introduced.\nIt’s maybe also worth nothing that Julia lets us solve the equation via the \\ operator, which apparently provides a shorthand for solving systems of linear equations:\n\n#we have to include a column of 1s in the matrix to get the intercept\nxmat = hcat(ones(length(x)), x)\n\nxmat \\ y\n\n2-element Vector{Float64}:\n 0.6262717121103298\n 1.9908750493937837\n\n\n\n\nCase 3: Multiple Regression\nAnd I think we can extend the same logic above to multiple regression. The first step is to generate some data:\n\n#make a 100x3 matrix of random numbers\ntmp = randn(100, 3)\n\n#append a leading column of 1s (so we can get the intercept)\n𝐗 = hcat(ones(100), tmp)\n\n#provide 'ground truth' coefficients\n𝚩 = [.5, 1, 2, 3]\n\n#define a function to multiply X by B\nf₂(X) = X*𝚩\n\n#create some error\nϵ = rand(Normal(0, .5), size(𝐗)[1])\n\n#make outcome values that comprise our generating function plus error\n𝐘 = f₂(𝐗) + ϵ\n\n100-element Vector{Float64}:\n  1.9539840955199055\n  2.2807060973619135\n  2.406665555383834\n -0.014731760693462048\n  0.6002032048684441\n -3.531148702629271\n  3.194699866301238\n -1.17858666703318\n -0.31117832513371646\n  1.5595030004201824\n  7.314823243199307\n -2.1182414214687673\n -3.502694667516171\n  ⋮\n  9.205296659574488\n  4.233455153011074\n  5.620823053237128\n -2.4088759447640156\n  5.127431971734125\n  1.0043279157869205\n  3.6343775497324184\n  2.611885689812401\n  0.11077494956658729\n  3.06142672043232\n  1.961141975667116\n -0.013605916161866072\n\n\nThen we can define another function to return the maximum likelihood. This is the same as the simple regression function above, except it’s generalized to allow for more than 1 slope coefficient. Julia provides some neat functionality via the begin and end keywords that let us access the first and last elements of a vector, and we can even do things like end-1 to get the second-to-last element, which is pretty nifty.\n\nfunction max_ll_mreg(x, y, params)\n    𝚩 = params[begin:end-1]\n    σ = params[end]\n\n    ŷ = x*𝚩\n\n    resids = y .- ŷ\n\n    d = Normal(0, σ)\n\n    ll = -loglikelihood(d, resids)\n\n    return ll\nend\n\nmax_ll_mreg (generic function with 1 method)\n\n\nThen we can do the same thing as before – provide some starting parameters (coefficients), and tell our super-smart optimizer function to give us the parameters that maximize the likelihood.\n\nstart_params = [.4, .5, 1.5, 4.0, 1.0]\n\nmreg_res = optimize(params -> max_ll_mreg(𝐗, 𝐘, params), start_params)\n\n * Status: success\n\n * Candidate solution\n    Final objective value:     6.860916e+01\n\n * Found with\n    Algorithm:     Nelder-Mead\n\n * Convergence measures\n    √(Σ(yᵢ-ȳ)²)/n ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   0  (vs limit Inf)\n    Iterations:    221\n    f(x) calls:    392\n\n\nAnd then we can show the results:\n\nOptim.minimizer(mreg_res)\n\n5-element Vector{Float64}:\n 0.42976727494283473\n 1.0367345683471323\n 1.8923643524058003\n 3.0304421915621127\n 0.4805357922701782\n\n\nAnd we can check that these are returning the correct values\n\n𝐗 \\ 𝐘\n\n4-element Vector{Float64}:\n 0.42976708591782187\n 1.0367343909692166\n 1.8923640529063954\n 3.0304412982361364\n\n\nAlternatively, we could have written out the joint pdf for the normal distribution by hand, like below.\nFirst we can define this function:\n\nfunction alt_mle_mlr(x, y, params)\n    𝚩 = params[begin:end-1]\n    σ = params[end]\n\n    ŷ = x*𝚩\n\n    n = length(ŷ)\n\n    ll = -n/2*log(2π) - n/2* log(σ^2) - (sum((y .- ŷ).^2) / (2σ^2))\n    \n    ll = -ll\n\n    return ll\nend\n\nalt_mle_mlr (generic function with 1 method)\n\n\nThen see what the loglikelihood is given our starting parameters:\n\nalt_mle_mlr(𝐗, 𝐘, start_params)\n\n174.11351826768353\n\n\nThen optimize the function:\n\nmreg_res2 = optimize(params -> alt_mle_mlr(𝐗, 𝐘, params), start_params)\n\n * Status: success\n\n * Candidate solution\n    Final objective value:     6.860916e+01\n\n * Found with\n    Algorithm:     Nelder-Mead\n\n * Convergence measures\n    √(Σ(yᵢ-ȳ)²)/n ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   0  (vs limit Inf)\n    Iterations:    221\n    f(x) calls:    392\n\n\nAnd check the results:\n\nOptim.minimizer(mreg_res2)\n\n5-element Vector{Float64}:\n 0.42976727494283473\n 1.0367345683471323\n 1.8923643524058003\n 3.0304421915621127\n 0.4805357922701782\n\n\nAnd there we go. Hopefully that was helpful for some others. I’ll probably do some more of these “learning out loud” posts as I dig into some more math, Julia, or other topics.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-nc/4.0/CitationBibTeX citation:@online{ekholm2022,\n  author = {Eric Ekholm and EE},\n  title = {MLE {Learning} {Out} {Loud}},\n  date = {2022-08-31},\n  url = {https://www.ericekholm.com/posts/mle-learning-julia},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEric Ekholm, and EE. 2022. “MLE Learning Out Loud.” August\n31, 2022. https://www.ericekholm.com/posts/mle-learning-julia."
  },
  {
    "objectID": "posts/personalizing-the-distill-template/index.html",
    "href": "posts/personalizing-the-distill-template/index.html",
    "title": "Personalizing the Distill Template",
    "section": "",
    "text": "One thing that’s been on my to-do list re: {distill}, though, has been to look into how to modify the Rmd template. What I mean by this is not changing the styling (although I’ve played around with that a little bit, too), but rather how to modify to the template to automatically include the packages I use in pretty much every blog post as well as to set knit options. Obviously, it’s not a huge deal to add this stuff in every time I make a new post – it’s only a few lines of code – but it is a little bit tedious.\nMaking an Rmd template is fairly straightforward, and the {rmarkdown} bookdown site has some great resources. But the process for creating an Rmd template while simultaneously taking advantage of the features of distill::create_post() felt less obvious to me. I had a little bit of time a couple of days ago to noodle about it, so I wanted to share what I came up with here.\nDisclaimer: This solution feels a bit hacky, so I’d love feedback from anyone reading on how to improve this. Regardless, let’s get into it.\n\nThe Default Distill Template\nRunning distill::create_post() while you have your website project open will create a new subdirectory in your _posts directory (assuming you’ve stuck with the defaults). The name of this subdirectory will contain the date and a slug related to the title of your post (again, assuming you’ve stuck with the defaults). It will also create an Rmd file in the newly-created subdirectory that looks something like this:\n\nAgain, this is great, but what if we want the template to come with, say, library(tidyverse) already included? Or what if we wanted to specify some knit options? That is, what if we want our template to look more like this:\n\n\n\nA Personalized Distill Template\nThe function below can do just that. I’ll post it in its entirety, then walk through what it does and how you can modify this if you want.\nHere’s the function, which I’m calling create_ee_post(), but you might want to use your own initials:\n\ncreate_ee_post <- function(..., open = TRUE) {\n\n  tmp <- distill::create_post(..., edit = FALSE)\n\n  yaml <- readLines(tmp, n = 12)\n\n  con <- file(tmp, open = \"w\")\n\n  on.exit(close(con), add = TRUE)\n\n  body <- \n  '\n  \n#```{r setup, echo = TRUE, results = \"hide\", warning = FALSE, message = FALSE}\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\n\nlibrary(tidyverse)\nlibrary(eemisc)\nlibrary(harrypotter)\n\nherm <- harrypotter::hp(n = 1, option = \"HermioneGranger\")\n\nopts <- options(\n  ggplot2.discrete.fill = list(\n    harrypotter::hp(n = 3, option = \"HermioneGranger\"),\n    harrypotter::hp(n = 7, option = \"Always\")\n  )\n)\n\ntheme_set(theme_ee())\n\n#```\n\nA message\n'\n\nxfun::write_utf8(yaml, con)\nxfun::write_utf8(body, con)\n\nif (open == TRUE) usethis::edit_file(tmp)\n\n}\n\nI’ve included the function in my personal R package if you want to take a peek at it there. One note re: the above is that you’ll want to remove the un-comment the ``` within the body. I’ve commented those out here because I’m embedding them in a code chunk.\nRight, so, big picture, the above function just wraps distill::create_post() so we still get all of the goodies of that function, then modifies the Rmd file produced by distill::create_post() after it’s already created. Let’s take a look at pieces more closely.\n\n\nStep by Step\n\ntmp <- distill::create_post(..., edit = FALSE)\n\nThis runs distill::create_post() and allows you to pass whatever arguments you typically would (e.g. setting the title) to that function. I’m setting edit = FALSE here because we don’t want to open the file that this function creates since we’re going to modify it after the fact (n.b. that we’ll add in a line later that will open the modified file, if we want). This will also store the path of the file that’s created to a variable called tmp, which is useful later.\n\nyaml <- readLines(tmp, n = 12)\n\nThis will read the first 12 lines of the Rmd file we just created and store them in the yaml variable. If you use the defaults of distill::create_post(), then your YAML header should be 12 lines, although I suppose your mileage could vary. It’s probably possible, too, to use some regex to read in lines between the dashes setting off the YAML header, but I didn’t play around with that.\n\ncon <- file(tmp, open = \"w\")\n\non.exit(close(con), add = TRUE)\n\nThese lines will open a connection to the tmp file we specified earlier and then, once the function exits, close this connection. I basically just copied this from the source code of distill::create_post().\n\nbody <- \n'\n  \n#```{r setup, echo = TRUE, results = \"hide\", warning = FALSE, message = FALSE}\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\n\nlibrary(tidyverse)\nlibrary(eemisc)\nlibrary(harrypotter)\n\nherm <- harrypotter::hp(n = 1, option = \"HermioneGranger\")\n\nopts <- options(\n  ggplot2.discrete.fill = list(\n    harrypotter::hp(n = 3, option = \"HermioneGranger\"),\n    harrypotter::hp(n = 7, option = \"Always\")\n  )\n)\n\ntheme_set(theme_ee())\n#```\n\nA message\n'\n\nHere’s the heart of this function. In this chunk, we’re specifying what we want the body (as opposed to the YAML header) of the Rmd file to look like, and everything gets passed in as a string. So, here, I’m doing a few things:\n\nSetting the options for the “setup” chunk,\nSetting the global chunk options for this file,\nLoading some libraries I commonly use (n.b. that {harrypotter} provides HP-themed color palettes and is my go-to option for colors),\nSetting ggplot fill/color options (including just grabbing the hex code for the dark red that’s the first color in the “Hermione Granger” palette)\nAnd finally setting the {ggplot2} theme to be the custom theme I’ve created in my {eemisc} package.\n\nI’ve also added a message (“A message”) to the file just for funsies.\nIf you’re interested in adapting this function, this is the code you’ll want to change to specify whatever options you prefer in your Rmd files/blog posts. You’ll also need to un-comment the ```s.\n\nxfun::write_utf8(yaml, con)\nxfun::write_utf8(body, con)\n\nThese lines will write the contents of the yaml variable (which we just pulled from the distill::create_post() default – we didn’t change anything here) and the body variable (which we just specified above) to con, which is the file we want to edit.\n\nif (open == TRUE) usethis::edit_file(tmp)\n\nFinally, this will open the file we just created in RStudio.\nSo that’s that. Again, this may not be the best way to do this, but it does seem to work. I’d love to hear if others have other (and better) ways of accomplishing this.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-nc/4.0/CitationBibTeX citation:@online{ekholm2021,\n  author = {Eric Ekholm},\n  title = {Personalizing the {Distill} {Template}},\n  date = {2021-04-05},\n  url = {https://www.ericekholm.com/posts/personalizing-the-distill-template},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEric Ekholm. 2021. “Personalizing the Distill Template.”\nApril 5, 2021. https://www.ericekholm.com/posts/personalizing-the-distill-template."
  },
  {
    "objectID": "posts/predicting-mobile-phone-subscription-growth/index.html",
    "href": "posts/predicting-mobile-phone-subscription-growth/index.html",
    "title": "Predicting Mobile Phone Subscription Gorwth",
    "section": "",
    "text": "I’ll keep the intro short and sweet for this one. A few weeks ago, I watched this screencast from Julia Silge in which she used splines to model the relationship between wins and seed in the Women’s NCAA Tournament. I don’t have a ton of experience using splines, and so that screencast made me want to learn a bit more and practice using them myself. Lo and behold, the phone subscription data from week 46 (2020) of #TidyTuesday seemed like a pretty good opportunity, so that’s what I’m doing here. More specifically, I’m using splines to model the relationship between year and the number of mobile phone subscriptions per 100 people across different continents and then investigating which countries these models perform best and worst for.\nLet’s get right into it, then."
  },
  {
    "objectID": "posts/predicting-mobile-phone-subscription-growth/index.html#setup",
    "href": "posts/predicting-mobile-phone-subscription-growth/index.html#setup",
    "title": "Predicting Mobile Phone Subscription Gorwth",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(eemisc)\nlibrary(harrypotter)\nlibrary(splines)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(kableExtra)\n\nherm <- harrypotter::hp(n = 1, option = \"HermioneGranger\")\n\nopts <- options(\n  ggplot2.discrete.fill = list(\n    harrypotter::hp(n = 3, option = \"HermioneGranger\"),\n    harrypotter::hp(n = 5, option = \"Always\")\n  )\n)\n\ntheme_set(theme_ee())\n\nmobile <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-11-10/mobile.csv')"
  },
  {
    "objectID": "posts/predicting-mobile-phone-subscription-growth/index.html#very-brief-exploration",
    "href": "posts/predicting-mobile-phone-subscription-growth/index.html#very-brief-exploration",
    "title": "Predicting Mobile Phone Subscription Gorwth",
    "section": "Very Brief Exploration",
    "text": "Very Brief Exploration\nSo, I kinda already know what I want to do, so I’m going to keep the exploration pretty minimal here. I will plot the number of mobile phone subscriptions for each country over time, though, just so I can get some feel for the relationships.\n\nmobile %>%\n  ggplot(aes(x = year, y = mobile_subs, group = entity)) +\n  geom_line(alpha = .7, color = herm) +\n  facet_wrap(~ continent)\n\n\n\n\nThe most apparent takeaway here to me is that the relationship between year and mobile phone subscriptions is not linear – it looks sigmoidal to me. It also seems to differ by continent (although that may just be an artifact of faceting the continents).\nEither way, I got into this to do some splines, so that’s what I’m going to do. First, let’s do a very brief and not statistically rigorous overview of what a spline is (n.b. for a better summary, go here). At a very high level, splines allow us to estimate flexible models to data. They do this by allowing us to include “knots” in our regression models and fitting smooth functions that model the data between consecutive knots. The person building the model can specify the number of knots (or degrees of freedom) they want to include in the model. Including more knots makes the function more flexible (but also, maybe obviously, increases model complexity), whereas including fewer knots makes the model simpler but less flexible.\nLet’s try plotting a few different splines to the full dataset here to illustrate this.\n\nplot_spline <- function(df) {\n  ggplot(mobile, aes(x = year, y = mobile_subs)) +\n    geom_smooth(\n      method = lm,\n      se = FALSE,\n      formula = y ~ ns(x, df = df),\n      color = herm\n    ) +\n    labs(\n      title = glue::glue(\"{ df } degrees of freedom\")\n    ) +\n    theme_minimal()\n}\n\nplots_list <- map(c(2, 3, 4, 6, 8, 10), plot_spline)\n\nwrap_plots(plots_list)\n\n\n\n\nNote that the number of knots is equal to 1 - the degrees of freedom. Anyway – looking at this, we can see noticeable differences between the models wtih 2, 3, and 4 degrees of freedom, but very little difference once we get beyond that, which makes me think that a 4 df spline is the way to go. I could more rigorously tune the degrees of freedom by fitting models with each and comparing the accuracy on holdout data, but visually examining it feels good enough here."
  },
  {
    "objectID": "posts/predicting-mobile-phone-subscription-growth/index.html#fitting-models",
    "href": "posts/predicting-mobile-phone-subscription-growth/index.html#fitting-models",
    "title": "Predicting Mobile Phone Subscription Gorwth",
    "section": "Fitting Models",
    "text": "Fitting Models\nNow, let’s fit a spline model for each continent. To do this, I’m going to first filter down to only the countries that have mobile phone data for every year in the dataset (1990 - 2017). Next, I’m going to create a nested tibble for each continent using the nest() function, fit a spline for each continent using a combination of map(), lm(), and ns() (which is a function for natural splines). Finally, I’m going to use glance() from the {broom} package to get the R-squared for each continent’s model. I like this workflow for fitting multiple models because it keeps everything together in a tibble and really facilitates iterating with map().\n\ncomplete_countries <- mobile %>%\n  group_by(entity) %>%\n  summarize(miss = sum(is.na(mobile_subs))) %>%\n  filter(miss == 0) %>%\n  pull(entity)\n\ncontinent_df <- mobile %>%\n  select(continent, entity, year, mobile_subs) %>%\n  filter(entity %in% complete_countries) %>%\n  group_by(continent) %>%\n  nest() %>%\n  mutate(model = map(data, ~lm(mobile_subs ~ ns(year, df = 4), data = .x)),\n         rsq = map_dbl(model, ~glance(.x) %>% pull(1)))\n\ncontinent_df\n\n# A tibble: 5 × 4\n# Groups:   continent [5]\n  continent data                 model    rsq\n  <chr>     <list>               <list> <dbl>\n1 Asia      <tibble [1,202 × 3]> <lm>   0.689\n2 Europe    <tibble [1,119 × 3]> <lm>   0.869\n3 Africa    <tibble [1,146 × 3]> <lm>   0.684\n4 Americas  <tibble [846 × 3]>   <lm>   0.822\n5 Oceania   <tibble [221 × 3]>   <lm>   0.628\n\n\nSo, the R-squared values here seem pretty good considering we’re only using year as a predictor. In Europe we’re getting .86, which seems very high, and suggests that most countries follow similar trajectories (which we can see in the very first plot above). What could be interesting, though, is to see which country in each continent this model performs best on and which it performs worst on. This will give us a sense of what the most “typical” country is (the country that most closely follows the overall continent model) and what the most atypical country is (the country that least closely follows the overall continent model) in each continent."
  },
  {
    "objectID": "posts/predicting-mobile-phone-subscription-growth/index.html#examining-predictions-accuracy",
    "href": "posts/predicting-mobile-phone-subscription-growth/index.html#examining-predictions-accuracy",
    "title": "Predicting Mobile Phone Subscription Gorwth",
    "section": "Examining Predictions & Accuracy",
    "text": "Examining Predictions & Accuracy\nTo do this, I’m first going to predict values for each observation (each year for each country) using augment(), again from {broom}. I’m then going to do a little bit of binding and straightening up, ending by unnesting the data.\n\npreds_data <- continent_df %>%\n  ungroup() %>%\n  mutate(preds = map(model, ~augment(.x) %>% select(.fitted)),\n         joined_data = map2(data, preds, bind_cols)) %>%\n    select(joined_data, \n         continent_rsq = rsq,\n         continent) %>%\n    unnest(joined_data)\n\nNext, I’m going to calculate the R-squared for each country. There’s probably another way to do this, but it’s a pretty easy calculation to do by hand, so I’m just going to do that.\n\nrsq_df <- preds_data %>%\n  group_by(entity) %>%\n  mutate(avg = mean(mobile_subs),\n         res = (mobile_subs - .fitted)^2,\n         tot = (mobile_subs - .fitted)^2 + (.fitted - avg)^2) %>%\n  summarize(country_rsq = 1 - (sum(res)/sum(tot))) %>%\n  ungroup() %>%\n  left_join(x = preds_data %>% distinct(entity, continent), y = ., by = \"entity\")\n\nNow, I’m going to pick filter down to the countries in each continent that have the highest R-squared (the country the model performs best on) and the lowest R-squared (the country the model performs worst on). One thing to note is that a low R-squared doesn’t mean the country has few mobile phone subscriptions, it just means that the model does a relatively bad job predicting the mobile phone subscriptions for that country. This could be for a number of reasons, only one of which is that the country has considerably fewer subscriptions each year.\n\nselected_rsq <- rsq_df %>%\n  group_by(continent) %>%\n  filter(country_rsq == max(country_rsq) | country_rsq == min(country_rsq)) %>%\n  mutate(type = if_else(country_rsq == max(country_rsq), \"best fit\", \"worst fit\")) %>%\n  select(continent, entity, country_rsq, type) %>%\n  arrange(continent, country_rsq)\n\nselected_rsq %>%\n  select(-type) %>%\n  kable(format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"condensed\", \"striped\"))\n\n\n\n \n  \n    continent \n    entity \n    country_rsq \n  \n \n\n  \n    Africa \n    Seychelles \n    0.5109688 \n  \n  \n    Africa \n    Kenya \n    0.9895082 \n  \n  \n    Americas \n    Curacao \n    0.4431118 \n  \n  \n    Americas \n    Paraguay \n    0.9864706 \n  \n  \n    Asia \n    Hong Kong \n    0.5080236 \n  \n  \n    Asia \n    Philippines \n    0.9916221 \n  \n  \n    Europe \n    Moldova \n    0.6943629 \n  \n  \n    Europe \n    Faeroe Islands \n    0.9868045 \n  \n  \n    Oceania \n    Kiribati \n    0.5775901 \n  \n  \n    Oceania \n    French Polynesia \n    0.9451227 \n  \n\n\n\n\n\nRight, so, for example, we can see that the model fit using Europe’s data does the worst job predicting for Moldova and the best job for the Faeroe Islands.\nFinally, let’s take a look at these best- and worst-fitting countries graphically\n\nuse_countries <- pull(selected_rsq, entity)\n\nmobile_small_joined <- mobile %>%\n  filter(entity %in% use_countries) %>%\n  left_join(selected_rsq, by = c(\"entity\", \"continent\"))\n\nlabel_df <- mobile_small_joined %>%\n  group_by(entity) %>%\n  filter(mobile_subs == max(mobile_subs)) %>%\n  ungroup()\n  \nmobile %>%\n  filter(entity %in% use_countries) %>%\n  left_join(selected_rsq, by = c(\"entity\", \"continent\")) %>%\n  ggplot(aes(x = year, y = mobile_subs, group = entity, color = type)) +\n  geom_line() +\n  geom_text(data = label_df, aes(label = entity), x = max(mobile$year), hjust = 1, fontface = \"bold\", show.legend = FALSE) +\n  facet_wrap(~ continent) +\n  scale_color_hp_d(option = \"HermioneGranger\",\n                   name = \"Model Fit\") +\n  labs(\n    title = \"Best and Worst Fitting Models\"\n  )\n\n\n\n\nLet’s take a look at Hong Kong. It has the worst fit for the Asian model, but that’s because Hong Kong has way more mobile phones per person than other countries in Asia. On the other hand, we see that Kiribati (in Oceania) has way fewer than the model would predict. Both have (relatively) poor accuracy.\nOkie dokie, I think that’s it for now. Hopefully this is helpful for others in dipping your toes into spline models but also for demonstrating a workflow for nesting data and using map() along with some other functions to fit and interrogate multiple models."
  },
  {
    "objectID": "posts/publishing-rmarkdown-to-google-sites/index.html",
    "href": "posts/publishing-rmarkdown-to-google-sites/index.html",
    "title": "Publishing Rmarkdown to Google Sites",
    "section": "",
    "text": "This post – and the digging behind it – was inspired by Allison Horst and Jacqueline Nolis’s recent-ish blog post about conflicts between data scientists and the teams they’ may be’re joining/supporting. It’s a great post, and I’d recommend it to anyone, whether you’re looking for a job, new to a job, or relatively senior in your organization. When I read it, the post struck a chord with me and led me to do some introspection about how much change I should be initiating/pushing for as a new employee in my organization vs how much I should be adapting to established workflows.\nBasically, this was my response:"
  },
  {
    "objectID": "posts/publishing-rmarkdown-to-google-sites/index.html#step-1-create-your-report",
    "href": "posts/publishing-rmarkdown-to-google-sites/index.html#step-1-create-your-report",
    "title": "Publishing Rmarkdown to Google Sites",
    "section": "Step 1: Create Your Report",
    "text": "Step 1: Create Your Report\nMaybe obviously, the first thing you want to do is write and knit your report as an html file. Again, this can be any content you want, but I’m using the {palmerpenguins} README here. Another note is that, since we’re using html, we can include features like {reactable} tables or {leaflet} maps if we want, although those aren’t included here. We could also knit this as a {distill} article or add whatever styling/css we want."
  },
  {
    "objectID": "posts/publishing-rmarkdown-to-google-sites/index.html#step-2-create-a-google-site",
    "href": "posts/publishing-rmarkdown-to-google-sites/index.html#step-2-create-a-google-site",
    "title": "Publishing Rmarkdown to Google Sites",
    "section": "Step 2: Create A Google Site",
    "text": "Step 2: Create A Google Site\nOnce you have your report created, you can create the Google Site in Google Drive, like so:\n\nOr you can navigate to a site you already own/can edit.\nn.b. that I’m not really going to get into the weeds of working in Google Sites because that’s not really the point of this post, plus I’m not an expert myself, although basic usage is fairly straightforward."
  },
  {
    "objectID": "posts/publishing-rmarkdown-to-google-sites/index.html#step-3-create-a-page-in-google-sites-to-house-your-report",
    "href": "posts/publishing-rmarkdown-to-google-sites/index.html#step-3-create-a-page-in-google-sites-to-house-your-report",
    "title": "Publishing Rmarkdown to Google Sites",
    "section": "Step 3: Create a Page in Google Sites to House Your Report",
    "text": "Step 3: Create a Page in Google Sites to House Your Report\nThe details of what this means will depend on the layout of your site, but essentially you want to create a page that can house the report you just knit. You might end up with something that looks like this – a basic banner and then a blank body:"
  },
  {
    "objectID": "posts/publishing-rmarkdown-to-google-sites/index.html#step-4-view-the-source-code-of-your-html-report",
    "href": "posts/publishing-rmarkdown-to-google-sites/index.html#step-4-view-the-source-code-of-your-html-report",
    "title": "Publishing Rmarkdown to Google Sites",
    "section": "Step 4: View the Source Code of Your HTML Report",
    "text": "Step 4: View the Source Code of Your HTML Report\nNext, you want to get to the source code of the html file you created. You can do this by either opening the file with a browser and then inspecting the page source (the exact process for doing this will depend on which browser you use), or you can open the html file in RStudio (or another text editor).\nOnce you’re there, select and copy all of the source code"
  },
  {
    "objectID": "posts/publishing-rmarkdown-to-google-sites/index.html#step-5-embed-the-source-code-in-your-google-sites-page",
    "href": "posts/publishing-rmarkdown-to-google-sites/index.html#step-5-embed-the-source-code-in-your-google-sites-page",
    "title": "Publishing Rmarkdown to Google Sites",
    "section": "Step 5: Embed the Source Code in Your Google Sites Page",
    "text": "Step 5: Embed the Source Code in Your Google Sites Page\nReturning to the page where we want this report to live on our Google Site, we want to select the “Embed” option from the “Insert” menu on the right-hand side. You can also double left-click a blank part of your web page to have options pop up (& you can select “Embed” from there).\nOnce we click the “Embed” button, a dialogue box pops up:\n\nAnd we want to select the “Embed code” option. Once we’re here, we want to paste in all of the html source code from our report file. Then, click “Next,” and “Insert”\nDon’t stress if you see a box that says “trouble loading embed. Reload to try again.” (see below) The content should load once you publish your site.\n\nYou also probably want to resize & reposition the embedded content at this point. As far as I can tell, this is only possible via dragging, and there aren’t parameters you can set anywhere in the site to ensure consistent alignment (although there are grid lines to guide you)."
  },
  {
    "objectID": "posts/publishing-rmarkdown-to-google-sites/index.html#step-6-publish-your-site",
    "href": "posts/publishing-rmarkdown-to-google-sites/index.html#step-6-publish-your-site",
    "title": "Publishing Rmarkdown to Google Sites",
    "section": "Step 6: Publish Your Site",
    "text": "Step 6: Publish Your Site\nNow we’re ready to publish our site. Click the “Publish” button in the upper-right of the screen and select the options you want in the popup box. If you’re publishing from a Google account that belongs to an organization, your options may be pre-specified. For instance, when I publish from my work account, the default option is to allow only internal users to see the site (which is what I want in most cases).\nOnce you’ve published your site, you can navigate to it and see the following:\n\nIf you want to navigate to the example site I just created, you can find it here."
  },
  {
    "objectID": "posts/riddler-express-march-20-2020/index.html",
    "href": "posts/riddler-express-march-20-2020/index.html",
    "title": "Riddler Express - March 20, 2020",
    "section": "",
    "text": "One of my personal goals for 2020 is to improve my proficiency doing data-y things – mostly using R, but potentially other software as well. Typically, I’ve been using data from the #TidyTuesday project to practice data visualization and data from Kaggle, personal research projects, and other potentially interesting datasets to work on statistical modeling. I recently discovered The Riddler series – a weekly math/logic puzzle – that seems to be a good medium for brushing up on other skills (e.g. certain types of math and programming) that may not come up as often when I do visualizations or statistics."
  },
  {
    "objectID": "posts/riddler-express-march-20-2020/index.html#the-problem",
    "href": "posts/riddler-express-march-20-2020/index.html#the-problem",
    "title": "Riddler Express - March 20, 2020",
    "section": "The Problem",
    "text": "The Problem\nAnyway, this post solves the Riddler Express puzzle from March 20, 2020. The problem is this:\n\nA manager is trying to produce sales of his company’s widget, so he instructs his team to hold a sale every morning, lowering the price of the widget by 10 percent. However, he gives very specific instructions as to what should happen in the afternoon: Increase the price by 10 percent from the sale price, with the (incorrect) idea that it would return it to the original price. The team follows his instructions quite literally, lowering and then raising the price by 10 percent every day.\n\n\nAfter N days, the manager walks through the store in the evening, horrified to see that the widgets are marked more than 50 percent off of their original price. What is the smallest possible value of N?\n\nI’ll walk through a couple of ways to solve this – first, I’ll solve it algebraically, and next, I’ll solve it by “brute force” using the accumulate() function from the {purrr} package."
  },
  {
    "objectID": "posts/riddler-express-march-20-2020/index.html#solving-algebraically",
    "href": "posts/riddler-express-march-20-2020/index.html#solving-algebraically",
    "title": "Riddler Express - March 20, 2020",
    "section": "Solving Algebraically",
    "text": "Solving Algebraically\nSo, the first thing that strikes me when reading this is that it’s essentially a compounding interest problem, except in this case the interest is negative. That is, rather than gaining value exponentially over the number of compounding periods, we’re losing value exponentially. The formula for calculating compound interest is:\n\\[A = P(1 + r)^n\\]\nwhere A equals the final amount, P equals the principal (our initial value), r equals the interest rate, and n equals the number of compounding periods (the number of days in this case). We’re interested in solving for the value of n where our final amount, A, is less than .5. Our principal amount, P, in this case, is 1 (i.e. 100% of the value). So, our equation looks like this:\n\\[.5 > ((1-1*.1)*1.1)^n\\]\nThe internal logic here is that we subtract 10% from our initial value (1-1*.1) to represent the 10% decrease in price in the morning, then multiply this resulting value by 1.1 to represent the subsequent 10& increase in price in the afternoon. This simplifies to:\n\\[.5 > .99^n\\]\nFrom here, we can just solve by taking the log of each side and then dividing, which get us our answer\n\nn <- log(.5)/log(.99)\nn\n\n[1] 68.96756\n\n\nRounding this up (since we’re dealing in full days), we can say that after 69 days, the price of the widget will be below 50% of its initial price."
  },
  {
    "objectID": "posts/riddler-express-march-20-2020/index.html#solving-using-accumulate",
    "href": "posts/riddler-express-march-20-2020/index.html#solving-using-accumulate",
    "title": "Riddler Express - March 20, 2020",
    "section": "Solving using accumulate()",
    "text": "Solving using accumulate()\nWe can also solve this problem using the accumulate() function from the {purrr} package, which is part of the {tidyverse}. Essentially, accumulate() will take a function, evaluate it, and then pass the result of the evaluation back into the function, evaluate it again, pass the new result back into the function, etc. This makes it useful for solving problems like this one, where the end price of the widget on the previous day is the starting price of the widget on the current day.\nFirst, let’s load our packages. For this, we’ll just use {tidyverse}\n\nlibrary(tidyverse)\n\nNext, let’s set up a function that, if we give it the price of the widget at the beginning of the day, will calculate the price of the widget at the end of the day.\n\ndiscount_func <- function(x) {\n  (x-x*.1)*1.1\n}\n\nAnd then let’s test this function manually a few times.\n\ndiscount_func(1)\n\n[1] 0.99\n\ndiscount_func(.99)\n\n[1] 0.9801\n\ndiscount_func(.9801)\n\n[1] 0.970299\n\n\nNow, we can use accumulate() to automate what we just did manually. The first argument in accumulate() is, in this case, each day that we want to pass into the function. In the code below, I’m testing this for days 0-3 (but coded as 1-4 because we want the start value to be 1). The second argument is the function we just wrote.\n\naccumulate(1:4, ~discount_func(.))\n\n[1] 1.000000 0.990000 0.980100 0.970299\n\n\nAnd we can see that the values returned match our manual tests above, which is good!\nNow, we can use accumulate() to make a table with the end price of the widget each day. Note that because we want to start the widget price at 1, our first “day” in the table is day 0, which represents the beginning price of the widget on day 1.\n\ndays_tbl <- tibble(\n  day = c(0:1000),\n  end_price = accumulate(c(1:1001), ~discount_func(.))\n)\nhead(days_tbl)\n\n# A tibble: 6 × 2\n    day end_price\n  <int>     <dbl>\n1     0     1    \n2     1     0.99 \n3     2     0.980\n4     3     0.970\n5     4     0.961\n6     5     0.951\n\n\nAnd then we can plot the end price over time. I’ve added a little bit of transparency to each point so we can more easily see the clustering/overlap.\n\nggplot(days_tbl, aes(x = day, y = end_price)) +\n  geom_point(alpha = .3) +\n  theme_minimal() +\n  labs(\n    title = \"End Price of Widget over Time\"\n  )\n\n\n\n\nFinally, we can find the day where the end price is below .5 by filtering our table to only those where the price is less than .5 and then returning the first row.\n\ndays_tbl %>%\n  filter(end_price <= .5) %>%\n  slice(1)\n\n# A tibble: 1 × 2\n    day end_price\n  <int>     <dbl>\n1    69     0.500\n\n\nAnd we can see that this matches our algebraic result – great success!"
  },
  {
    "objectID": "posts/robustly-create-parameterized-reports/index.html",
    "href": "posts/robustly-create-parameterized-reports/index.html",
    "title": "Robustly Create Parameterized Reports",
    "section": "",
    "text": "Recently, I was working on creating parameterized reports for all of the schools in the division where I work. The basic idea was to provide school leadership teams with individualized reports on several (common) key metrics that they could use to both 1) reflect on the previous year(s) and 2) set goals for the upcoming year(s).\nThe beauty of parameterized reporting via RMarkdown is that you can build a template report, define some parameters that will vary within each iteration of the report, and then render several reports all from a single template along with a file that will loop (or purrr::walk()) through the parameters. (If you want to learn more about parameterized reporting, the always-incredible Alison Hill has a recent-ish tutorial on them that you can find here). In my case, this meant creating one template for all 65+ schools and then looping through a function that rendered the report for each school. Sounds great, right?"
  },
  {
    "objectID": "posts/robustly-create-parameterized-reports/index.html#see-what-had-happened-was",
    "href": "posts/robustly-create-parameterized-reports/index.html#see-what-had-happened-was",
    "title": "Robustly Create Parameterized Reports",
    "section": "See, what had happened was…",
    "text": "See, what had happened was…\n\nThis workflow is great…when it works. Except it doesn’t always. This isn’t to say that {rmarkdown} mysteriously breaks or anything, but rather that when you create these reports using real (read: usually messy) data, and when you’re trying to present a lot of data in a report, the probability that one of your iterations throws an error increases. This is especially true when you work in a school division and the integrity of your data has been absolutely ravaged by COVID during the past ~18 months. When this happens, instead of watching the text zip by on your console as all of your reports render like they’re supposed to, you end up hunting through the data for each individual school wondering why calculating a particular metric threw an error. Which is like, not nearly as much fun."
  },
  {
    "objectID": "posts/robustly-create-parameterized-reports/index.html#so-what-can-we-do-about-this",
    "href": "posts/robustly-create-parameterized-reports/index.html#so-what-can-we-do-about-this",
    "title": "Robustly Create Parameterized Reports",
    "section": "So what can we do about this?",
    "text": "So what can we do about this?\nFortunately, we can get around this by making “safe” versions of our functions. What exactly that means will vary from function to function and from use case to use case, but generally it means wrapping a function in another function that can facilitate error handling (or prevent errors from occuring). In some cases, it might mean using purrr::safely() or purrr::possibly() to capture errors or provide default values to the functions. In other cases, it might mean writing your own wrapper (which is what I’ll demonstrate below) to deal with errors that pop up. Regardless of the exact route you go, the goal here is to prevent errors that would otherwise stop your document(s) from rendering.\nLet’s see this in action."
  },
  {
    "objectID": "posts/rstudio-table-contest-submission/index.html",
    "href": "posts/rstudio-table-contest-submission/index.html",
    "title": "RStudio Table Contest Submission",
    "section": "",
    "text": "is is going to be a pretty short post. After seeing the Rstudio Table Contest announced a few weeks ago, I decided that I wanted to submit something to give myself more of a reason to practice with the {gt} package. I (somewhat arbitrarily) chose to use some longitudinal Broadway data posted earlier in the year as part of #TidyTuesday that I thought would lend itself well to a table.\nAnyway, below is my submission to the contest, and below that are some initial impressions of the {gt} package (tl;dr – it’s pretty awesome)."
  },
  {
    "objectID": "posts/rstudio-table-contest-submission/index.html#table",
    "href": "posts/rstudio-table-contest-submission/index.html#table",
    "title": "RStudio Table Contest Submission",
    "section": "Table",
    "text": "Table\n\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(lubridate)\n\n# Cleaning and Setup ------------------------------------------------------\n\n#loading in broadway data from wk 18 of TidyTuesday 2020\ngrosses <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-04-28/grosses.csv', guess_max = 40000)\n\n#removing 2020 bc it's not yet complete & is an awkward year\ngrosses <- grosses %>%\n  filter(year(week_ending) != 2020)\n\n#getting top 20 selling shows\ntop_shows <- grosses %>%\n  count(show, wt = weekly_gross, sort = TRUE) %>%\n  slice_max(order_by = n, n = 10) %>%\n  pull(show)\n\n#getting data for line plot\nannual_sales <- grosses %>%\n  filter(show %in% top_shows) %>%\n  mutate(year = year(week_ending)) %>%\n  group_by(show, year) %>%\n  summarize(tix = sum(seats_sold, na.rm = TRUE)) %>%\n  ungroup()\n\n#setting values for plots/tables\nlabs_col <- '#d9b51c'\nlabs_back <- '#373737'\nbckgrnd <- '#474747'\nbck_light <- '#515151'\ntext_col <- '#fdfdfd'\nfont <- google_font(\"Rubik\")\nlob <- google_font(\"Lobster\")\nbckgrnd_vec <- rep(c(bckgrnd, bck_light), times = 5)\n\n#getting playbill img urls\nplaybills <- c(\"https://bsp-static.playbill.com/dims4/default/25f6db1/2147483647/resize/x250%3C/quality/90/?url=http%3A%2F%2Fpb-asset-replication.s3.amazonaws.com%2F06%2Ffb%2Ff6ba0a7c40ca918068a0fdc04bf7%2Fthe-lion-king-playbill-2017-11-web.jpg\",\n               \"https://bsp-static.playbill.com/dims4/default/823dbab/2147483647/resize/x250%3C/quality/90/?url=http%3A%2F%2Fpb-asset-replication.s3.amazonaws.com%2Feb%2Fc2%2F90b25fdb41f7a75d2f9532ff7da4%2Fwicked-playbill-2017-05-web.jpg\",\n               \"https://bsp-static.playbill.com/dims4/default/a4bd340/2147483647/resize/x250%3C/quality/90/?url=http%3A%2F%2Fpb-asset-replication.s3.amazonaws.com%2F50%2Fa4%2Fa71805ca472a8f12c97cac39f1e7%2Fthe-phantom-of-the-opera-playbill-2019-01-01-web.jpg\",\n               \"https://bsp-static.playbill.com/dims4/default/641d7e2/2147483647/resize/x250%3C/quality/90/?url=http%3A%2F%2Fpb-asset-replication.s3.amazonaws.com%2F1d%2Fa9%2F224d538645058e0877d400321174%2Fchicago-playbill-2020-01-01-web.jpg\",\n               \"https://bsp-static.playbill.com/dims4/default/38f17a2/2147483647/resize/x250%3C/quality/90/?url=http%3A%2F%2Fpb-asset-replication.s3.amazonaws.com%2F8e%2Fc9%2F60189ac7483b86d4f26f0d26d85e%2Fthe-book-of-mormon-playbill-2011-2-24-web.jpg\",\n               \"https://bsp-static.playbill.com/dims4/default/5101191/2147483647/resize/x250%3C/quality/90/?url=http%3A%2F%2Fpb-asset-replication.s3.amazonaws.com%2F23%2Fde%2F41ceb4094b0d9b7c9e53a51f2731%2Fmamma-mia-playbill-2001-10-05-web.jpg\",\n               \"https://bsp-static.playbill.com/dims4/default/9ce9b12/2147483647/resize/x250%3C/quality/90/?url=http%3A%2F%2Fpb-asset-replication.s3.amazonaws.com%2Fb2%2Fd5%2Fd007cd9d468f8b0ed107c00d119e%2Fhamilton-playbill-2018-01-web.jpg\",\n               \"https://bsp-static.playbill.com/dims4/default/a66da3d/2147483647/resize/x250%3C/quality/90/?url=http%3A%2F%2Fpb-asset-replication.s3.amazonaws.com%2F5f%2F0d%2F9372f5f347ee824597d030323716%2Fjersey-boys-playbill-2005-10-17-web.jpg\",\n               \"https://bsp-static.playbill.com/dims4/default/b0dcc9c/2147483647/resize/x250%3C/quality/90/?url=http%3A%2F%2Fpb-asset-replication.s3.amazonaws.com%2F8f%2Fd7%2F3d838c6441c192023d35429c4de5%2Fles-miserables-playbill-2014-03-01-web.jpg\",\n               \"https://bsp-static.playbill.com/dims4/default/4b36540/2147483647/resize/x250%3C/quality/90/?url=http%3A%2F%2Fpb-asset-replication.s3.amazonaws.com%2F41%2F3d%2Fc505cabe4e0fb51d74e83f26b453%2Faladdin-playbill-2018-04-01-web.jpg\")\n\n#creating plot function\nplot_line <- function(show, col = bckgrnd) {\n  annual_sales %>%\n    filter(show == {{ show }}) %>%\n    ggplot(aes(x = year, y = tix)) +\n    geom_line(size = 2, color = text_col) +\n    scale_x_continuous(limits = c(min(annual_sales$year) - 1, max(annual_sales$year) + 1)) +\n    theme_void() +\n    theme(\n      plot.background = element_rect(fill = col, color = col),\n      panel.background = element_rect(fill = col, color = col)\n    )\n}\n\n#set up tbl\ntbl <- grosses %>%\n  filter(show %in% top_shows) %>%\n  group_by(show) %>%\n  summarize(yr_of_premier = min(year(week_ending)),\n            total_gross = sum(weekly_gross),\n            avg_capacity = mean(pct_capacity),\n            tix_year = sum(seats_sold)/(max(year(week_ending)) - min(year(week_ending)) + 1)) %>%\n  arrange(desc(total_gross)) %>%\n  mutate(plots = map2(show, bckgrnd_vec, plot_line),\n         ranking = row_number(),\n         playbills = playbills)\n\n# Creating Table ----------------------------------------------------------\n\nstyled_tbl <- tbl %>%\n  select(ranking, show, playbills, everything()) %>%\n  gt() %>%\n  opt_table_font(\n    font = font\n  ) %>%\n  opt_row_striping() %>%\n  tab_header(\n    title = md(\"**Top Earning Broadway Shows**\"),\n    subtitle = md(\"*through 2019*\")\n  ) %>%\n  cols_label(\n    ranking = \"Rank\",\n    show = \"Show\",\n    playbills = \"\",\n    yr_of_premier = \"Premiere Year\",\n    total_gross = \"Total Earnings\",\n    avg_capacity = md(\"Avg Theater<br>Capacity Filled\"),\n    tix_year = \"Average\",\n    plots = \"1987 - 2019\"\n  ) %>%\n  tab_spanner(\n    label = \"Tickets Sold Per Year\",\n    columns = vars(tix_year, plots)\n  ) %>%\n  tab_style(\n    style = cell_text(font = lob, align = \"center\", size = px(50), color = labs_col),\n    locations = cells_title(\"title\")\n  ) %>%\n  tab_style(\n    style = cell_fill(color = labs_back),\n    location = cells_title(\"title\")\n  ) %>%\n  tab_style(\n    style = cell_text(font = lob, align = \"center\", size = px(12), color = labs_col),\n    locations = cells_title(\"subtitle\")\n  ) %>% \n  tab_style(\n    style = cell_fill(color = labs_back),\n    locations = cells_title(\"subtitle\")\n  ) %>%\n  tab_style(\n    style = cell_text(size = px(15), style = \"italic\"),\n    locations = list(cells_column_labels(vars(ranking, show, yr_of_premier, total_gross, avg_capacity)), cells_column_spanners(everything()))\n  ) %>%\n  tab_style(\n    style = cell_text(size = px(12), style = \"italic\", align = \"center\"),\n    locations = cells_column_labels(vars(tix_year, plots))\n  ) %>%\n  tab_style(\n    style = cell_text(align = \"center\"),\n    locations = cells_body(everything())\n  ) %>%\n  tab_style(\n    style = cell_borders(sides = c(\"top\", \"bottom\"), color = bckgrnd),\n    locations = cells_body(\n      columns = everything(),\n      rows = everything()\n    )\n  ) %>%\n  text_transform(locations = cells_body(vars(plots)),\n                 fn = function(x) {\n                   map(tbl$plots, ggplot_image)\n                 }) %>%\n  text_transform(\n    locations = cells_body(vars(playbills)),\n    fn = function(x) {\n      map(tbl$playbills, ~web_image(.x, height = 50))\n    }\n  ) %>%\n  fmt_currency(\n    columns = vars(total_gross),\n    currency = \"USD\",\n    decimals = 2,\n    suffixing = TRUE\n  ) %>%\n  fmt_number(\n    columns = vars(tix_year),\n    decimals = 0,\n    sep_mark = \",\",\n    use_seps = TRUE\n  ) %>%\n  fmt_percent(\n    columns = vars(avg_capacity),\n    decimals = 1\n  ) %>%\n  tab_options(\n    column_labels.background.color = labs_back,\n    table.background.color = bckgrnd,\n    table.font.color = text_col,\n    data_row.padding = -30,\n    row.striping.background_color = bck_light\n  ) %>%\n  tab_source_note(\n    source_note = md(\"**Data:** Playbill | **Table:** Eric Ekholm (@ekholm_e)\")\n  )\n\nstyled_tbl\n\n\n\n\n\n  \n    \n      Top Earning Broadway Shows\n    \n    \n      through 2019\n    \n  \n  \n    \n      Rank\n      Show\n      \n      Premiere Year\n      Total Earnings\n      Avg TheaterCapacity Filled\n      \n        Tickets Sold Per Year\n      \n    \n    \n      Average\n      1987 - 2019\n    \n  \n  \n    1\nThe Lion King\n\n1997\n$1.66B\n97.7%\n676,795\n\n    2\nWicked\n\n2003\n$1.35B\n97.2%\n708,412\n\n    3\nThe Phantom of the Opera\n\n1988\n$1.24B\n89.8%\n600,508\n\n    4\nChicago\n\n1996\n$673.91M\n82.7%\n392,534\n\n    5\nThe Book of Mormon\n\n2011\n$647.07M\n102.4%\n445,435\n\n    6\nMamma Mia!\n\n2001\n$624.39M\n89.7%\n504,408\n\n    7\nHamilton\n\n2015\n$620.60M\n101.7%\n498,052\n\n    8\nJersey Boys\n\n2005\n$557.51M\n89.5%\n395,571\n\n    9\nLes Miserables\n\n1987\n$548.80M\n88.5%\n353,014\n\n    10\nAladdin\n\n2014\n$447.72M\n98.0%\n686,538\n\n  \n  \n    \n      Data: Playbill | Table: Eric Ekholm (@ekholm_e)"
  },
  {
    "objectID": "posts/rstudio-table-contest-submission/index.html#impressions",
    "href": "posts/rstudio-table-contest-submission/index.html#impressions",
    "title": "RStudio Table Contest Submission",
    "section": "Impressions",
    "text": "Impressions\nRight, so, my overall impression of {gt} is that it’s an amazing package for building stylized static tables that fills a gap in R’s table ecosystem (to the extent that’s a thing). In my day job, I often find myself having to build tables either 1) as part of documents I’m creating on my own or 2) as stand-alone pieces that end up getting dropped into Powerpoints other people are putting together, and I’m excited about incorporating {gt} into my workflow for both of those types of tasks. Some more specific impressions of {gt}:\n\nIt feels a lot like {ggtplot2}. This isn’t surprising, given that the intent of the package is to provide a “grammar of tables.” But the flow & general process felt very familiar to me even though I haven’t used it extensively before, and I imagine anyone else who’s reasonably proficient using ggplot will feel the same when picking up gt. Which is a big plus, because it mitigates a lot of that difficulty of learning a new package.\nIt’s refreshingly easy to work with fonts. My biggest sore spot with ggplot is incorporating different fonts, which I always seem to struggle with (and my understanding is that this is a common struggle for Windows users). The {ragg} package seems to have made using fonts in ggplot easier, though. That said, using any Google font in gt is as easy as dropping in the google_font() function and voila, it works! Such a nice change of pace after my typical long troubleshooting sessions with fonts in ggplot.\nThe ability to include ggplot images and web images in a table is pretty cool. You’ll see that I added both to my table above, and each felt very easy to include. Honestly, the most time-consuming part was finding the urls for the playbill images.\nThe helper functions to format numbers, percents, and currency are great. I work with a lot of large $ amounts – as well as percents – in my job, and I’m super stoked about not having to manually format these anymore.\nI may have run into a small bug passing where font types (e.g. Lobster, Rubik) weren’t being recognized when called from within a list in the tab_style() function, which I’ll open an issue for. I found a workaround, but it involved essentially stylizing the same element multiple times, which doesn’t feel ideal.\n\nOverall, {gt} is a really awesome package – huge thanks to the team at Rstudio for putting it together and maintaining it!"
  },
  {
    "objectID": "posts/rva-pets/index.html",
    "href": "posts/rva-pets/index.html",
    "title": "RVA Pets",
    "section": "",
    "text": "First, let’s load our packages and set our plot themes/colors\n\nlibrary(tidyverse)\nlibrary(osmdata)\nlibrary(sf)\nlibrary(janitor)\nlibrary(hrbrthemes)\nlibrary(wesanderson)\nlibrary(tidytext)\nlibrary(kableExtra)\nlibrary(ggtext)\ntheme_set(theme_ipsum())\npal <- wes_palette(\"Zissou1\")\ncolors <- c(\"Dog\" = pal[1], \"Cat\" = pal[3])\n\nNext, we’ll read in the data and clean it up a little bit. In this dataset, each row represents a licensed pet in Richmond, Virginia. The dataset includes animal type (dog, cat, puppy, kitten) and the address of the owners. Whoever set up the data was also nice enough to include longitude and latitude for each address in the dataset, which means I don’t need to go out and get it. For our purposes here, I’m going to lump puppies in with dogs and kittens in with cats. I’m also going to extract the “location” column into a few separate columns. Let’s take a look at the first few entries.\n\npets_raw <- read_csv(here::here(\"data/rva_pets_2019.csv\"))\npets_clean <- pets_raw %>%\n  clean_names() %>%\n  extract(col = location_1,\n          into = c(\"address\", \"zip\", \"lat\", \"long\"),\n          regex = \"(.*)\\n.*(\\\\d{5})\\n\\\\((.*), (.*)\\\\)\") %>%\n  mutate(animal_type = str_replace_all(animal_type, c(\"Puppy\" = \"Dog\", \"Kitten\" = \"Cat\")))\nhead(pets_clean) %>%\n  kable(format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n \n  \n    animal_type \n    animal_name \n    address \n    zip \n    lat \n    long \n    load_date \n  \n \n\n  \n    Cat \n    Molly \n    301 Virginia Street APT 1008 \n    23219 \n    37.53294 \n    -77.433825 \n    20180627 \n  \n  \n    Dog \n    Sam \n    1407 Wilmington Avenue \n    23227 \n    37.58294 \n    -77.455213 \n    20180627 \n  \n  \n    Cat \n    Taffy \n    114 N Harvie Street \n    23220 \n    37.548414 \n    -77.45745 \n    20180627 \n  \n  \n    Dog \n    Jackson \n    4804 Riverside Drive \n    23225 \n    37.527326 \n    -77.483249 \n    20180627 \n  \n  \n    Dog \n    Cirrus \n    3107 E Marshall Street \n    23223 \n    37.52904 \n    -77.412272 \n    20180627 \n  \n  \n    Dog \n    Henri \n    1900 Maple Shade Lane \n    23227 \n    37.581979 \n    -77.466207 \n    20180627 \n  \n\n\n\n\n\nOk, now that our data is set up, let’s see if there are more cats or dogs in the city.\n\npets_clean %>%\n  count(animal_type) %>%\n  ggplot(aes(x = n, y = animal_type)) +\n  geom_col(color = pal[1], fill = pal[1]) +\n  geom_text(aes(x = n-50, label = n), hjust = 1, color = \"white\", fontface = \"bold\") +\n  labs(\n    title = \"Number of Cats vs Dogs\"\n  )\n\n\n\n\nAlright, so, lots more dogs. Like almost 4 to 1 dogs to cats. Which is something I can get behind. I’m a firm believer in the fact that dogs are wayyy better than cats.\nI’m also interested in the most common names for pets in RVA.\n\npets_clean %>%\n  group_by(animal_type) %>%\n  count(animal_name, sort = TRUE) %>%\n  slice(1:15) %>%\n  ungroup() %>%\n  ggplot(aes(x = n, y = reorder_within(animal_name, n, animal_type))) +\n    geom_col(color = pal[1], fill = pal[1]) +\n    geom_text(aes(x = if_else(animal_type == \"Cat\", n - .25, n - 1), label = n), hjust = 1, color = \"white\", fontface = \"bold\") +\n    facet_wrap(~animal_type, scales = \"free\") +\n    scale_y_reordered() +\n    labs(\n      title = \"Top Pet Names\",\n      y = NULL\n    )\n\n\n\n\nThese seem pretty standard to me, and unfortunately, nothing is screaming “RVA” here. No “Bagels,” no “Gwars,” etc.\nI also pulled out zip codes into their own column earlier, so we can take a look at which zip codes have the most dogs and cats.\n\npets_clean %>%\n  filter(!is.na(zip)) %>%\n  group_by(zip) %>%\n  count(animal_type, sort = TRUE)%>%\n  ungroup() %>%\n  group_by(animal_type) %>%\n  top_n(n = 10) %>%\n  ungroup() %>%\n  ggplot(aes(x = n, y = reorder_within(zip, n, animal_type))) +\n    geom_col(color = pal[1], fill = pal[1]) +\n    geom_text(aes(x = if_else(animal_type == \"Cat\", n - 1, n - 4), label = n), hjust = 1, color = \"white\", fontface = \"bold\") +\n    facet_wrap(~animal_type, scales = \"free\") +\n    scale_y_reordered() +\n    labs(\n      title = \"Number of Pets by Zipcode\",\n      y = NULL\n    )\n\n\n\n\nAlright, so most of the pets here live in Forest Hill/generally south of the river in 23225, and another big chunk live in 23220, which covers a few neighborhoods & includes The Fan, which is probably where most of the pet action is.\nAnd finally, since we have the latitude and longitude, I can put together a streetmap of the city showing where all of these little critters live. To do this, I’m going to grab some shape files through the OpenStreetMaps API and plot the pet datapoints on top of those.\n\npets_map <- st_as_sf(pets_clean %>%\n                       filter(!is.na(long)), coords = c(\"long\", \"lat\"),\n                     crs = 4326)\n\nget_rva_maps <- function(key, value) {\n  getbb(\"Richmond Virginia United States\") %>%\n    opq() %>%\n    add_osm_feature(key = key,\n                    value = value) %>%\n    osmdata_sf()\n}\n\nrva_streets <- get_rva_maps(key = \"highway\", value = c(\"motorway\", \"primary\", \"secondary\", \"tertiary\"))\n\nsmall_streets <- get_rva_maps(key = \"highway\", value = c(\"residential\", \"living_street\",\n                                                         \"unclassified\",\n                                                         \"service\", \"footway\", \"cycleway\"))\n\nriver <- get_rva_maps(key = \"waterway\", value = \"river\")\n\ndf <- tibble(\n  type = c(\"big_streets\", \"small_streets\", \"river\"),\n  lines = map(\n    .x = lst(rva_streets, small_streets, river),\n    .f = ~pluck(., \"osm_lines\")\n  )\n)\n\ncoords <- pluck(rva_streets, \"bbox\")\n\nannotations <- tibble(\n  label = c(\"<span style='color:#FFFFFF'><span style='color:#EBCC2A'>**Cats**</span> and <span style='color:#3B9AB2'>**Dogs**</span> in RVA</span>\"),\n  x = c(-77.555),\n  y = c(37.605),\n  hjust = c(0)\n)\n\nrva_pets <- ggplot() +\n  geom_sf(data = df$lines[[1]],\n          inherit.aes = FALSE,\n          size = .3,\n          alpha = .8, \n          color = \"white\") +\n  geom_sf(data = pets_map, aes(color = animal_type), alpha = .6, size = .75) +\n  geom_richtext(data = annotations, aes(x = x, y = y, label = label, hjust = hjust), fill = NA, label.color = NA, \n                label.padding = grid::unit(rep(0, 4), \"pt\"), size = 11) + \n  coord_sf(\n    xlim = c(-77.55, -77.4),\n    ylim = c(37.5, 37.61),\n    expand = TRUE\n  ) +\n  theme_void() +\n  scale_color_manual(\n    values = colors\n  ) +\n  theme(\n    legend.position = \"none\",\n    plot.background = element_rect(fill = \"grey10\"),\n    panel.background = element_rect(fill = \"grey10\"),\n    text = element_markdown()\n  )\n\n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-nc/4.0/CitationBibTeX citation:@online{ekholm2020,\n  author = {Eric Ekholm},\n  title = {RVA {Pets}},\n  date = {2020-04-23},\n  url = {https://www.ericekholm.com/posts/rva-pets},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEric Ekholm. 2020. “RVA Pets.” April 23, 2020. https://www.ericekholm.com/posts/rva-pets."
  },
  {
    "objectID": "posts/scooby-doo-eda/index.html",
    "href": "posts/scooby-doo-eda/index.html",
    "title": "Scooby Doo EDA",
    "section": "",
    "text": "I’m going to shoot for spending an hour-ish on this, but I might end up doing more or less.\n\nlibrary(tidyverse)\nlibrary(eemisc)\nlibrary(harrypotter)\nlibrary(lubridate)\n\nherm <- harrypotter::hp(n = 1, option = \"HermioneGranger\")\n\nopts <- options(\n  ggplot2.discrete.fill = list(\n    harrypotter::hp(n = 2, option = \"HermioneGranger\"),\n    harrypotter::hp(n = 3, option = \"HermioneGranger\"),\n    harrypotter::hp(n = 7, option = \"Always\")\n  )\n)\n\ntheme_set(theme_ee())\n\nscooby_raw <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-07-13/scoobydoo.csv', na = c(\"\", \"NA\", \"NULL\"))\n\nWhat does the data look like?\n\nglimpse(scooby_raw)\n\nRows: 603\nColumns: 75\n$ index                    <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14…\n$ series_name              <chr> \"Scooby Doo, Where Are You!\", \"Scooby Doo, Wh…\n$ network                  <chr> \"CBS\", \"CBS\", \"CBS\", \"CBS\", \"CBS\", \"CBS\", \"CB…\n$ season                   <chr> \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", …\n$ title                    <chr> \"What a Night for a Knight\", \"A Clue for Scoo…\n$ imdb                     <dbl> 8.1, 8.1, 8.0, 7.8, 7.5, 8.4, 7.6, 8.2, 8.1, …\n$ engagement               <dbl> 556, 479, 455, 426, 391, 384, 358, 358, 371, …\n$ date_aired               <date> 1969-09-13, 1969-09-20, 1969-09-27, 1969-10-…\n$ run_time                 <dbl> 21, 22, 21, 21, 21, 21, 21, 21, 21, 21, 21, 2…\n$ format                   <chr> \"TV Series\", \"TV Series\", \"TV Series\", \"TV Se…\n$ monster_name             <chr> \"Black Knight\", \"Ghost of Cptn. Cuttler\", \"Ph…\n$ monster_gender           <chr> \"Male\", \"Male\", \"Male\", \"Male\", \"Female\", \"Ma…\n$ monster_type             <chr> \"Possessed Object\", \"Ghost\", \"Ghost\", \"Ancien…\n$ monster_subtype          <chr> \"Suit\", \"Suit\", \"Phantom\", \"Miner\", \"Witch Do…\n$ monster_species          <chr> \"Object\", \"Human\", \"Human\", \"Human\", \"Human\",…\n$ monster_real             <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n$ monster_amount           <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, …\n$ caught_fred              <lgl> FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, TRUE,…\n$ caught_daphnie           <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n$ caught_velma             <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n$ caught_shaggy            <lgl> TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ caught_scooby            <lgl> TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE,…\n$ captured_fred            <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ captured_daphnie         <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ captured_velma           <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ captured_shaggy          <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n$ captured_scooby          <lgl> FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALS…\n$ unmask_fred              <lgl> FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, …\n$ unmask_daphnie           <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n$ unmask_velma             <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n$ unmask_shaggy            <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRU…\n$ unmask_scooby            <lgl> TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE…\n$ snack_fred               <lgl> TRUE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE,…\n$ snack_daphnie            <lgl> FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE…\n$ snack_velma              <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE…\n$ snack_shaggy             <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n$ snack_scooby             <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n$ unmask_other             <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n$ caught_other             <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n$ caught_not               <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n$ trap_work_first          <lgl> NA, FALSE, FALSE, TRUE, NA, TRUE, FALSE, FALS…\n$ setting_terrain          <chr> \"Urban\", \"Coast\", \"Island\", \"Cave\", \"Desert\",…\n$ setting_country_state    <chr> \"United States\", \"United States\", \"United Sta…\n$ suspects_amount          <dbl> 2, 2, 0, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, …\n$ non_suspect              <lgl> FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE…\n$ arrested                 <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FAL…\n$ culprit_name             <chr> \"Mr. Wickles\", \"Cptn. Cuttler\", \"Bluestone th…\n$ culprit_gender           <chr> \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male…\n$ culprit_amount           <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, …\n$ motive                   <chr> \"Theft\", \"Theft\", \"Treasure\", \"Natural Resour…\n$ if_it_wasnt_for          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"thes…\n$ and_that                 <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"dog\"…\n$ door_gag                 <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n$ number_of_snacks         <chr> \"2\", \"1\", \"3\", \"2\", \"2\", \"4\", \"4\", \"0\", \"1\", …\n$ split_up                 <dbl> 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, …\n$ another_mystery          <dbl> 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ set_a_trap               <dbl> 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, …\n$ jeepers                  <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ jinkies                  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ my_glasses               <dbl> 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, …\n$ just_about_wrapped_up    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ zoinks                   <dbl> 1, 3, 1, 2, 0, 2, 1, 0, 0, 0, 0, 6, 3, 5, 8, …\n$ groovy                   <dbl> 0, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, …\n$ scooby_doo_where_are_you <dbl> 0, 1, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 1, 0, …\n$ rooby_rooby_roo          <dbl> 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 3, 0, 0, 0, …\n$ batman                   <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n$ scooby_dum               <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n$ scrappy_doo              <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n$ hex_girls                <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n$ blue_falcon              <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n$ fred_va                  <chr> \"Frank Welker\", \"Frank Welker\", \"Frank Welker…\n$ daphnie_va               <chr> \"Stefanianna Christopherson\", \"Stefanianna Ch…\n$ velma_va                 <chr> \"Nicole Jaffe\", \"Nicole Jaffe\", \"Nicole Jaffe…\n$ shaggy_va                <chr> \"Casey Kasem\", \"Casey Kasem\", \"Casey Kasem\", …\n$ scooby_va                <chr> \"Don Messick\", \"Don Messick\", \"Don Messick\", …\n\n\nWhat’s the range of dates we’re looking at here?\n\nrange(scooby_raw$date_aired)\n\n[1] \"1969-09-13\" \"2021-02-25\"\n\n\nAnd how many episodes are we seeing each year?\n\nscooby_raw %>%\n  count(year(date_aired)) %>%\n  rename(year = 1) %>%\n  ggplot(aes(x = year, y = n)) +\n  geom_col(fill = herm)\n\n\n\n\nWhat about episodes by decade?\n\nscooby_raw%>%\n  count(10*year(date_aired) %/% 10) %>%\n  rename(decade = 1) %>%\n  ggplot(aes(x = decade, y = n)) +\n  geom_col(fill = herm)\n\n\n\n\nNext, let’s look at what ratings look like over time:\n\nscooby_raw %>%\n  ggplot(aes(x = index, y = imdb)) +\n  geom_point() +\n  geom_line() +\n  geom_smooth()\n\n\n\n\nAnd what if we color the points by series – I’d imagine series might have different ratings:\n\nscooby_raw %>%\n  ggplot(aes(x = index, y = imdb)) +\n  geom_point(aes(color = series_name)) +\n  geom_line(color = \"grey70\") +\n  theme(legend.position = \"none\")\n\n\n\n\nNext, I’m interested in looking at some comparisons across characters for different actions they take, like unmasking baddies, getting caught, etc. There are a bunch of these logical columns (e.g. unmask_fred), and so I’ll write a little helper function to summarize them and then pivot them into a shape that’ll be easier to plot later.\n\nsummarize_pivot <- function(df, str) {\n  \n  df %>%\n    summarize(across(starts_with(str), ~sum(.x, na.rm = TRUE))) %>%\n    pivot_longer(\n      cols = everything(),\n      names_to = \"key\",\n      values_to = \"value\"\n    ) %>%\n    extract(col = key, into = c(\"key\", \"char\"), regex = \"^(.*)_(.*)$\") %>%\n    arrange(desc(value))\n}\n\nAn example of what this does:\n\nscooby_raw %>%\n  summarize_pivot(\"unmask\")\n\n# A tibble: 6 × 3\n  key    char    value\n  <chr>  <chr>   <int>\n1 unmask fred      102\n2 unmask velma      94\n3 unmask daphnie    37\n4 unmask other      35\n5 unmask scooby     23\n6 unmask shaggy     13\n\n\nAaaand another example:\n\nscooby_raw %>%\n  summarize_pivot(\"caught\")\n\n# A tibble: 7 × 3\n  key    char    value\n  <chr>  <chr>   <int>\n1 caught scooby    160\n2 caught fred      132\n3 caught other      84\n4 caught shaggy     77\n5 caught velma      41\n6 caught not        31\n7 caught daphnie    29\n\n\nNext, let’s use purrr::map() to do this a few times, combine the results into a df, and then make a plot\n\niter_strs <- c(\"caught\", \"captured\", \"unmask\", \"snack\")\n\nactions_df <- map_dfr(iter_strs, ~summarize_pivot(scooby_raw, .x))\n\nglimpse(actions_df)\n\nRows: 23\nColumns: 3\n$ key   <chr> \"caught\", \"caught\", \"caught\", \"caught\", \"caught\", \"caught\", \"cau…\n$ char  <chr> \"scooby\", \"fred\", \"other\", \"shaggy\", \"velma\", \"not\", \"daphnie\", …\n$ value <int> 160, 132, 84, 77, 41, 31, 29, 91, 85, 83, 74, 71, 102, 94, 37, 3…\n\n\n\nactions_df %>%\n  ggplot(aes(x = value, y = char, fill = key)) +\n  geom_col() +\n  facet_wrap(vars(key), scales = \"free_y\") +\n  theme(\n    legend.position = \"none\"\n  )\n\n\n\n\nRight, so we see that all of the characters get captured more or less the same amount, Fred and Scooby tend to catch monsters the most, Daphnie and Shaggy eat the most snacks, and Velma and Fred do the most unmasking.\nSwitching up a bit, what if we want to look at monster’s motives? First let’s take a look at all of the unique motives.\n\nunique(scooby_raw$motive)\n\n [1] \"Theft\"            \"Treasure\"         \"Natural Resource\" \"Competition\"     \n [5] \"Extortion\"        \"Safety\"           \"Counterfeit\"      \"Inheritance\"     \n [9] \"Smuggling\"        \"Preservation\"     NA                 \"Experimentation\" \n[13] \"Food\"             \"Trespassing\"      \"Assistance\"       \"Abduction\"       \n[17] \"Haunt\"            \"Anger\"            \"Imagination\"      \"Bully\"           \n[21] \"Loneliness\"       \"Training\"         \"Conquer\"          \"Mistake\"         \n[25] \"Automated\"        \"Production\"       \"Entertainment\"    \"Simulation\"      \n\n\nAnd it’s probably useful to count these:\n\nscooby_raw %>% \n  count(motive, sort = TRUE)\n\n# A tibble: 28 × 2\n   motive               n\n   <chr>            <int>\n 1 Competition        168\n 2 Theft              125\n 3 <NA>                67\n 4 Treasure            54\n 5 Conquer             42\n 6 Natural Resource    26\n 7 Smuggling           22\n 8 Trespassing         15\n 9 Abduction           12\n10 Food                11\n# … with 18 more rows\n\n\nSo, “Competition” is far and away the most common motive. I’m not sure I really understand what this means? But it’s also been a while since I’ve watched Scooby Doo.\nI’m also interested in how often we see “zoinks” in episodes, bc I feel like this is the defining line of the show (along with the meddling kids, which I’ll look at next).\n\nscooby_raw %>%\n  ggplot(aes(x = zoinks)) +\n  geom_histogram(bins = 20, fill = herm)\n\n\n\n\nThis feels weird to me. Most often, we get 0 or 1, but then there are episodes with more than 10? I’d imagine these are probably movies?\n\nscooby_raw %>%\n  ggplot(aes(x = zoinks)) +\n  geom_histogram(bins = 10, fill = herm) +\n  facet_wrap(vars(format), scales = \"free_y\")\n\n\n\n\nWell, so, there are still some TV shows that have a ton of zoinks’s. But also our biggest outlier is a movie, which makes sense to me since there’s more time for zoinking.\nAnd what about our “if it wasn’t for those meddling kids” data?\n\nlength(unique(scooby_raw$if_it_wasnt_for))\n\n[1] 108\n\n\nOk, wow, so that’s a lot of different values for “if it wasn’t for…”\nFirst, let’s just see how many episodes have the “if it wasn’t for…” catchphrase\n\nscooby_raw %>%\n  mutate(has_catchphrase =  if_else(!is.na(if_it_wasnt_for), TRUE, FALSE)) %>%\n  count(has_catchphrase)\n\n# A tibble: 2 × 2\n  has_catchphrase     n\n  <lgl>           <int>\n1 FALSE             414\n2 TRUE              189\n\n\nCool, so, 189 of our 603 episodes have the “if it wasn’t for…” catchphrase.\nAnd now which of these also use the term “meddling?”\n\nscooby_raw %>%\n  filter(!is.na(if_it_wasnt_for)) %>%\n  mutate(meddling = if_else(str_detect(if_it_wasnt_for, \"meddling\"), TRUE, FALSE)) %>%\n  count(meddling) %>%\n  ggplot(aes(x = n, y = meddling)) +\n  geom_col(fill = herm) +\n  geom_text(aes(label = n, x = n - 1), hjust = 1, color = \"white\")\n\n\n\n\nAlright, so, of the 189 episodes that have the “if it wasn’t for…” catchphrase, most of those also include the word “meddling!”\nThe last little bit here – because I’m trying to keep my time to about an hour (again, to test out the feel for if this is a viable approach to streaming or making videos), is going to be to fit a quick linear model predicting the imdb rating of an episode.\n\nlibrary(tidymodels)\n\nLet’s just use numeric/logical columns in our model, mostly because preprocessing them is pretty straightforward (although note that this doesn’t mean what I’m doing below is anywhere near the best approach). Then let’s look at how much missing data we have for each of these columns.\n\nmod_df <- scooby_raw %>%\n  select(where(is.numeric) | where(is.logical)) %>%\n  filter(!is.na(imdb))\n\nmiss_df <- mod_df %>%\n  summarize(across(everything(), ~(sum(is.na(.x))/length(.x))))\n\nmiss_df\n\n# A tibble: 1 × 51\n  index  imdb engagement run_time monster_amount suspects_amount culprit_amount\n  <dbl> <dbl>      <dbl>    <dbl>          <dbl>           <dbl>          <dbl>\n1     0     0          0        0              0               0              0\n# … with 44 more variables: split_up <dbl>, another_mystery <dbl>,\n#   set_a_trap <dbl>, jeepers <dbl>, jinkies <dbl>, my_glasses <dbl>,\n#   just_about_wrapped_up <dbl>, zoinks <dbl>, groovy <dbl>,\n#   scooby_doo_where_are_you <dbl>, rooby_rooby_roo <dbl>, monster_real <dbl>,\n#   caught_fred <dbl>, caught_daphnie <dbl>, caught_velma <dbl>,\n#   caught_shaggy <dbl>, caught_scooby <dbl>, captured_fred <dbl>,\n#   captured_daphnie <dbl>, captured_velma <dbl>, captured_shaggy <dbl>, …\n\n\nSo, some of these columns have a ton of missing data. Just to keep moving forward on this, I’m going to chuck any columns with more than 20% missing data, then median impute cases with missing data in the remaining columns (which we’ll do in the recipes step below).\n\nkeep_vars <- miss_df %>%\n  pivot_longer(cols = everything(),\n               names_to = \"nms\",\n               values_to = \"vals\") %>%\n  filter(vals < .2) %>%\n  pull(1)\n\nmod_df <- mod_df %>%\n  select(all_of(keep_vars)) %>%\n  mutate(across(where(is.logical), as.numeric))\n\nNow we’ll set up some bootstrap resamples. I’m using bootstrap resamples here rather than k-fold because it’s a relatively small dataset.\n\nset.seed(0408)\nbooties <- bootstraps(mod_df, times = 10)\n\nAnd then let’s define some very basic preprocessing using a recipe:\n\nrec <- recipe(imdb ~ ., data = mod_df) %>%\n  step_impute_median(all_numeric_predictors()) %>%\n  step_normalize(all_numeric_predictors()) \n\nAnd let’s do a lasso regression, just using a small and kinda of arbitrary penalty value (we could tune this, but I’m not going to).\n\nlasso_spec <- linear_reg(mixture = 1, penalty = .001) %>%\n  set_engine(\"glmnet\")\n\n#combining everything into a workflow\nlasso_wf <- workflow() %>%\n  add_recipe(rec) %>%\n  add_model(lasso_spec)\n\nAnd now let’s fit!\n\nlasso_res <- fit_resamples(\n  lasso_wf,\n  resamples = booties\n)\n\nThe main reason for fitting on these resamples is to check our model performance, so let’s do that.\n\ncollect_metrics(lasso_res)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.626    10  0.0104 Preprocessor1_Model1\n2 rsq     standard   0.280    10  0.0165 Preprocessor1_Model1\n\n\nOur R-squared is .29, which isn’t great, but it’s also not terrible considering we really didn’t put much effort into our preprocessing here, and we discarded a bunch of data.\nLet’s fit one final time on the full dataset to look at the importance of our predictor variables:\n\nprepped_df <- rec %>%\n  prep() %>%\n  bake(new_data = NULL)\n\nmod_fit <- lasso_spec %>%\n  fit(imdb ~ ., data = prepped_df)\n\nAnd then finally we can look at our coefficients.\n\nmod_fit %>%\n  tidy() %>%\n  filter(term != \"(Intercept)\") %>%\n  arrange(desc(abs(estimate))) %>%\n  ggplot(aes(x = estimate, y = fct_reorder(term, abs(estimate)), fill = estimate >= 0)) +\n  geom_col() +\n  labs(\n    y = NULL\n  )\n\n\n\n\nAnd there we go. That was a bit more than an hour, but it was worth it to get to a reasonable stopping point!\n\n\n\nReusehttps://creativecommons.org/licenses/by-nc/4.0/CitationBibTeX citation:@online{ekholm2021,\n  author = {Eric Ekholm},\n  title = {Scooby {Doo} {EDA}},\n  date = {2021-07-20},\n  url = {https://www.ericekholm.com/posts/scooby-doo-eda},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEric Ekholm. 2021. “Scooby Doo EDA.” July 20, 2021. https://www.ericekholm.com/posts/scooby-doo-eda."
  },
  {
    "objectID": "posts/scrantonicity-part-1/index.html",
    "href": "posts/scrantonicity-part-1/index.html",
    "title": "Scratonicity - Part 1",
    "section": "",
    "text": "Who doesn’t love The Office? I went through high school and college following the on-again off-again romance of Jim and Pam, the Icarus-esque ascendancy and fall of Ryan the Temp, and the perpetual cringey-ness of Michael Scott. And aside from that handful of people who fled the room in a cold panic at even the mention of “Scott’s Tots,” I think this was probably true for most of my generation. You’d be hard pressed to go to a Halloween party in the late aughts without seeing someone dressed in the tan-and-yellow palette of Dwight Schrute, and before the modern era of Netflix and Hulu, we regularly set aside Thursday nights to tune into NBC.\nAnd although I was a big Office fan several years ago, I haven’t thought too too much about it recently – at least until I stumbled across the release of the {schrute} package recently. {schrute} is an R package with one purpose – presenting the entire transcripts of The Office in tibble format, making the dialogue of the show much easier to analyze. I played around with the package and a quick sentiment analysis back in December when I looked at the sentiments expressed by Jim and Pam over the course of the series:\n\n\n\n\n\nThere’s a ton more we can do with the package, though, and with the transcripts available and in a clean format, plus all of the tools R has available for text analysis, I figured I’d do a mini-series of blog posts analyzing some of the data. The plan (as of now) is to start this first post with some exploratory analyses and visualizations, then move into some other types of modeling in later posts. I’ll also include all of my code throughout.\n\nAs a quick aside, a lot of the text analyses I’m going to work through in this first post come from the Text Mining with R book by Julia Silge and David Robinson. I’d strongly recommend this to anyone looking to dive into analyzing text data."
  },
  {
    "objectID": "posts/scrantonicity-part-1/index.html#setup",
    "href": "posts/scrantonicity-part-1/index.html#setup",
    "title": "Scratonicity - Part 1",
    "section": "Setup",
    "text": "Setup\nFirst, let’s read in the data. I’m also going to limit the data to the first seven seasons, which spans the “Michael Scott” era. Not only because these are the best seasons (which they undoubtedly are), but also because doing so eliminates a major confounding factor (i.e. Steve Carell leaving the show) from the analysis.\n\noffice <- theoffice %>%\n  filter(as.numeric(season) <= 7)\n\nglimpse(office)\n\nRows: 41,348\nColumns: 12\n$ index            <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16…\n$ season           <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ episode          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ episode_name     <chr> \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\",…\n$ director         <chr> \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis…\n$ writer           <chr> \"Ricky Gervais;Stephen Merchant;Greg Daniels\", \"Ricky…\n$ character        <chr> \"Michael\", \"Jim\", \"Michael\", \"Jim\", \"Michael\", \"Micha…\n$ text             <chr> \"All right Jim. Your quarterlies look very good. How …\n$ text_w_direction <chr> \"All right Jim. Your quarterlies look very good. How …\n$ imdb_rating      <dbl> 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6…\n$ total_votes      <int> 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706,…\n$ air_date         <fct> 2005-03-24, 2005-03-24, 2005-03-24, 2005-03-24, 2005-…\n\n\nJust to check that the data we have matches what we’re expecting, let’s take a look at which seasons we have, plus how many episodes we have per season.\n\noffice %>%\n  distinct(season, episode) %>%\n  count(season, name = \"num_episodes\")\n\n# A tibble: 7 × 2\n  season num_episodes\n   <int>        <int>\n1      1            6\n2      2           22\n3      3           23\n4      4           14\n5      5           26\n6      6           24\n7      7           24\n\n\nThis generally matches what Wikipedia is telling me once we account for two-part episodes, and we can see that we only have the first seven seasons.\n\nMe think, why waste time say lot word, when few word do trick\nA few questions we can ask here involve how much/how often different characters speak. Probably the most basic question is: who has the most lines?\n\ntop_20_chars <- office %>%\n  count(character, sort = TRUE) %>%\n  top_n(20) %>%\n  pull(character)\noffice %>%\n  filter(is.element(character, top_20_chars)) %>%\n  count(character, sort = TRUE) %>%\n  ggplot(aes(x = fct_reorder(character, n), y = n)) +\n  geom_col(fill = purple) +\n  labs(\n    x = \"\",\n    y = \"Number of Lines\",\n    title = \"Who Has the Most Lines?\"\n  ) +\n  coord_flip()\n\n\n\n\nIt’s not surprising to me that Michael has the most lines, but the magnitude of the difference between him and Dwight is a bit surprising.\nWhat if we look at the number of lines per season?\n\noffice %>%\n  filter(is.element(character, top_20_chars)) %>%\n  count(character, season, sort = TRUE) %>%\n  ggplot(aes(x = as.numeric(season), y = n, color = character)) +\n    geom_line() +\n    geom_point()\n\n\n\n\nThis isn’t terribly informative – let’s go back to our bar graph.\n\noffice %>%\n  filter(is.element(character, top_20_chars)) %>%\n  count(character, season, sort = TRUE) %>%\n  group_by(season) %>%\n  top_n(n = 5) %>%\n  ungroup() %>%\n  ggplot(aes(x = fct_reorder(character, n), y = n)) +\n    geom_col(fill = purple) +\n    coord_flip() +\n    facet_wrap(~season, scales = \"free\") +\n    labs(\n      title = \"Number of Lines by Season\",\n      x = \"\",\n      y = \"\"\n    ) +\n    theme_minimal()\n\n\n\n\nAgain, not surprising that Michael has the most lines across all seasons. Dwight, Jim, and Pam are always the next three, but the orders change a bit between seasons. The fifth spot is where we see some movement, with Oscar and Jan sneaking in before Andy joins the show in Season 3. And check out Ryan in S4!\n\n\nSometimes I’ll start a sentence and I don’t even know where it’s going\nSo, above, we just looked at the number of lines each character had. Another option is to do some analyses at the word level. For instance, we can look at patterns of word usage for individual characters, between characters, and over time.\nTo start with this, I’m going to restructure the data so we have one word per row in our tibble. I’m also going to remove “stop words” (e.g. “a,” “the,” “at”), since these will show up a lot but (for our purposes) aren’t actually all that meaningful:\n\noffice_words <- office %>%\n  unnest_tokens(word, text) %>%\n  anti_join(stop_words)\nglimpse(office_words)\n\nRows: 125,040\nColumns: 12\n$ index            <int> 1, 1, 1, 2, 2, 3, 3, 3, 4, 4, 6, 6, 6, 6, 6, 6, 6, 6,…\n$ season           <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ episode          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ episode_name     <chr> \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\",…\n$ director         <chr> \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis…\n$ writer           <chr> \"Ricky Gervais;Stephen Merchant;Greg Daniels\", \"Ricky…\n$ character        <chr> \"Michael\", \"Michael\", \"Michael\", \"Jim\", \"Jim\", \"Micha…\n$ text_w_direction <chr> \"All right Jim. Your quarterlies look very good. How …\n$ imdb_rating      <dbl> 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6…\n$ total_votes      <int> 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706,…\n$ air_date         <fct> 2005-03-24, 2005-03-24, 2005-03-24, 2005-03-24, 2005-…\n$ word             <chr> \"jim\", \"quarterlies\", \"library\", \"told\", \"close\", \"ma…\n\n\nWe can see that we have a new column, word, with one word per row. We can also see that the only words in the first line of dialogue (All right Jim. Your quarterlies look very good. How are things at the library?) that make it through the stop words filter are jim, quarterlies, and library. We could fiddle with the stop words list if we wanted to keep words like “good” or “things,” but I’m not too concerned about that for now.\nAs a first pass, let’s take a look at our 20 characters with the most lines of dialogue and see what each character’s most commonly-used word is:\n\noffice_words %>%\n  filter(is.element(character, top_20_chars)) %>%\n  count(character, word, sort = TRUE) %>%\n  group_by(character) %>%\n  top_n(n = 1) %>%\n  kable(format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"condensed\", \"hover\"))\n\n\n\n \n  \n    character \n    word \n    n \n  \n \n\n  \n    Michael \n    yeah \n    563 \n  \n  \n    Dwight \n    michael \n    280 \n  \n  \n    Jim \n    yeah \n    274 \n  \n  \n    Pam \n    michael \n    257 \n  \n  \n    Jan \n    michael \n    159 \n  \n  \n    Andy \n    yeah \n    138 \n  \n  \n    Kevin \n    yeah \n    79 \n  \n  \n    David \n    michael \n    67 \n  \n  \n    Ryan \n    yeah \n    66 \n  \n  \n    Oscar \n    michael \n    65 \n  \n  \n    Phyllis \n    michael \n    59 \n  \n  \n    Toby \n    michael \n    50 \n  \n  \n    Darryl \n    na \n    48 \n  \n  \n    Kelly \n    god \n    44 \n  \n  \n    Angela \n    dwight \n    40 \n  \n  \n    Holly \n    yeah \n    39 \n  \n  \n    Erin \n    michael \n    37 \n  \n  \n    Karen \n    yeah \n    28 \n  \n  \n    Stanley \n    michael \n    27 \n  \n  \n    Meredith \n    wait \n    22 \n  \n\n\n\n\n\nSo, that’s not great. We can see that our stop words didn’t pick up “yeah.” One way around this would be to filter out additional words like “yeah,” “hey,” etc. that aren’t in our stop words list. But we’ll probably still leave out some common words that we might not want to show up in our exploration. A better approach is probably to use the tf-idf statistics (term frequency-inverse document frequency), which adjusts the weight a term is given in the analysis for each character by how commonly the word is used by all characters, with more common words getting lower weights. Essentially, this lets us figure out which words are important/unique to each of our characters.\n\noffice_words %>%\n  filter(is.element(character, top_20_chars)) %>%\n  count(character, word, sort = TRUE) %>%\n  bind_tf_idf(word, character, n) %>%\n  group_by(character) %>%\n  top_n(n = 5, wt = tf_idf) %>%\n  slice(1:5) %>%\n  ungroup() %>%\n  ggplot() +\n    geom_col(aes(x = reorder_within(word, tf_idf, within = character), y = tf_idf), fill = purple) +\n    facet_wrap(~character, scales = \"free\") +\n    coord_flip() +\n    scale_x_reordered() +\n    theme_minimal() +\n    labs(\n      x = \"\",\n      y = \"\",\n      title = \"Which Words are Important to Which Characters?\"\n    ) +\n    theme(\n      axis.text.x = element_blank()\n    )\n\n\n\n\nThis looks right – we see that “tuna” and “nard” are important to Andy, which totally makes sense. Some other gems in here are “wuphf” for Ryan, “wimowheh” for Jim, and “awesome” for Kevin.\nNext, let’s take a closer look at how Michael’s speech compares to some of the other main characters – Dwight, Jim, and Pam. We’ll also leave Kelly in here because I think she’ll be interesting to compare to Michael.\n\nmain_char_words <-  office_words %>%\n  filter(character %in% c(\"Michael\", \"Dwight\", \"Jim\", \"Pam\", \"Kelly\"),\n         str_detect(word, \"\\\\d+\", negate = TRUE)) %>%\n  count(character, word) %>%\n  group_by(character) %>%\n  mutate(word_prop = n/sum(n)) %>%\n  ungroup() %>%\n  select(-n) %>%\n  pivot_wider(names_from = character,\n              values_from = word_prop)\nchar_plot <- function(df, char) {\n  df %>%\n  select(word, Michael, {{char}}) %>%\n  mutate(color = log(abs(Michael-{{char}}))) %>%\n  ggplot(aes(y = Michael, x = {{char}})) +\n    geom_text(aes(label = word, color = color), check_overlap = TRUE, vjust = 1) +\n    geom_abline(color = \"grey50\", lty = 2) +\n    scale_x_log10(labels = scales::percent_format()) +\n    scale_y_log10(labels = scales::percent_format()) +\n    scale_color_distiller(\n      type = \"seq\",\n      palette = \"Purples\",\n      direction = 1\n    ) +\n    theme_minimal() +\n    theme(\n      legend.position = \"none\"\n    )\n}\nmain_char_words %>%\n  char_plot(Dwight)\n\n\n\n\nOk, so let’s walk through how to read this. For a given word, the y-axis shows how frequently Michael uses that word, and the x-axis shows how frequently Dwight uses that word. The diagonal dotted line represents equal usage – words that appear on or close to the line are words that Michael and Dwight use about as frequently as one another. Words above the line are those that Michael uses more; words below the line are those that Dwight uses more. Words closer to the line will appear lighter in the graph, whereas words farther way will have more color. So, looking at the graph, we can see that Dwight and Michael both say “hey” pretty often and use the word more or less equally. Dwight says “Mose” way more often than Michael does (because it’s farther from the line), whereas Michael says “Scott” more often than Dwight.\nLet’s take a look at what these graphs look like for Jim and Pam\n\nmain_char_words %>%\n  char_plot(Jim)\n\n\n\n\n\nmain_char_words %>%\n  char_plot(Pam)\n\n\n\n\nAand let’s throw Kelly in there too because it might be interesting.\n\nmain_char_words %>%\n  char_plot(Kelly)\n\n\n\n\nWhat we see here is that, at least when compared to Michael, Kelly’s speech is pretty idiosyncratic – there are lots of words (“blah”, “bitch”, “god”) that she uses waaaayy more frequently than Michael does.\nAnd finally (for this section), I would be remiss if I made it through an analysis of how characters from The Office speak without giving a “that’s what she said” tally…\n\noffice %>%\n  filter(str_detect(text, \"what she said\")) %>%\n  count(character) %>%\n  ggplot(aes(x = fct_reorder(character, n), y = n)) +\n    geom_col(fill = purple) +\n    labs(\n      x = \"\",\n      y = \"Count\",\n      title = \"That's What She Said!\"\n    ) +\n    coord_flip()\n\n\n\n\nNot at all a surprise….\n\n\nIdentity theft is not a joke, Jim!\nFinally, I want to visualize who characters talk to. To do this, I’m going to put together a network plot showing links between characters based on how frequently they interact.\n\nset.seed(0408)\noffice_links <- office %>%\n  filter(character %in% top_20_chars) %>%\n  group_by(episode) %>%\n  mutate(to = lead(character)) %>%\n  ungroup() %>%\n  rename(from = character) %>%\n  count(from, to) %>%\n  filter(from != to,\n         !is.na(to),\n         n > 25)\noffice_verts <- office_links %>%\n  group_by(from) %>%\n  summarize(size = log(sum(n), base = 2)) %>%\n  ungroup()\nnetwork_graph <- graph_from_data_frame(office_links, vertices = office_verts)\nnetwork_graph %>%\n  ggraph(layout = \"igraph\", algorithm = \"fr\") +\n  geom_edge_link(aes(edge_alpha = n^.5), color = purple, edge_width = 1) +\n  geom_node_point(aes(size = size, color = size)) +\n  geom_node_text(aes(label = name, size = size), repel = TRUE, family = \"Garamond\", fontface = \"bold\") +\n  scale_color_distiller(\n      type = \"seq\",\n      palette = \"Purples\",\n      direction = 1\n    ) +\n  labs(\n    title = \"Who Talks to Whom in The Office?\"\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(hjust = .5)\n  )\n\n\n\n\nThe network graph shows links between characters. The size and color of the node (point) associated with a person corresponds to the the total number of interactions they have, with larger and purple-r nodes representing more interactions. The color of the link between characters also corresponds to the number of interactions between two characters, with darker purple links representing more interactions and lighter links representing fewer interactions. Also, characters with more total interactions are sorted toward the center of the network, which is where we see Michael, Jim, Pam, and Dwight. Finally, interactions are only shown if characters have more than 25 total interactions (this prevents the graph from showing a jillion lines).\nI’m going to wrap this one up here, but later on I’ll probably play around a bit with doing some statistical modeling – predicting who is speaking, who a character is speaking to, something like that."
  },
  {
    "objectID": "posts/scrantonicity-part-2/index.html",
    "href": "posts/scrantonicity-part-2/index.html",
    "title": "Scrantonicity - Part 2",
    "section": "",
    "text": "A few weeks ago, I did some exploratory analyses of dialogue from The Office. That blog could easily have been a lot longer than it was, and so instead of writing some gigantic post that would have taken 30 minutes+ to read, I decided to separate it out into several different blog posts. And so here’s volume 2.\nIn this post, I want to try using k-means clustering to identify patterns in who talks to whom in different episodes.\nOnce again, huge thanks to Brad Lindblad, the creator of the {schrute} package for R, which makes the dialogue from The Office easy to work with.\n\n\nAs in the previous blog, I’ll be using the {schrute} package to get the transcripts from the show, and I’m going to limit the dialogue to the first 7 seasons of the show, which is when Michael Scott was around. I’ll also use a handful of other packages for data cleaning, analysis, and visualization. Let’s load all of this in and do some general setup.\n\nset.seed(0408)\n\nlibrary(schrute) #office dialogue\nlibrary(tidyverse) #data wrangling tools\nlibrary(broom) #tidying models\nlibrary(tidytext) #tools for working with text data\nlibrary(knitr) #markdown functionality\nlibrary(kableExtra) #styling for tables\nlibrary(hrbrthemes) #ggplot themes\n\ntheme_set(theme_ipsum())\n\noffice <- theoffice %>%\n  filter(as.numeric(season) <= 7)"
  },
  {
    "objectID": "posts/scrantonicity-part-2/index.html#im-not-superstitious-but-i-am-a-little-stitious.",
    "href": "posts/scrantonicity-part-2/index.html#im-not-superstitious-but-i-am-a-little-stitious.",
    "title": "Scrantonicity - Part 2",
    "section": "I’m not superstitious, but I am a little stitious.",
    "text": "I’m not superstitious, but I am a little stitious.\nNow that we have our data read in and our packages loaded, let’s start with the cluster analysis. The goal here is going to be to figure out if there are certain “types” (clusters, groups, whatever you want to call them) of episodes. There are several frameworks we could use to go about doing this. One approach would be a mixture modeling approach (e.g. latent profile analysis, latent class analysis). I’m not doing that here because I want each episode to be an observation when we cluster, and I’m not sure we have enough episodes here to get good model fits using this approach. Instead, I’m going to use k-means clustering, which basically places observations (episodes, in this case) into one of k groups (where k is supplied by the user) by trying to minimize the “distance” between each observation and the center of the group. The algorithm iteratively assigns observations to groups, updates the center of each group, reassigns observations to groups, etc. until it reaches a stable solution.\nWe can also include all sorts of different variables in the k-means algorithm to serve as indicators. For this analysis, I’m going to use the number of exchanges between different characters per episode – i.e. the number of exchanges between Michael and Jim, between Jim and Dwight, etc. – to estimate groups. This could tell us, for instance, that one “type” of Office episode features lots of exchanges between Michael and Dwight, lots between Pam and Jim, and few between Pam and Michael. One consideration when we use the k-means algorithm is that, because we’re looking at distance between observations, we typically want our observations to be on the same scale. Fortunately, since all of our indicators will be “number of lines per episode,” they’re already on the same scale, so we don’t need to worry about standardizing.\nLet’s go ahead and set up our data. I’m also going to decide to only use the 5 characters who speak the most during the first 7 seasons in this analysis, otherwise the number of combinations of possible exchanges would be huge. These five characters are:\n\ntop5_chars <- office %>%\n  count(character, sort = TRUE) %>%\n  top_n(5) %>%\n  pull(character)\n\ntop5_chars\n\n[1] \"Michael\" \"Dwight\"  \"Jim\"     \"Pam\"     \"Andy\"   \n\n\nOk, so our top 5 characters here are Michael, Dwight, Jim, Pam, and Andy. Since Andy doesn’t join the show until season 3, I’m actually going to narrow our window of usable episodes to those in seasons 3-7. Otherwise, the clustering algorithm would likely group episodes with a focus on those in seasons 1 and 2, where Andy will obviously have 0 lines, vs episodes in later seasons.\nAdditionally, we want to code our changes so that Michael & Jim is the same as Jim & Michael.\n\ncombos <- t(combn(top5_chars, 2)) %>%\n  as_tibble() %>%\n  mutate(comb = glue::glue(\"{V1}&{V2}\"),\n         comb_inv = glue::glue(\"{V2}&{V1}\"))\n\nreplace_comb <- combos$comb\n\nnames(replace_comb) <- combos$comb_inv\n\noffice_exchanges <- office %>%\n  filter(as.numeric(season) >= 3) %>%\n  mutate(char2 = lead(character)) %>% #this will tell us who the speaker is talking to\n  filter(character %in% top5_chars &\n         char2 %in% top5_chars &\n         character != char2) %>% #this filters down to just exchanges between our top 5 characters\n  mutate(exchange = glue::glue(\"{character}&{char2}\") %>%\n           str_replace_all(replace_comb)) %>% #these lines ensure that, e.g. Michael & Jim is coded the same as Jim & Michael\n  select(season, episode_name, character, char2, exchange) %>%\n  count(season, episode_name, exchange) %>%\n  pivot_wider(names_from = exchange,\n              values_from = n,\n              values_fill = list(n = 0))\n\nhead(office_exchanges)\n\n# A tibble: 6 × 12\n  season episode_name         `Dwight&Andy` `Dwight&Jim` `Dwight&Pam` `Jim&Andy`\n   <int> <chr>                        <int>        <int>        <int>      <int>\n1      3 A Benihana Christma…             6           10           17         10\n2      3 Back from Vacation               1           16            6          2\n3      3 Beach Games                      8            8            3          1\n4      3 Ben Franklin                     0           14            2          0\n5      3 Branch Closing                   0            5            1          4\n6      3 Business School                  0           10            3          0\n# … with 6 more variables: `Jim&Pam` <int>, `Michael&Andy` <int>,\n#   `Michael&Dwight` <int>, `Michael&Jim` <int>, `Michael&Pam` <int>,\n#   `Pam&Andy` <int>\n\n\nGreat – now our data is all set up so that we know the number of lines exchanged between main characters in each episode. We can run some clustering algorithms now to see if there are patterns in these exchanges. To do this, we’ll fit models testing out 1-10 clusters. We’ll then look at the error for each of these models graphically and use this to choose how many clusters we want to include in our final model.\n\nclusters_fit <- tibble(\n  k = c(1:10),\n  km_fit = map(c(1:10), ~kmeans(office_exchanges %>% select(-c(1:2)), centers = .))\n) %>%\n  mutate(within_ss = map_dbl(km_fit, ~pluck(., 5)))\n\nclusters_fit %>%\n  ggplot(aes(x = k, y = within_ss)) +\n  geom_point() +\n  geom_line() +\n  labs(\n    title = \"Within Cluster Sum of Squares vs K\"\n  )\n\n\n\n\nWe can see that error decreases as we add more clusters, and error will always decrease as k increases. But we can also see that the rate of decrease slows down a bit as we increase our number of clusters. Ideally, there would be a definitive bend, or “elbow” in this plot where the rate of decrease levels off (which is also the number of clusters we’d choose), but that’s not quite the case here. It seems like there’s some slight elbow-ing at 5 clusters, so let’s just go ahead and choose that. Now we can look at the patterns of exchanges in each of these clusters.\n\noffice_clustered <- augment(clusters_fit$km_fit[[5]], data = office_exchanges)\n\nclusters_long <- office_clustered %>%\n  mutate(season = as_factor(season)) %>%\n  group_by(.cluster) %>%\n  summarize_if(is.numeric, mean, na.rm = TRUE) %>%\n  ungroup() %>%\n  pivot_longer(cols = -c(\".cluster\"),\n               names_to = \"chars\",\n               values_to = \"lines\")\n\nclusters_long %>%\n  ggplot(aes(x = lines, y = chars, fill = .cluster)) +\n    geom_col() +\n    facet_wrap(~.cluster, ncol = 2, scales = \"free_y\") +\n    #scale_y_reordered() +\n    scale_fill_ipsum() +\n    theme_minimal() +\n    labs(\n      title = \"Types of Office Episodes\"\n    ) +\n    theme(\n      legend.position = \"none\"\n    )\n\n\n\n\nSo, these plots show us the average number of exchanges between characters by cluster. Cluster 1 episodes seem to center around exchanges between Michael and Pam, and we also see a fair amount of exchanges between Michael & Jim, Michael & Dwight, and Jim & Pam. Cluster 2 episodes overwhelmingly feature interactions between Michael and Dwight. Cluster 3 episodes have relatively few exchanges between all of our main characters – this probably means that there’s a lot of side character action going on (recall that we didn’t include exchanges between anyone other than Michael, Dwight, Jim, Pam, and Andy in our clustering algorithm). Cluster 4 episodes have a lot of Michael and Andy interactions, along with a fair number of Michael-Dwight and Jim-Pam interactions. And Cluster 5 seems to be predominantly Michael and Jim, but also a fair amount of Michael-Dwight and Dwight-Jim, which makes sense. Usually when Jim talks to Michael in the show, Dwight finds a way to intrude.\nOne thing to remember is that these clusters aren’t necessarily balanced. As the table below shows, most episodes fit into Cluster 3.\n\noffice_clustered %>%\n  count(.cluster, name = \"num_episodes\") %>%\n  kable(format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"condensed\", \"striped\", \"hover\"))\n\n\n\n \n  \n    .cluster \n    num_episodes \n  \n \n\n  \n    1 \n    16 \n  \n  \n    2 \n    10 \n  \n  \n    3 \n    60 \n  \n  \n    4 \n    8 \n  \n  \n    5 \n    17 \n  \n\n\n\n\n\nAnother thing to keep in mind is that, across the all of the characters, Michael has far and away the most lines, so his interactions tend to drive this clustering. If we centered and scaled our variables, this would likely change, but we’d also lose some of the interpretability that comes with working in the raw metrics.\nFinally, let’s just choose a random episode from each cluster to see which episodes are falling into which categories.\n\noffice_clustered %>%\n  group_by(.cluster) %>%\n  sample_n(size = 1) %>%\n  select(.cluster, season, episode_name) %>%\n  kable(format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"striped\"))\n\n\n\n \n  \n    .cluster \n    season \n    episode_name \n  \n \n\n  \n    1 \n    3 \n    Women's Appreciation \n  \n  \n    2 \n    3 \n    The Coup \n  \n  \n    3 \n    3 \n    Diwali \n  \n  \n    4 \n    7 \n    Andy's Play \n  \n  \n    5 \n    3 \n    The Merger \n  \n\n\n\n\n\nThat’s all for now. I might do one more with some predictive modeling in the future."
  },
  {
    "objectID": "posts/scrantonicity-part-3/index.html",
    "href": "posts/scrantonicity-part-3/index.html",
    "title": "Scrantonicity - Part 3",
    "section": "",
    "text": "TL;DR:: In this blog, I use LASSO logistic regression and multilevel logistic regression to predict the speaker of lines of dialogue from The Office.\nWhat feels like forever ago, I wrote two blog posts analyzing transcripts from The Office. The first was a basic EDA of the dialogue, and the second used k-means clustering to determine types of Office episodes based on who speaks to whom. At the end of that second blog, I mentioned that I might do some predictive analysis with that data in the future. Well, it’s four months later, and I’m declaring that the future is now!\nBasically, the goal here is going to be, for a given line of dialogue from the show, to predict whether it’s Michael talking or someone else. At first blush, this seems like it shouldn’t be too hard. Many of Michael’s lines are iconic (e.g. see the above gif), but I feel like this might be more a function of the delivery than the actual words themselves, and I’m curious to see how well a model (or multiple models) could predict this just from the text.\nIn doing this, there are a couple of things I’m interested in doing here:\nAlso, before getting too much further, I want to acknowledge that I looked at this blog by Julia Silge and this blog by Emil Hvitfeldt for some background on {textrecipes}. Both are really great for people interested in text analysis.\nAnyway, without much further ado, let’s get into it. As has been the case in all of my “Scrantonicity” posts, the data I’m using here comes from the {schrute} package. First, I’ll load in libraries and set some defaults/options. I’m also going to read in the data, limiting the dialogue to the first seven seasons of the show (the Michael Scott era)."
  },
  {
    "objectID": "posts/scrantonicity-part-3/index.html#setup",
    "href": "posts/scrantonicity-part-3/index.html#setup",
    "title": "Scrantonicity - Part 3",
    "section": "Setup",
    "text": "Setup\n\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\nlibrary(tidyverse)\nlibrary(eemisc)\nlibrary(tidymodels)\nlibrary(schrute)\nlibrary(textrecipes)\nlibrary(themis)\nlibrary(vip)\nlibrary(glmmTMB)\nherm <- harrypotter::hp(n = 1, option = \"HermioneGranger\")\nopts <- options(\n  ggplot2.discrete.fill = list(\n    harrypotter::hp(n = 3, option = \"HermioneGranger\"),\n    harrypotter::hp(n = 5, option = \"Always\")\n  )\n)\ntheme_set(theme_ee())\noffice <- schrute::theoffice %>%\n  filter(as.numeric(season) <= 7) %>%\n  mutate(is_mike = if_else(character == \"Michael\", \"Yes\", \"No\"))"
  },
  {
    "objectID": "posts/scrantonicity-part-3/index.html#brief-eda-and-data-preprocessing",
    "href": "posts/scrantonicity-part-3/index.html#brief-eda-and-data-preprocessing",
    "title": "Scrantonicity - Part 3",
    "section": "Brief EDA and Data Preprocessing",
    "text": "Brief EDA and Data Preprocessing\nBefore modeling data, I would typically do a more thorough EDA. But I’ve already explored this data pretty closely (albeit months ago) in two previous blog posts, so rather than re-doing that EDA, I’m just going to look at those posts. One thing I will include here, though, is a quick look at the number of lines spoken by Michael Scott vs other characters, since this is the outcome I’m interested in predicting here.\n\noffice %>%\n  count(character) %>%\n  top_n(10) %>%\n  ggplot(aes(x = n, y = fct_reorder(character, n))) +\n  geom_col(fill = herm) +\n  labs(\n    title = \"Lines by Character\",\n    subtitle = \"First seven seasons\",\n    y = NULL,\n    x = \"Number of Lines\"\n  )\n\n\n\n\nSo, Michael has far and away the most lines of any character. But it’ll also be useful to look at Michael vs all of the others lumped together (since this is what I’m actually predicting).\n\noffice %>%\n  count(is_mike) %>%\n  ggplot(aes(x = n, y = fct_reorder(is_mike, n))) +\n  geom_col(fill = herm) +\n  labs(\n    title = \"Mike vs Not Mike\",\n    y = \"Is Michael?\",\n    x = \"Number of Lines\"\n  )\n\n\n\n\nEven though Michael speaks more than any other given character, he speaks about a third as many lines as all of the other characters combined. This is relevant here because it means I’ll want to downsample when I train my model to ensure the number of observations in each class are similar, which will help the model fit.\n\nData Splitting & Preprocessing\nNext, I’m going to split my data into a training a testing set.\n\nset.seed(0408)\noffice_split <- initial_split(office, strata = is_mike)\ntr <- training(office_split)\nte <- testing(office_split)\n\nNow that I’ve split my data, I’m going to preprocess the data using {recipes}, {textrecipes}, and {themis} (to handle class imbalance). One thing to clarify here: I’m building a model to predict whether the speaker of a given line of dialogue is Michael. In this analysis, I want to build this model using only the text data, although there are plenty of other text-based features I could include. More specifically, I am going to handle the preprocessing such that the model I end up fitting is a bag-of-words model. This means that I want my data to include a variable for each word* (not really each word, but I’ll show later) in the transcript, each row to represent a line of dialogue, and the value in each cell to represent the tf-idf of that word. From this data structure, I can build a model where each word has an individual effect on the odds that the line is spoken by Michael, although note that this model will have no sense of word order.\nI’ll specify this recipe and then walk through each step afterward.\n\noffice_recipe <- recipe(is_mike ~ text + episode_name, data = tr) %>%\n  themis::step_downsample(is_mike) %>%\n  step_tokenize(text) %>%\n  step_stopwords(text) %>%\n  step_tokenfilter(text, max_tokens = 200) %>%\n  step_tfidf(text) %>%\n  prep()\ntr_prepped <- juice(office_recipe)\ntr_prepped_noep <- tr_prepped %>%\n  select(-episode_name)\nte_prepped <- bake(office_recipe, te)\nte_prepped_noep <- te_prepped %>%\n  select(-episode_name)\n\nLet’s unpack this step-by-step:\n\nstep_downsample() will balance the data so that the number of cases where Michael is the speaker is equal to the number of cases where Michael is not the speaker. This is done by randomly dropping rows.\nstep_tokenize() will take the text column in the data and create a isolate each word per line.\nstep_stopwords() will remove stop words (e.g. “the”, “it”, “a”) that likely won’t contain much useful information.\nstep_tokenfilter(), as I’m using it here, will retain only the 200 most frequently used words. This is a pretty large number, but I’m going to fit a LASSO regression later, which can select out some of these if necessary.\nstep_tfidf() calculates the term frequency-inverse document frequency, which provides a metric for how important a word is to a given document (e.g. a line in this case).\n\nAnother thing to note here is that I’m creating two versions of this preprocessed data for the training and test sets. The differences between “tr_prepped” and “tr_prepped_noep” (as well as their “te” counterparts) is that the “noep” versions do not have a variable identifying which line the episode came from (but are otherwise identical). This is because I don’t want to include the episode identifier in my single-level LASSO model but do want to include it in the multilevel model. I could also accomplish this by specifying the formula and having it not include the episode_number variable rather than creating two datasets.\nMoving along! Next, I’m going to specify my model. Since I have a binary outcomes (yes/no if the speaker is Michael), I’m going to run a logistic regression. I’m going to run this as a LASSO model, which will provide some feature selection and generally shrink coefficients. I’m going to tune the model to choose the best amount of penalty as well.\n\nreg_spec <- logistic_reg(mixture = 1, penalty = tune()) %>%\n  set_engine(\"glmnet\")\nreg_spec\n\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\nHere, I’m creating some resamples of my training data to help with the tuning. I’m creating 10 bootstrap samples here.\n\nset.seed(0408)\nbooties <- bootstraps(tr_prepped_noep, strata = is_mike, times = 10)"
  },
  {
    "objectID": "posts/scrantonicity-part-3/index.html#lasso-model-fitting-examination",
    "href": "posts/scrantonicity-part-3/index.html#lasso-model-fitting-examination",
    "title": "Scrantonicity - Part 3",
    "section": "LASSO Model Fitting & Examination",
    "text": "LASSO Model Fitting & Examination\nNow it’s time to fit the LASSO model. I’m going to add the logistic regression specification that I just created to a workflow. Along with that model specification, I’m also going to add a formula where is_mike is regressed on all of the word features I just created. Then, I’m going to tune the model across 10 candidate values of the penalty parameter (i.e. how much regularization I’m adding).\n\noffice_wf <- workflow() %>%\n  add_model(reg_spec) %>%\n  add_formula(is_mike ~ .)\nset.seed(0408)\nlogreg_fit <- tune_grid(\n  office_wf,\n  resamples = booties,\n  grid = 10\n)\n\nGreat. Now that the models have been fit with various penalty values across the bootstrap resamples, I can check to see what the best penalty value is to move forward with & finalize a model. I’m going to choose the best by one standard error (which, in this case, happens also to be the best model). The one standard error rule will let me choose the most parsimonious model (in this case, the one with the most penalty) that is within one standard error of the best model. And once I choose the best penalty value, I’ll go ahead and finalize the model and refit on the training set.\n\nlogreg_fit %>%\n  show_best(\"accuracy\")\n\n# A tibble: 5 × 7\n   penalty .metric  .estimator  mean     n std_err .config              \n     <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n1 2.27e- 3 accuracy binary     0.577    10 0.00142 Preprocessor1_Model08\n2 1.02e-10 accuracy binary     0.576    10 0.00110 Preprocessor1_Model01\n3 1.27e- 9 accuracy binary     0.576    10 0.00110 Preprocessor1_Model02\n4 7.94e- 8 accuracy binary     0.576    10 0.00110 Preprocessor1_Model03\n5 4.46e- 7 accuracy binary     0.576    10 0.00110 Preprocessor1_Model04\n\nbest_params <- logreg_fit %>%\n  select_by_one_std_err(metric = \"accuracy\", desc(penalty))\nfinal_logreg <- office_wf %>%\n  finalize_workflow(best_params) %>%\n  fit(data = tr_prepped_noep)\n\nSo, the best model here has an accuracy of ~58%. Not great, but better than just straight-up guessing. Remember that this is on the training set. Now, I’ll take a look at what the accuracy is on the test set.\n\nbind_cols(\n  predict(final_logreg, te_prepped_noep), te_prepped_noep\n) %>%\n  accuracy(is_mike, .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.621\n\n\n61% – not bad! It’s actually better than the training set accuracy, which means our training process didn’t lead to overfitting, which is great.\nNow, I’m going to take a look at which words are the most important to predicting whether the speaker of a line of dialogue is Michael or not.\n\nfinal_logreg %>%\n  pull_workflow_fit() %>%\n  vi() %>%\n  slice_max(order_by = abs(Importance), n = 10) %>%\n  ggplot(aes(x = abs(Importance), y = fct_reorder(Variable %>% str_remove(\"tfidf_text_\"), abs(Importance)), fill = Sign)) +\n  geom_col() +\n  labs(\n    title = \"Most Important Words Identifying Michael Scott\",\n    subtitle = \"Positive values more representative of MS, negative values more representative of others\",\n    y = NULL\n  )\n\n\n\n\nNot surprisingly, the word “Michael” is the strongest predictor, and has a negative effect – if a line has the word “Michael” in it, it is less likely to be spoken by Michael. Intuitively, this makes sense. Other people use Michael’s name when speaking to or about him. The rest of the effects in this chart make sense to me as well (except for “mifflin” and “dunder,” which I don’t really get). But Michael is certainly more likely to talk about Jan and David than are other characters, and “everybody” feels right to me as well…\n\nAnd the final thing I’m going to do with this logistic regression is to pull out names of the non-zero coefficients. Recall that the lasso penalty can (but doesn’t always) shrink coefficients to zero. These variables will have no effect on the outcome. The reason I’m doing this is because I want to fit a multilevel model next, but I’m not going to regularize that model. Instead, I’ll just specify a formula that doesn’t include the variables that got shrunk to zero in this model.\n\nkeep_vars <- final_logreg %>%\n  pull_workflow_fit() %>%\n  vi() %>%\n  filter(Importance != 0) %>%\n  pull(Variable)"
  },
  {
    "objectID": "posts/scrantonicity-part-3/index.html#multilevel-model-fitting",
    "href": "posts/scrantonicity-part-3/index.html#multilevel-model-fitting",
    "title": "Scrantonicity - Part 3",
    "section": "Multilevel Model Fitting",
    "text": "Multilevel Model Fitting\nNow, I’m going to dive into fitting a multilevel model. To give a very brief overview of multilevel models, they are models that can take into account dependencies (nesting) within data. Recall that one of the assumptions of a linear regression is that each observation is independent. We often violate that assumption in the real world. In my work, for instance, students are often nested within classrooms (i.e. a common effect – their teacher – influences them & introduces a dependency). Another common case of nesting is when you have multiple observations over time from the same set of people. In the case of this current data, we can consider that each line is nested within an episode (terminology note: episode would be the “clustering variable” or “grouping variable” here). We could also go a step further and nest episodes within seasons to get a 3-level model rather than a 2-level model, but I’m not going to do that here.\nFitting multilevel models allows for random effects, where the coefficient of a given term differs based on the clustering variable. Any term in the model can have a random effect, but the simplest form of a multilevel model – and the one I’m going to fit here – is a random intercept model, where the value of the intercept changes depending on the clustering variable. In the current dataset, this would mean that Michael might be more (or less) likely to speak overall in a given episode (when compared to all other episodes), and so the intercept value will change to reflect that. It’s also possible to fit random slopes, where the effect of a given non-intercept term differs from episode to episode. Contextualizing that in the current data, it might mean that the word “Jan” is more (or less) associated with being spoken by Michael depending on the episode. Usually, you want a pretty clear theoretical rationale for specifying random slopes, and I don’t really have that here. Plus, it would be unreasonable to try to estimate random slopes for all of the words in the dataset (even though I only have a subset of ~190).\nIf you’re interested in learning more about multilevel models, Raudenbush & Bryk (2002) is a classic, and John Fox’s Applied Regression Analysis is just generally a really good book that has a chapter on MLMs.\nAnyway – onward and upward. First, I want to specify the formula of the model. I’m going to include all of the variables that had non-zero coefficients in the lasso model earlier, and I’m also going to add a term at the end to specify the random intercept for each episode – (1 | episode_name).\n\nglmm_formula <- as.formula(paste(\"is_mike ~ \", paste(keep_vars, collapse = \" + \"), \" + (1 | episode_name)\"))\n\nI’m going to fit this model using the {glmmTMB} package, which provides an interface for fitting all sort of generalized linear mixed models. I haven’t used this specific package before, but I have used {lme4}, which has similar syntax and is essentially the same thing for fitting linear models. I’m going to fit the model using the training data – note that I’m not tuning anything here – and I’m specifying the binomial family because this is a logistic regression.\n\nglmm_fit <- glmmTMB(glmm_formula, data = tr_prepped, family = binomial)\n\nI’m going to show the summary of the model here, but it’s going to be a biiig printout since we have so many terms in the model, so feel free to scroll on by. One thing you might want to check out, though, is the summary of the variance of the intercept, which summarizes the amount of randomness in that effect.\n\nsummary(glmm_fit)\n\n Family: binomial  ( logit )\nFormula:          \nis_mike ~ tfidf_text_michael + tfidf_text_everybody + tfidf_text_scott +  \n    tfidf_text_somebody + tfidf_text_ryan + tfidf_text_jan +  \n    tfidf_text_friend + tfidf_text_holly + tfidf_text_well +  \n    tfidf_text_life + tfidf_text_else + tfidf_text_god + tfidf_text_may +  \n    tfidf_text_going + tfidf_text_stanley + tfidf_text_someone +  \n    tfidf_text_head + tfidf_text_give + tfidf_text_coming + tfidf_text_room +  \n    tfidf_text_tonight + tfidf_text_alright + tfidf_text_fun +  \n    tfidf_text_know + tfidf_text_mifflin + tfidf_text_new + tfidf_text_name +  \n    tfidf_text_david + tfidf_text_okay + tfidf_text_today + tfidf_text_care +  \n    tfidf_text_stop + tfidf_text_next + tfidf_text_yes + tfidf_text_people +  \n    tfidf_text_ok + tfidf_text_angela + tfidf_text_night + tfidf_text_toby +  \n    tfidf_text_hold + tfidf_text_say + tfidf_text_business +  \n    tfidf_text_dwight + tfidf_text_person + tfidf_text_scranton +  \n    tfidf_text_dunder + tfidf_text_good + tfidf_text_mean + tfidf_text_probably +  \n    tfidf_text_go + tfidf_text_two + tfidf_text_sorry + tfidf_text_years +  \n    tfidf_text_day + tfidf_text_need + tfidf_text_many + tfidf_text_around +  \n    tfidf_text_check + tfidf_text_come + tfidf_text_meet + tfidf_text_um +  \n    tfidf_text_home + tfidf_text_pam + tfidf_text_everyone +  \n    tfidf_text_wow + tfidf_text_ever + tfidf_text_listen + tfidf_text_guess +  \n    tfidf_text_five + tfidf_text_place + tfidf_text_right + tfidf_text_little +  \n    tfidf_text_look + tfidf_text_real + tfidf_text_car + tfidf_text_oscar +  \n    tfidf_text_bad + tfidf_text_thing + tfidf_text_party + tfidf_text_please +  \n    tfidf_text_find + tfidf_text_boss + tfidf_text_work + tfidf_text_man +  \n    tfidf_text_idea + tfidf_text_take + tfidf_text_love + tfidf_text_want +  \n    tfidf_text_told + tfidf_text_thinking + tfidf_text_lot +  \n    tfidf_text_wanted + tfidf_text_old + tfidf_text_thanks +  \n    tfidf_text_kind + tfidf_text_paper + tfidf_text_great + tfidf_text_hear +  \n    tfidf_text_believe + tfidf_text_second + tfidf_text_fine +  \n    tfidf_text_big + tfidf_text_friends + tfidf_text_maybe +  \n    tfidf_text_said + tfidf_text_guy + tfidf_text_never + tfidf_text_wait +  \n    tfidf_text_thought + tfidf_text_call + tfidf_text_hi + tfidf_text_cause +  \n    tfidf_text_help + tfidf_text_even + tfidf_text_job + tfidf_text_sure +  \n    tfidf_text_together + tfidf_text_tell + tfidf_text_done +  \n    tfidf_text_hey + tfidf_text_phyllis + tfidf_text_us + tfidf_text_andy +  \n    tfidf_text_things + tfidf_text_long + tfidf_text_might +  \n    tfidf_text_first + tfidf_text_ah + tfidf_text_kevin + tfidf_text_three +  \n    tfidf_text_just + tfidf_text_cool + tfidf_text_last + tfidf_text_keep +  \n    tfidf_text_also + tfidf_text_trying + tfidf_text_try + tfidf_text_talk +  \n    tfidf_text_gonna + tfidf_text_jim + tfidf_text_much + tfidf_text_sales +  \n    tfidf_text_manager + tfidf_text_leave + tfidf_text_see +  \n    tfidf_text_always + tfidf_text_got + tfidf_text_baby + tfidf_text_hot +  \n    tfidf_text_time + tfidf_text_can + tfidf_text_guys + tfidf_text_pretty +  \n    tfidf_text_everything + tfidf_text_best + tfidf_text_get +  \n    tfidf_text_uh + tfidf_text_like + tfidf_text_every + tfidf_text_part +  \n    tfidf_text_money + tfidf_text_another + tfidf_text_saying +  \n    tfidf_text_yeah + tfidf_text_oh + tfidf_text_stuff + tfidf_text_getting +  \n    tfidf_text_hello + tfidf_text_hmm + tfidf_text_still + tfidf_text_office +  \n    tfidf_text_ask + tfidf_text_think + tfidf_text_show + tfidf_text_actually +  \n    tfidf_text_talking + tfidf_text_nothing + tfidf_text_wrong +  \n    tfidf_text_now + tfidf_text_happy + tfidf_text_let + tfidf_text_put +  \n    tfidf_text_company + tfidf_text_really + tfidf_text_way +  \n    tfidf_text_nice + tfidf_text_huh + tfidf_text_back + tfidf_text_thank +  \n    tfidf_text_anything + tfidf_text_went + tfidf_text_made +  \n    tfidf_text_feel + tfidf_text_one + tfidf_text_make + tfidf_text_year +  \n    (1 | episode_name)\nData: tr_prepped\n\n     AIC      BIC   logLik deviance df.resid \n 21757.9  23283.2 -10680.9  21361.9    16180 \n\nRandom effects:\n\nConditional model:\n Groups       Name        Variance Std.Dev.\n episode_name (Intercept) 0.2213   0.4704  \nNumber of obs: 16378, groups:  episode_name, 139\n\nConditional model:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)           -2.889e-01  5.458e-02  -5.293 1.21e-07 ***\ntfidf_text_michael    -1.483e+00  1.300e-01 -11.409  < 2e-16 ***\ntfidf_text_everybody   1.240e+00  2.027e-01   6.120 9.33e-10 ***\ntfidf_text_scott       1.259e+00  1.925e-01   6.540 6.17e-11 ***\ntfidf_text_somebody    8.438e-01  2.307e-01   3.657 0.000255 ***\ntfidf_text_ryan        6.094e-01  1.478e-01   4.122 3.76e-05 ***\ntfidf_text_jan         5.035e-01  1.248e-01   4.034 5.49e-05 ***\ntfidf_text_friend      4.708e-01  1.730e-01   2.722 0.006497 ** \ntfidf_text_holly       4.885e-01  1.468e-01   3.328 0.000876 ***\ntfidf_text_well        4.064e-01  6.064e-02   6.701 2.07e-11 ***\ntfidf_text_life        4.068e-01  1.488e-01   2.733 0.006282 ** \ntfidf_text_else        3.945e-01  1.599e-01   2.467 0.013623 *  \ntfidf_text_god         3.547e-01  8.441e-02   4.203 2.64e-05 ***\ntfidf_text_may         4.080e-01  1.590e-01   2.565 0.010305 *  \ntfidf_text_going       3.320e-01  6.886e-02   4.821 1.43e-06 ***\ntfidf_text_stanley     2.634e-01  9.954e-02   2.646 0.008146 ** \ntfidf_text_someone    -2.919e-01  1.470e-01  -1.985 0.047093 *  \ntfidf_text_head        2.986e-01  1.410e-01   2.118 0.034179 *  \ntfidf_text_give        3.001e-01  8.625e-02   3.479 0.000503 ***\ntfidf_text_coming     -3.234e-01  1.333e-01  -2.426 0.015261 *  \ntfidf_text_room        3.042e-01  1.262e-01   2.411 0.015915 *  \ntfidf_text_tonight     2.900e-01  1.322e-01   2.194 0.028230 *  \ntfidf_text_alright     2.677e-01  6.547e-02   4.089 4.34e-05 ***\ntfidf_text_fun         2.815e-01  1.220e-01   2.307 0.021050 *  \ntfidf_text_know        2.953e-01  5.634e-02   5.241 1.60e-07 ***\ntfidf_text_mifflin    -8.200e-01  1.023e+00  -0.802 0.422807    \ntfidf_text_new        -2.660e-01  1.190e-01  -2.235 0.025426 *  \ntfidf_text_name        2.824e-01  1.070e-01   2.640 0.008279 ** \ntfidf_text_david       2.304e-01  1.035e-01   2.225 0.026065 *  \ntfidf_text_okay        2.766e-01  4.434e-02   6.239 4.42e-10 ***\ntfidf_text_today       2.525e-01  1.223e-01   2.065 0.038902 *  \ntfidf_text_care       -2.315e-01  1.586e-01  -1.460 0.144292    \ntfidf_text_stop        2.599e-01  7.668e-02   3.389 0.000701 ***\ntfidf_text_next        2.418e-01  1.330e-01   1.818 0.069059 .  \ntfidf_text_yes         2.642e-01  4.061e-02   6.505 7.76e-11 ***\ntfidf_text_people      2.238e-01  1.104e-01   2.028 0.042552 *  \ntfidf_text_ok          2.195e-01  5.397e-02   4.067 4.77e-05 ***\ntfidf_text_angela     -2.588e-01  1.274e-01  -2.031 0.042283 *  \ntfidf_text_night      -2.296e-01  1.356e-01  -1.693 0.090481 .  \ntfidf_text_toby        2.277e-01  9.975e-02   2.283 0.022437 *  \ntfidf_text_hold        2.231e-01  1.443e-01   1.546 0.121996    \ntfidf_text_say         2.526e-01  7.512e-02   3.363 0.000771 ***\ntfidf_text_business    2.436e-01  1.083e-01   2.249 0.024530 *  \ntfidf_text_dwight      2.144e-01  5.973e-02   3.590 0.000331 ***\ntfidf_text_person      2.314e-01  1.676e-01   1.381 0.167432    \ntfidf_text_scranton    2.259e-01  1.218e-01   1.855 0.063645 .  \ntfidf_text_dunder      7.955e-01  1.027e+00   0.775 0.438353    \ntfidf_text_good        2.385e-01  5.450e-02   4.376 1.21e-05 ***\ntfidf_text_mean       -2.255e-01  8.503e-02  -2.652 0.008001 ** \ntfidf_text_probably   -2.104e-01  1.411e-01  -1.491 0.136073    \ntfidf_text_go          2.284e-01  6.273e-02   3.640 0.000272 ***\ntfidf_text_two        -1.793e-01  9.670e-02  -1.854 0.063733 .  \ntfidf_text_sorry      -1.875e-01  7.491e-02  -2.504 0.012290 *  \ntfidf_text_years       2.072e-01  1.186e-01   1.747 0.080612 .  \ntfidf_text_day         2.033e-01  1.093e-01   1.860 0.062880 .  \ntfidf_text_need        2.245e-01  8.925e-02   2.515 0.011896 *  \ntfidf_text_many        2.043e-01  1.386e-01   1.474 0.140397    \ntfidf_text_around      2.345e-01  1.501e-01   1.563 0.118148    \ntfidf_text_check       1.492e-01  1.175e-01   1.269 0.204298    \ntfidf_text_come        1.596e-01  6.523e-02   2.446 0.014430 *  \ntfidf_text_meet        1.980e-01  1.331e-01   1.488 0.136689    \ntfidf_text_um          1.270e-01  8.297e-02   1.531 0.125825    \ntfidf_text_home       -1.864e-01  1.550e-01  -1.203 0.229116    \ntfidf_text_pam         1.765e-01  6.416e-02   2.751 0.005935 ** \ntfidf_text_everyone   -1.795e-01  1.136e-01  -1.580 0.114018    \ntfidf_text_wow         1.913e-01  6.665e-02   2.869 0.004113 ** \ntfidf_text_ever        1.624e-01  9.321e-02   1.742 0.081538 .  \ntfidf_text_listen     -1.861e-01  1.455e-01  -1.279 0.200786    \ntfidf_text_guess      -1.441e-01  1.298e-01  -1.110 0.267065    \ntfidf_text_five       -1.533e-01  9.831e-02  -1.559 0.118905    \ntfidf_text_place       1.715e-01  1.220e-01   1.406 0.159750    \ntfidf_text_right       1.775e-01  5.076e-02   3.498 0.000470 ***\ntfidf_text_little      2.040e-01  1.018e-01   2.004 0.045104 *  \ntfidf_text_look        1.599e-01  7.754e-02   2.062 0.039254 *  \ntfidf_text_real       -1.534e-01  1.218e-01  -1.260 0.207683    \ntfidf_text_car         1.841e-01  1.090e-01   1.688 0.091374 .  \ntfidf_text_oscar       1.749e-01  1.150e-01   1.520 0.128407    \ntfidf_text_bad         1.396e-01  1.068e-01   1.308 0.190988    \ntfidf_text_thing       1.652e-01  1.054e-01   1.567 0.117113    \ntfidf_text_party       1.370e-01  8.894e-02   1.540 0.123461    \ntfidf_text_please      1.279e-01  7.363e-02   1.738 0.082259 .  \ntfidf_text_find        2.275e-01  1.144e-01   1.989 0.046664 *  \ntfidf_text_boss        1.655e-01  1.197e-01   1.383 0.166706    \ntfidf_text_work        1.457e-01  9.918e-02   1.469 0.141698    \ntfidf_text_man        -1.355e-01  8.999e-02  -1.506 0.132116    \ntfidf_text_idea        1.382e-01  1.040e-01   1.328 0.184013    \ntfidf_text_take        1.146e-01  8.350e-02   1.373 0.169791    \ntfidf_text_love        1.539e-01  7.600e-02   2.025 0.042866 *  \ntfidf_text_want        1.237e-01  6.791e-02   1.821 0.068589 .  \ntfidf_text_told        1.135e-01  9.761e-02   1.163 0.244858    \ntfidf_text_thinking    1.138e-01  1.120e-01   1.016 0.309807    \ntfidf_text_lot         1.385e-01  1.081e-01   1.282 0.199966    \ntfidf_text_wanted      9.466e-02  1.288e-01   0.735 0.462497    \ntfidf_text_old         9.108e-02  1.275e-01   0.714 0.475033    \ntfidf_text_thanks     -1.185e-01  7.088e-02  -1.672 0.094555 .  \ntfidf_text_kind        1.104e-01  1.123e-01   0.983 0.325461    \ntfidf_text_paper      -8.431e-02  1.160e-01  -0.727 0.467431    \ntfidf_text_great      -9.186e-02  6.868e-02  -1.337 0.181062    \ntfidf_text_hear        9.896e-02  8.917e-02   1.110 0.267106    \ntfidf_text_believe     1.779e-01  1.088e-01   1.635 0.101979    \ntfidf_text_second      1.084e-01  1.528e-01   0.709 0.478112    \ntfidf_text_fine       -8.491e-02  8.343e-02  -1.018 0.308774    \ntfidf_text_big         1.175e-01  9.609e-02   1.223 0.221363    \ntfidf_text_friends     6.781e-02  1.371e-01   0.495 0.620913    \ntfidf_text_maybe       8.042e-02  8.685e-02   0.926 0.354501    \ntfidf_text_said        1.035e-01  6.781e-02   1.526 0.127028    \ntfidf_text_guy         1.553e-01  9.269e-02   1.675 0.093841 .  \ntfidf_text_never       1.108e-01  9.056e-02   1.223 0.221293    \ntfidf_text_wait       -9.395e-02  8.804e-02  -1.067 0.285875    \ntfidf_text_thought    -1.016e-01  9.995e-02  -1.017 0.309199    \ntfidf_text_call        9.420e-02  8.598e-02   1.096 0.273236    \ntfidf_text_hi         -7.907e-02  6.464e-02  -1.223 0.221225    \ntfidf_text_cause      -7.056e-02  1.492e-01  -0.473 0.636364    \ntfidf_text_help        8.608e-02  1.145e-01   0.752 0.452167    \ntfidf_text_even       -8.496e-02  1.044e-01  -0.814 0.415886    \ntfidf_text_job         1.357e-01  1.233e-01   1.101 0.270822    \ntfidf_text_sure       -8.151e-02  7.593e-02  -1.074 0.283047    \ntfidf_text_together    1.705e-01  1.657e-01   1.029 0.303552    \ntfidf_text_tell        1.082e-01  7.872e-02   1.375 0.169152    \ntfidf_text_done        1.013e-01  9.403e-02   1.077 0.281267    \ntfidf_text_hey         8.886e-02  4.457e-02   1.994 0.046205 *  \ntfidf_text_phyllis     7.437e-02  9.666e-02   0.769 0.441651    \ntfidf_text_us         -6.456e-02  9.928e-02  -0.650 0.515470    \ntfidf_text_andy       -4.710e-02  8.261e-02  -0.570 0.568547    \ntfidf_text_things     -7.999e-02  1.342e-01  -0.596 0.551297    \ntfidf_text_long       -8.711e-02  1.348e-01  -0.646 0.518304    \ntfidf_text_might       8.425e-02  1.286e-01   0.655 0.512557    \ntfidf_text_first       6.138e-02  1.181e-01   0.520 0.603095    \ntfidf_text_ah          7.144e-02  7.023e-02   1.017 0.309049    \ntfidf_text_kevin      -6.116e-02  8.453e-02  -0.724 0.469369    \ntfidf_text_three      -6.102e-02  1.118e-01  -0.546 0.585055    \ntfidf_text_just        5.903e-02  6.580e-02   0.897 0.369634    \ntfidf_text_cool       -8.012e-02  7.629e-02  -1.050 0.293664    \ntfidf_text_last       -7.431e-02  1.338e-01  -0.555 0.578627    \ntfidf_text_keep       -8.455e-02  1.023e-01  -0.827 0.408512    \ntfidf_text_also       -5.022e-02  1.437e-01  -0.349 0.726735    \ntfidf_text_trying      8.787e-02  1.189e-01   0.739 0.459783    \ntfidf_text_try         7.888e-02  1.134e-01   0.695 0.486796    \ntfidf_text_talk        7.701e-02  9.262e-02   0.831 0.405728    \ntfidf_text_gonna      -7.706e-02  7.991e-02  -0.964 0.334874    \ntfidf_text_jim        -5.136e-02  6.463e-02  -0.795 0.426841    \ntfidf_text_much        7.526e-02  9.277e-02   0.811 0.417217    \ntfidf_text_sales      -5.136e-02  1.301e-01  -0.395 0.693031    \ntfidf_text_manager     5.981e-04  1.126e-01   0.005 0.995762    \ntfidf_text_leave       6.724e-02  1.183e-01   0.568 0.569758    \ntfidf_text_see         5.790e-02  7.216e-02   0.802 0.422343    \ntfidf_text_always     -6.619e-02  9.800e-02  -0.675 0.499401    \ntfidf_text_got        -5.335e-02  6.489e-02  -0.822 0.410947    \ntfidf_text_baby       -4.943e-02  1.003e-01  -0.493 0.622171    \ntfidf_text_hot        -6.815e-02  1.006e-01  -0.677 0.498169    \ntfidf_text_time        6.449e-02  8.634e-02   0.747 0.455151    \ntfidf_text_can         7.058e-02  6.784e-02   1.040 0.298157    \ntfidf_text_guys        7.052e-02  7.839e-02   0.900 0.368365    \ntfidf_text_pretty     -3.402e-02  1.152e-01  -0.295 0.767849    \ntfidf_text_everything -2.060e-02  1.160e-01  -0.178 0.859084    \ntfidf_text_best        5.798e-02  1.247e-01   0.465 0.641940    \ntfidf_text_get         5.734e-02  6.260e-02   0.916 0.359696    \ntfidf_text_uh         -6.083e-02  6.326e-02  -0.962 0.336269    \ntfidf_text_like        4.704e-02  5.972e-02   0.788 0.430918    \ntfidf_text_every      -6.714e-02  1.454e-01  -0.462 0.644178    \ntfidf_text_part       -7.239e-02  1.038e-01  -0.698 0.485382    \ntfidf_text_money       2.986e-02  1.400e-01   0.213 0.831097    \ntfidf_text_another    -3.247e-02  1.464e-01  -0.222 0.824418    \ntfidf_text_saying      3.430e-02  1.208e-01   0.284 0.776385    \ntfidf_text_yeah       -4.495e-02  3.926e-02  -1.145 0.252269    \ntfidf_text_oh          4.034e-02  4.628e-02   0.872 0.383401    \ntfidf_text_stuff       2.432e-02  1.334e-01   0.182 0.855384    \ntfidf_text_getting    -4.037e-05  1.014e-01   0.000 0.999682    \ntfidf_text_hello       5.223e-02  5.796e-02   0.901 0.367535    \ntfidf_text_hmm         3.320e-02  5.309e-02   0.625 0.531687    \ntfidf_text_still       5.201e-02  8.753e-02   0.594 0.552374    \ntfidf_text_office      3.081e-02  9.246e-02   0.333 0.738953    \ntfidf_text_ask         3.300e-02  1.333e-01   0.248 0.804461    \ntfidf_text_think       5.221e-02  6.263e-02   0.834 0.404518    \ntfidf_text_show        1.016e-01  1.276e-01   0.796 0.426087    \ntfidf_text_actually    2.435e-02  1.048e-01   0.232 0.816185    \ntfidf_text_talking     2.833e-02  8.161e-02   0.347 0.728490    \ntfidf_text_nothing     3.542e-02  8.566e-02   0.414 0.679223    \ntfidf_text_wrong       7.353e-02  9.722e-02   0.756 0.449444    \ntfidf_text_now         4.111e-02  7.994e-02   0.514 0.607014    \ntfidf_text_happy       4.873e-02  8.697e-02   0.560 0.575216    \ntfidf_text_let        -3.145e-02  1.029e-01  -0.306 0.759908    \ntfidf_text_put         1.453e-02  9.001e-02   0.161 0.871714    \ntfidf_text_company     4.967e-02  1.342e-01   0.370 0.711297    \ntfidf_text_really     -9.223e-03  5.789e-02  -0.159 0.873426    \ntfidf_text_way         9.263e-04  8.786e-02   0.011 0.991588    \ntfidf_text_nice        3.397e-02  7.138e-02   0.476 0.634094    \ntfidf_text_huh         1.383e-02  8.952e-02   0.154 0.877239    \ntfidf_text_back        3.227e-02  8.637e-02   0.374 0.708690    \ntfidf_text_thank      -1.892e-03  5.140e-02  -0.037 0.970643    \ntfidf_text_anything    3.215e-02  1.128e-01   0.285 0.775641    \ntfidf_text_went        2.915e-02  1.248e-01   0.234 0.815323    \ntfidf_text_made       -9.107e-03  1.042e-01  -0.087 0.930359    \ntfidf_text_feel        4.747e-03  1.192e-01   0.040 0.968235    \ntfidf_text_one         1.089e-02  7.353e-02   0.148 0.882311    \ntfidf_text_make        2.607e-02  9.071e-02   0.287 0.773847    \ntfidf_text_year       -1.681e-02  1.234e-01  -0.136 0.891641    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nRight, so, the next logical step in my mind is to take a closer look at the random intercepts. We see some variance in the intercept (.23), which suggests that there are meaningful between-episode differences in the number of times Michael Scott speaks. Rather than looking at all of these, let’s take a look at the largest 10 effects (as a benchmark, recall that the mean intercept is -.3)\n\nranef(glmm_fit) %>%\n  as.data.frame() %>%\n  select(grp, condval) %>%\n  slice_max(order_by = abs(condval), n = 10) %>%\n  ggplot(aes(x = abs(condval), y = fct_reorder(grp, abs(condval)), fill = if_else(condval > 0, \"Pos\", \"Neg\"))) +\n  geom_col() +\n  scale_fill_discrete(name = \"Sign\") +\n  labs(\n    y = NULL,\n    title = \"Top Random Intercepts\"\n  )\n\n\n\n\nThis plot shows the largest (in absolute value) intercepts. The way to interpret this is that, in these episodes, Michael is more or less likely to speak. The effects of each of the words remains the same across episodes (since I didn’t specify random slopes), but these change the assumed “base rate” that Michael speaks. What we see here makes sense, because Michael actually isn’t in the three episodes that have the highest values here (I should have addressed this in data cleaning – whoops!).\nFinally, I’ll take a look at the accuracy of the predictions from the multilevel model.\n\nglmm_preds_response <- predict(glmm_fit, te_prepped, type = \"response\")\nglmm_preds <- ifelse(glmm_preds_response < .5, \"No\", \"Yes\") %>% as_factor() %>%\n  fct_relevel(\"No\", \"Yes\")\nbind_cols(te_prepped$is_mike, glmm_preds) %>%\n  repair_names() %>%\n  accuracy(truth = ...1, estimate = ...2)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.601\n\n\nIt’s a little bit disappointing that the multilevel model isn’t more accurate than the single-level model I ran previously, but one thing to keep in mind is that the single level model was regularized, whereas the multilevel model wasn’t (beyond omitting the variables that got completely omitted from the single level model). So, even though our intercept seems to have a decent amount of variance – meaning random effects are probably warranted – the gains in predictive accuracy we’d get from that are more than offset by the regularization in the first model. There’s probably a way to regularize a multilevel model, but I might save that one for another day. I could also play around with changing the probability threshold for classifying a line as Michael by setting it to something higher than 50% (e.g. a line needs to have a 70% probability before being classified as spoken by Michael), but I’m also not going to go down that rabbit hole here.\nSo, I’m going to wrap it up for now. And who knows, maybe I’ll revisit this dataset in another 4 months."
  },
  {
    "objectID": "posts/simulating-triangles/index.html",
    "href": "posts/simulating-triangles/index.html",
    "title": "Simulating Triangles?",
    "section": "",
    "text": "I saw this question on Reddit last night, and it seemed interesting to me & like a pretty quick blog post (which feels like all I have time for lately), so I wanted to run with it.\nThe question is: Three numbers between 0 and 1 are randomly chosen. What is the probability that the 3 numbers are the lengths of the sides of a triangle?\nThere’s probably a mathematical way to answer this question, but I’m going to approach it using simulation. First, let’s load packages and do some setup (note that we can do all of this first part in base R, but I’m loading some packages to help out with the bonus section at the end)."
  },
  {
    "objectID": "posts/simulating-triangles/index.html#defining-the-problem",
    "href": "posts/simulating-triangles/index.html#defining-the-problem",
    "title": "Simulating Triangles?",
    "section": "Defining the Problem",
    "text": "Defining the Problem\nOur problem is fairly straightforward. My approach is to think about it like this: - If we choose 3 random numbers between 0 and 1, could they correspond to the lengths of a triangle? - If we repeated the previous step thousands (or millions) of times, what percent of the trials would form triangles. This will approximate the actual probability.\nThe other piece of information that’s relevant here is the requirement that, to form a triangle, the sum of the lengths of the two shorter segments must be longer than the length of the longest segment. Which makes sense intuitively if you think about what a triangle actually looks like."
  },
  {
    "objectID": "posts/simulating-triangles/index.html#simulating-evaluating-a-single-case",
    "href": "posts/simulating-triangles/index.html#simulating-evaluating-a-single-case",
    "title": "Simulating Triangles?",
    "section": "Simulating & Evaluating a Single Case",
    "text": "Simulating & Evaluating a Single Case\nSimulating a single trial is very straightforward and only requires the runif() function, which randomly samples from a uniform distribution (note that I used to think this function meant “run if…” and was very confused by it). We can do this as follows:\n\nset.seed(0408)\nx <- runif(n = 3, min = 0, max = 1)\nx\n\n[1] 0.009171368 0.084638904 0.618047172\n\n\nNow that we have these random numbers, the task is to figure out whether they can serve as the lengths of a triangle. Thinking back to our previous rule, we need the sum of the two shorter numbers to be greater than the largest number. We can evaluate this by:\n\nmin(x) + median(x) > max(x)\n\n[1] FALSE\n\n\nFor our first example, we see that this is false"
  },
  {
    "objectID": "posts/simulating-triangles/index.html#simulating-evaluating-many-cases",
    "href": "posts/simulating-triangles/index.html#simulating-evaluating-many-cases",
    "title": "Simulating Triangles?",
    "section": "Simulating & Evaluating Many Cases",
    "text": "Simulating & Evaluating Many Cases\nOk, great. Now that we’ve managed to do this once, we just need to repeat the process several times. After evaluating many repeated trials, we’ll have our (approximate) probability. Here, I’m going to write a function to do simulate some numbers and then evaluate whether they could be the lengths of the sides of a triangle.\n\nsim_func <- function() {\n  x <- runif(n = 3, min = 0, max = 1)\n  \n  min(x) + median(x) > max(x)\n}\nsim_func()\n\n[1] TRUE\n\n\nAnd then the next step is to run that function several times (I’m going to go with 100,000) and then calculate the % that evaluate to TRUE.\n\ncalc_prob <- function(sims) {\n  tmp <- replicate(sims, sim_func())\n  \n  sum(tmp)/sims\n}\nset.seed(0408)\ncalc_prob(1e5)\n\n[1] 0.50059\n\n\nWe arrive at an answer of about 50%."
  },
  {
    "objectID": "posts/simulating-triangles/index.html#bonus-calculating-triangle-angles",
    "href": "posts/simulating-triangles/index.html#bonus-calculating-triangle-angles",
    "title": "Simulating Triangles?",
    "section": "Bonus – Calculating Triangle Angles",
    "text": "Bonus – Calculating Triangle Angles\nNow that we’ve determined how to simulate a bunch of triangles by evaluating the lengths of their sides, we can go a step further and calculate all the interior angles of these triangles. To do this, I’ll need to tweak my earlier function a little bit – rather than returning TRUE/FALSE, I need it to now return each of the sides. After simulating 10,000 (potential) triangles, I’ll filter for just those that actually can be triangles.\n\nsim_sides <- function() {\n  x <- runif(n = 3)\n  \n  tmp <- tibble(a = min(x), b = median(x), c = max(x))\n  \n}\nset.seed(0408)\nsimmed_triangles <- replicate(10000, sim_sides(), simplify = FALSE) %>%\n  bind_rows() %>%\n  filter(a + b > c)\n\nThe next step is to throw it back to high school geometry and the law of cosines. Using the formula below, we can calculate any angle of a triangle if we know the lengths of the three sides:\n\\(\\gamma = cos^{-1}(\\frac{a^2 + b^2 - c^2}{2ab})\\)\nwhere \\(\\gamma\\) is the angle we’re solving for, a and b are adjacent angles, and c is the angle opposite \\(\\gamma\\). One other thing to keep in mind is that R will calculate these angles in radians, so I’ll need to convert them to degrees (well, I don’t need to, but I’d prefer to).\n\ncalc_angle <- function(opp, adj1, adj2) {\n  180*(acos((adj1^2 + adj2^2 - opp^2)/(2*adj1*adj2)))/pi\n}\nsimmed_angles <- simmed_triangles %>%\n  mutate(ang_a = calc_angle(a, b, c),\n         ang_b = calc_angle(b, a, c),\n         ang_c = calc_angle(c, a, b),\n         id = row_number())\n\nI’m going to check to make sure that the sum of the angles equals 180 (which much be true for triangles).\n\nsimmed_angles %>%\n  mutate(is_180 = between((ang_a + ang_b + ang_c), 179.9, 180.1)) %>%\n  distinct(is_180)\n\n# A tibble: 1 × 1\n  is_180\n  <lgl> \n1 TRUE  \n\n\nCool, so the sum of all of the angles is 180.\nAngle C should always be the largest angle, which means it must be >= 60. Let’s do a quick check on that here.\n\nrange(simmed_angles$ang_c)\n\n[1]  60.36463 179.27215\n\n\nGreat. And let’s finally plot the distribution of these angles\n\nsimmed_angles %>%\n  select(-c(\"a\", \"b\", \"c\")) %>%\n  pivot_longer(cols = -id,\n               names_to = \"nms\",\n               values_to = \"value\") %>%\n  ggplot(aes(y = nms, x = value)) +\n  geom_density_ridges(aes(fill = nms), alpha = .8, stat = \"binline\") +\n  scale_y_discrete(labels = c(\"Angle A\", \"Angle B\", \"Angle C\")) +\n  labs(\n    x = \"Angle (in degrees)\",\n    y = NULL,\n    title = \"Distribution of Angles\"\n  ) +\n  theme(\n    legend.position = \"none\",\n  )\n\n\n\n\nGreat, so we learned how to determine whether 3 random numbers could form the sides of a triangle and how to calculate the angles for those that did form a triangle."
  },
  {
    "objectID": "posts/this-code-is-bigtime/index.html",
    "href": "posts/this-code-is-bigtime/index.html",
    "title": "This Code is Big Time",
    "section": "",
    "text": "big_time(), which will play the “THIS SHIT IS BIG TIME” drop from the YMH intro, and\nbig_time_operator(), which lets you wrap a function and play the “THIS SHIT IS BIG TIME” drop whenever you call that function.\n\nI’ll demonstrate these below.\n\nSetup\nTo use these functions, you’ll need my personal package, {eemisc}, installed. You can install it via Github via the following command:\n\nremotes::install_github(\"ekholme/eemisc\")\n\nEventually, I might put these functions into their own package, but for now they live in my odds-and-ends package.\n\n\nBig Time\nTo play the “THIS SHIT IS BIG TIME” drop, you just need to call the function big_time():\n\nlibrary(eemisc)\n\nbig_time()\n\nThat’s it. That will play the drop.\n\n\nBig Time Operator\nBut I took it a step further for all of my kings and queens above 18. The big_time_operator() takes a function of your choosing and produces as its output a new function that wraps the input function with the “big time” drop. Want to let everyone know that taking the mean is big time?\n\nbig_time_mean <- big_time_operator(mean)\n\nx <- 1:10\n\nbig_time_mean(x)\n\n[1] 5.5\n\n\nYou can pass any function you want into big_time_operator() to get the same effect.\nAnyway, that’s all for now. Keep it high and tight, Jeans, and you bet I’m coming up in May.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-nc/4.0/CitationBibTeX citation:@online{ekholm2021,\n  author = {Eric Ekholm},\n  title = {This {Code} Is {Big} {Time}},\n  date = {2021-04-25},\n  url = {https://www.ericekholm.com/posts/this-code-is-bigtime},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEric Ekholm. 2021. “This Code Is Big Time.” April 25, 2021.\nhttps://www.ericekholm.com/posts/this-code-is-bigtime."
  },
  {
    "objectID": "posts/unconsciousness-in-the-xmen/index.html",
    "href": "posts/unconsciousness-in-the-xmen/index.html",
    "title": "Unconsciousness in the Xmen",
    "section": "",
    "text": "A part of me has always wanted to get into comic books. I think it would be a really good fit for me – I’m definitely a nerd. I play video games, I read fantasy novels, I code/do data science for fun. Comic books should be right up my alley. But for whatever reason, I’ve never taken the plunge. Maybe it’s a time commitment thing. Maybe I know I’ll like them too much. Maybe it’s too daunting to figure out how to start. Regardless, even thought I’m not into comic books, they are intriguing to me, and the X-Men particularly so, which is why I wanted to take a little bit of time to analyze this X-men data promoted by the #tidytuesday project.\nThe other main purpose of this blog post is to toy around with running a Poisson regression. A few months ago, I saw a post about how the tidymodels framework had some new “parsnip-adjacent” packages, with one being {poissonreg} which fits – you guessed it – Poisson regressions. I haven’t had much reason to use Poisson regression in any of my previous work or in datasets I’ve toyed around with, but this X-men dataset seems like a good excuse to try it out. So, onward and upward!"
  },
  {
    "objectID": "posts/unconsciousness-in-the-xmen/index.html#setup",
    "href": "posts/unconsciousness-in-the-xmen/index.html#setup",
    "title": "Unconsciousness in the Xmen",
    "section": "Setup",
    "text": "Setup\nFirst, I’ll load some packages, set some miscellaneous options, and import the data. This data comes from the Claremont Run project, which mines data from Chris Claremont’s run (1975-1991) writing the X-men comics. To learn more about the project, you can visit the website. There are several datasets available, but for this analysis, I’m going to use data from the characters dataset, the character_visualization dataset, and the locations dataset.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(hrbrthemes)\nlibrary(gameofthrones)\nlibrary(vip)\n\ntheme_set(theme_ipsum())\n\nlann <- got(1, option = \"Lannister\")\n\ncharacters <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-06-30/characters.csv')\nlocations <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-06-30/locations.csv')\ncharacter_visualization <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-06-30/character_visualization.csv')"
  },
  {
    "objectID": "posts/unconsciousness-in-the-xmen/index.html#exploring-the-data",
    "href": "posts/unconsciousness-in-the-xmen/index.html#exploring-the-data",
    "title": "Unconsciousness in the Xmen",
    "section": "Exploring the Data",
    "text": "Exploring the Data\nLet’s first look at the characters dataset. In this dataset, each row corresponds to a character in an issue, and each column corresponds to actions or events relevant to that character. Here’s a glimpse of that data:\n\ncharacters %>%\n  glimpse()\n\nRows: 4,209\nColumns: 34\n$ issue                                         <dbl> 97, 97, 97, 97, 97, 97, …\n$ character                                     <chr> \"Professor X\", \"Wolverin…\n$ rendered_unconcious                           <dbl> 0, 0, 0, 1, 0, 0, 0, 0, …\n$ captured                                      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ declared_dead                                 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ redressed                                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ depowered                                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ clothing_torn                                 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ subject_to_torture                            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ quits_team                                    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ surrenders                                    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ number_of_kills_humans                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ number_of_kills_non_humans                    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ initiates_physical_conflict                   <chr> NA, NA, \"1\", NA, NA, NA,…\n$ expresses_reluctance_to_fight                 <dbl> NA, NA, 1, NA, NA, NA, N…\n$ on_a_date_with_which_character                <chr> NA, NA, NA, NA, NA, NA, …\n$ kiss_with_which_character                     <chr> NA, NA, NA, NA, NA, NA, …\n$ hand_holding_with_which_character             <chr> \"Moira MacTaggert\", NA, …\n$ dancing_with_which_character                  <chr> NA, NA, NA, NA, NA, NA, …\n$ flying_with_another_character                 <chr> NA, NA, NA, \"Storm\", \"Je…\n$ arm_in_arm_with_which_character               <chr> NA, NA, NA, NA, NA, NA, …\n$ hugging_with_which_character                  <chr> NA, NA, NA, NA, NA, NA, …\n$ physical_contact_other                        <chr> \"Moira MacTaggert\", \"Sto…\n$ carrying_with_which_character                 <chr> NA, NA, NA, NA, NA, NA, …\n$ shared_bed_with_which_character               <lgl> NA, NA, NA, NA, NA, NA, …\n$ shared_room_domestically_with_which_character <lgl> NA, NA, NA, NA, NA, NA, …\n$ explicitly_states_i_love_you_to_whom          <chr> NA, NA, NA, NA, NA, NA, …\n$ shared_undress                                <chr> NA, NA, NA, NA, NA, NA, …\n$ shower_number_of_panels_shower_lasts          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ bath_number_of_panels_bath_lasts              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ depicted_eating_food                          <dbl> 1, 0, 0, 0, 0, 0, 0, 0, …\n$ visible_tears_number_of_panels                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ visible_tears_number_of_intances              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, …\n$ special_notes                                 <chr> NA, NA, NA, NA, NA, NA, …\n\n\nSo, we can see in this dataset things like who Professor X held hands with in issue 97, how many humans were killed by Magneto in issue 105, etc. We see lots of NAs and 0s in this dataset. The only column I’m going to use from this is the rendered unconscious column, which will be outcome variable in the models later.\nIn the character_visualization dataset, each row represents a per-issue count of the number of times a character is depicted, speaks, thinks, has a narrative statement (I think this is probably only relevant for the narrator character?), either when the character is in costume or not in costume.\n\ncharacter_visualization %>%\n  glimpse()\n\nRows: 9,800\nColumns: 7\n$ issue     <dbl> 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, …\n$ costume   <chr> \"Costume\", \"Costume\", \"Costume\", \"Costume\", \"Costume\", \"Cost…\n$ character <chr> \"Editor narration\", \"Omnipresent narration\", \"Professor X = …\n$ speech    <dbl> 0, 0, 0, 7, 24, 0, 11, 9, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ thought   <dbl> 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ narrative <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ depicted  <dbl> 0, 0, 0, 10, 23, 0, 9, 17, 17, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\nIn the location dataset, each row corresponds to a location in which part of the issue takes place, with as many locations listed per issue as appear in that issue. The dataset also includes a “context” column that describes things like whether the location is shown in the present, as part of a flashback, in a dream, etc. Here’s a glimpse:\n\nlocations %>%\n  glimpse()\n\nRows: 1,413\nColumns: 4\n$ issue    <dbl> 97, 97, 97, 97, 97, 98, 98, 98, 98, 98, 99, 99, 99, 99, 99, 9…\n$ location <chr> \"Space\", \"X-Mansion\", \"Rio Diablo Research Facility\", \"Kenned…\n$ context  <chr> \"Dream\", \"Present\", \"Present\", \"Present\", \"Present\", \"Present…\n$ notes    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Cuts back and fo…\n\n\nAcross these datasets, it probably makes the most sense to aggegrate data up to the issue level, since that’s kind of the lowest common denominator here. So, essentially the question I’m going to try to answer in this blog post is:\nWhat features of an X-men issue predict how many characters are rendered unconscious in that issue?\nFirst, let’s look at the distribution of rendered unconscious:\n\ncharacters %>%\n  count(issue, wt = rendered_unconcious, sort = TRUE) %>%\n  ggplot(aes(x = n)) +\n  geom_histogram(fill = lann, bins = 8)\n\n\n\n\nRight, so, this is a pretty strongly right-skewed distribution, which is sort of what we’d expect from a Poisson distribution, especially one with a low expected number of events (which I’d imagine is the case in comic books)."
  },
  {
    "objectID": "posts/unconsciousness-in-the-xmen/index.html#cleaning-aggregating-and-joining",
    "href": "posts/unconsciousness-in-the-xmen/index.html#cleaning-aggregating-and-joining",
    "title": "Unconsciousness in the Xmen",
    "section": "Cleaning, Aggregating, and Joining",
    "text": "Cleaning, Aggregating, and Joining\nNext, let’s aggregate our data up to the issue level. This will give us data where a row represents an issue rather than a character within an issue or a location within an issue. We’ll start with the characters dataset. There’s a lot we could do with this data, but because there are only 183 issues represented in this dataset, we need to be cognizant about how many predictors we’re including. So the only variable I’m going to use here is rendered unconscious as the outcome, which will represent the number of characters rendered unconscious in a given issue.\n\nrend_df <- characters %>%\n  group_by(issue) %>%\n  summarize(rendered_unconscious = sum(rendered_unconcious, na.rm = FALSE))\n\nNext, let’s work on the character_visualization dataset. Again, trying to keep the number of predictors relatively small, I’m going to winnow this down to represent counts of how many times a handful of key characters are depicted in each issue. I don’t know a ton about the X-men, but I know who some of the more important characters are, so I’m going to choose Wolverine, Professor X, Magneto, and Jean Grey here.\n\nchar_sum <- character_visualization %>%\n  filter(str_detect(character, \"Wolverine|Xavier|Jean Grey|Magneto\")) %>%\n  group_by(issue, character) %>%\n  summarize(depict = sum(depicted, na.rm = FALSE)) %>%\n  mutate(character = case_when(\n    str_detect(character, \"Jean Grey\") ~ \"Jean_Grey\",\n    str_detect(character, \"Wolv\") ~ \"Wolverine\",\n    str_detect(character, \"Magneto\") ~ \"Magneto\",\n    str_detect(character, \"Xavier\") ~ \"Professor_X\"\n  )) %>%\n  pivot_wider(\n    names_from = character,\n    values_from = depict\n  )\n\nNext, let’s work on our locations dataset. First, let’s look at the most common locations. Again, since we only have 183 rows in our dataset that we’re modeling with, I only want to choose a handful of variables to include in the model here.\n\nlocations %>%\n  count(location, sort = TRUE)\n\n# A tibble: 785 × 2\n   location                             n\n   <chr>                            <int>\n 1 X-Mansion                          100\n 2 Danger Room                         27\n 3 Space                               19\n 4 Muir Island, Scotland               14\n 5 Unspecified region in Australia     14\n 6 Eagle Plaza, Dallas Texas           11\n 7 Central Park                        10\n 8 Morlock residence under New York    10\n 9 Princess Lilandra's Home Planet     10\n10 San Francisco                       10\n# … with 775 more rows\n\n\nOk, so, I’m just going to go with the 3 most common locations: the X-mansion, the Danger Room (whatever that is), and Space. Danger Room sounds to me like a place where people might be rendered unconscious.\n\nuse_locs <- locations %>%\n  count(location, sort = TRUE) %>%\n  top_n(3) %>%\n  pull(location)\n\nlocs_sum <- locations %>%\n  group_by(issue) %>%\n  summarize(mansion = use_locs[[1]] %in% location,\n            danger_room = use_locs[[2]] %in% location,\n            space = use_locs[[3]] %in% location) %>%\n  mutate(across(where(is_logical), as.numeric))\n\nThis will return a dataset that tells us whether a given issue has the X-mansion, the Danger Room, or Space as a location.\n\nlocs_sum %>%\n  glimpse()\n\nRows: 183\nColumns: 4\n$ issue       <dbl> 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 1…\n$ mansion     <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0…\n$ danger_room <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0…\n$ space       <dbl> 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\nNow we can join the three datasets into one useful for modeling. I’m using an inner join here because, for whatever reason, the character visualization dataset has more issues represented than the others, and we only want issues that are represented in all 3 dataframes.\n\nissues_joined <- reduce(list(rend_df, char_sum, locs_sum), ~inner_join(.x, .y, by = \"issue\"))"
  },
  {
    "objectID": "posts/unconsciousness-in-the-xmen/index.html#modeling",
    "href": "posts/unconsciousness-in-the-xmen/index.html#modeling",
    "title": "Unconsciousness in the Xmen",
    "section": "Modeling",
    "text": "Modeling\nCool, so now we’re done preprocessing our data – now we can specify our model.\nI mentioned before that one issue here is that this is a small set of data. We have 183 observations (again, each observation is an issue), which isn’t many. One way to make our modeling more robust is to use bootstrap resampling (see our good friend Wikipedia for an explanation) and to fit models to several resamples.\n\nset.seed(0408)\n\nbooties <- bootstraps(issues_joined, times = 100)\n\nhead(booties$splits, n = 5)\n\n[[1]]\n<Analysis/Assess/Total>\n<183/68/183>\n\n[[2]]\n<Analysis/Assess/Total>\n<183/66/183>\n\n[[3]]\n<Analysis/Assess/Total>\n<183/66/183>\n\n[[4]]\n<Analysis/Assess/Total>\n<183/70/183>\n\n[[5]]\n<Analysis/Assess/Total>\n<183/64/183>\n\n\nWhat we can see here is that every bootstrap sample has 183 rows in the analysis set, which is what the model will be trained on, and then some other number of rows in the assessment set. This other number is the out-of-bag sample – the rows that weren’t randomly sampled by the bootstrap process.\nNext, I’m going to set up a workflow. I think of this as like a little suitcase that can carry things I want to use in my model around – I think that analogy might be from Julia Silge? Anyway, I’m going to start by adding the formula I want to use in my model.\n\nxmen_wf <- workflow() %>%\n  add_formula(rendered_unconscious ~ Magneto + Jean_Grey + Wolverine + Professor_X + mansion + danger_room + space)\n\nNow we can further specify the model. Remember that since our outcome is a count, we’ll be fitting a Poisson regression. Looking at the outcome distribution earlier, I don’t think I need to use a zero-inflated model here (although maybe? Again, this isn’t really my expertise), so I’m just going to proceed with a regular Poisson regression, fit using the {glmnet} engine. I’m also going to tune the penalty and mixture arguments, which control the amount of total regularization applied to the model as well as the proportion of the penalty that is L1 (lasso) vs L2 (ridge regression).\nBrief Interpolation on what a Poisson regression is A Poisson regression is a generalized linear model (GLM) used to model count data. Like the name implies, GLMs are generalizations of linear models that use a link function, g(), to transform the expected value of the response (outcome) to a linear function of the predictor variables. Poisson regression uses a log link function to accomplish this transformation. For people interested in reading more, I really like John Fox’s book, Applied Regression Analysis.\n\nlibrary(poissonreg)\n\npoisson_mod <- poisson_reg(\n  penalty = tune(),\n  mixture = tune()\n) %>%\n  set_engine(\"glmnet\")\n\nSince I’m tuning a couple of parameters, I need to make a grid with possible values to tune across\n\npoisson_tune <- grid_max_entropy(\n  penalty(),\n  mixture(), \n  size = 10\n)\n\nAnd I’ll drop the model spec into the previous workflow.\n\nxmen_wf <- xmen_wf %>%\n  add_model(poisson_mod)\n\nxmen_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: poisson_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nrendered_unconscious ~ Magneto + Jean_Grey + Wolverine + Professor_X + \n    mansion + danger_room + space\n\n── Model ───────────────────────────────────────────────────────────────────────\nPoisson Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = tune()\n\nComputational engine: glmnet \n\n\nAnd now we can fit the model using our bootstrap resamples.\n\nxmen_fit <- tune_grid(\n  xmen_wf,\n  resamples = booties,\n  grid = poisson_tune\n)\n\nOur models have fit, so now we can look at our results:\n\nxmen_fit %>%\n  collect_metrics()\n\n# A tibble: 20 × 8\n    penalty mixture .metric .estimator   mean     n std_err .config             \n      <dbl>   <dbl> <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n 1 8.81e- 4  0.0155 rmse    standard   1.93     100 0.0894  Preprocessor1_Model…\n 2 8.81e- 4  0.0155 rsq     standard   0.0277   100 0.00398 Preprocessor1_Model…\n 3 4.67e- 7  0.0676 rmse    standard   1.93     100 0.0896  Preprocessor1_Model…\n 4 4.67e- 7  0.0676 rsq     standard   0.0277   100 0.00398 Preprocessor1_Model…\n 5 5.56e- 1  0.148  rmse    standard   1.71     100 0.0201  Preprocessor1_Model…\n 6 5.56e- 1  0.148  rsq     standard   0.0288   100 0.00426 Preprocessor1_Model…\n 7 4.76e-10  0.190  rmse    standard   1.93     100 0.0895  Preprocessor1_Model…\n 8 4.76e-10  0.190  rsq     standard   0.0277   100 0.00398 Preprocessor1_Model…\n 9 1.09e- 2  0.500  rmse    standard   1.92     100 0.0841  Preprocessor1_Model…\n10 1.09e- 2  0.500  rsq     standard   0.0278   100 0.00403 Preprocessor1_Model…\n11 2.44e- 7  0.517  rmse    standard   1.94     100 0.0896  Preprocessor1_Model…\n12 2.44e- 7  0.517  rsq     standard   0.0277   100 0.00398 Preprocessor1_Model…\n13 1.73e-10  0.622  rmse    standard   1.94     100 0.0896  Preprocessor1_Model…\n14 1.73e-10  0.622  rsq     standard   0.0277   100 0.00398 Preprocessor1_Model…\n15 1.10e- 5  0.881  rmse    standard   1.94     100 0.0897  Preprocessor1_Model…\n16 1.10e- 5  0.881  rsq     standard   0.0277   100 0.00398 Preprocessor1_Model…\n17 1.99e- 1  0.942  rmse    standard   1.69     100 0.0190  Preprocessor1_Model…\n18 1.99e- 1  0.942  rsq     standard   0.0302   100 0.00404 Preprocessor1_Model…\n19 5.97e-10  0.985  rmse    standard   1.94     100 0.0897  Preprocessor1_Model…\n20 5.97e-10  0.985  rsq     standard   0.0277   100 0.00398 Preprocessor1_Model…\n\n\nOk, so, my limited understanding of Poisson regression is that neither RMSE or R-squared values are ideal metrics, and some googling led me to find that there’s an open issue to add a Poisson log loss metric to the yardstick package, so we’ll gloss over these for now.\nAnyway, let’s pick the best model here, finalize the model, and then fit it to our full training data.\n\nbest_params <- xmen_fit %>%\n  select_best(metric = \"rmse\")\n\nfinal_mod <- xmen_wf %>%\n  finalize_workflow(best_params) %>%\n  fit(data = issues_joined)\n\nAnd let’s check out how important how variables are. This should give us the coefficients from our model.\n\nfinal_mod %>%\n  pull_workflow_fit() %>% \n  vi()\n\n# A tibble: 7 × 3\n  Variable    Importance Sign \n  <chr>            <dbl> <chr>\n1 mansion        0.157   NEG  \n2 danger_room    0.113   NEG  \n3 Professor_X    0.0197  POS  \n4 Jean_Grey      0.0133  POS  \n5 Wolverine      0.00938 POS  \n6 Magneto        0.00701 POS  \n7 space          0       NEG  \n\n\n\nfinal_mod %>%\n  pull_workflow_fit() %>% \n  vip(num_features = 7, fill = lann)\n\n\n\n\nRight, so, one thing to keep in mind here is that the location variables and the character variables are on different scales, so the effects aren’t directly comparable. But the interpretation here is that more appearances of Professor X are more strongly associated with more characters rendered unconscious in an issue than are more appearances of Magneto, although all of these coefficients are positive, suggesting that more appearances of any of these four characters are associated with more renderings unconscious in that issue. Similarly, the effects of danger_room and mansion are negative, suggesting that if the issue features either of those locations, there tend to be fewer characters rendered unconscious. The coefficient for space is 0, which probably means it got regularized out. Probably the most important piece, here, though, is that these effects seem to be very small, which means they likely don’t actually matter.\nI’m going to call it right here. Even though the model I built doesn’t seem to have much explanatory power, it forced me to read some more about Poisson regression and to dig back into the tidymodels framework, which I’ll count as a win. Plus it gives me an excuse to gather “domain knowledge” about comic books so I can do a better job next time."
  },
  {
    "objectID": "posts/virginia-datathon-recap/index.html",
    "href": "posts/virginia-datathon-recap/index.html",
    "title": "2021 Virginia Datathon Recap",
    "section": "",
    "text": "Lessons Learned\nOne thing I appreciated about our approach to this year’s datathon is that it gave me the opportunity to practice with some skills/tools I’ve used before but certainly wouldn’t consider myself super proficient in. More specifically, I got to practice a bit with Shiny and with working with geographical data. Some things I learned/took away are:\n\nThe {leaflet} package is awesome, but I probably need to learn some Javascript. I’ve dabbled with leaflet before, but using it in this instance just reaffirmed how amazing it is. Creating a great-looking, interactive map requires like three lines of R code and a dataframe with some geometry in it. That’s it. And the map we created suited our purposes just fine (or at least it worked as a prototype). That said, when I dug into some of the functions, I think I really need to learn some JS if I want to fully take advantage of the features {leaflet} offers. I’ve also been working with the {reactable} package quite a bit lately, so between these two tools, that might be enough of a push to pick up some JS.\nThe {nngeo} package is also awesome. I’ve done a fair amount of geocoding and working with Census data as part of my job, so I’m reasonably familiar with tools like {tidycensus} and {tidygeocoder}. But I’ve only really had to do nearest neighbors with lat/long data once before, and although I figured it out, my code wasn’t super clean and I felt like I kind of stumbled my way through it. Fortunately, while we were working on this project, Mike found the {nngeo} package and its st_nn() function, which finds the nearest neighbor(s) to each row in X from a comparison dataset Y. So all I had to do was write a little wrapper around this function to tweak the inputs and outputs a little bit (you can see this in the get_closest_ind() function in the functions file in the Github repo).\nI ought to learn more about proxy functions in Shiny. I’ll begin this by saying that my understanding of proxy functions in Shiny is pretty minimal, but my general understanding is that they allow you to modify a specific aspect of a widget (a leaflet map, in this case) without recreating the entire output of the widget. So like you could change the colors of some markers or something. I think the filter functionality we included (allowing users to select all sites, school sites, or non-school sites) could be a candidate for using the leafletProxy() function, but I’m not sure. And given that we had a limited time to make a (very) rough prototype of an app, I didn’t feel like I had had enough time to play around with it on the fly. But it’s definitely something I want to dig into more when I have more time.\n\nOverall, I really enjoyed participating in the VA datathon this year because I felt like I got to expand my toolkit a little bit and work with tools that I don’t always use as part of my day job.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-nc/4.0/CitationBibTeX citation:@online{ekholm2021,\n  author = {Eric Ekholm},\n  title = {2021 {Virginia} {Datathon} {Recap}},\n  date = {2021-10-13},\n  url = {https://www.ericekholm.com/posts/virginia-datathon-recap},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEric Ekholm. 2021. “2021 Virginia Datathon Recap.” October\n13, 2021. https://www.ericekholm.com/posts/virginia-datathon-recap."
  },
  {
    "objectID": "posts/writing-window-functions/index.html",
    "href": "posts/writing-window-functions/index.html",
    "title": "Writing Window Functions",
    "section": "",
    "text": "Setup\nFirst, I’m going to load packages. For this, I’m only using {tidyverse} (and within tidyverse, mostly {purrr} for iteration) and {RcppRoll} as a ground-truth to test my functions. I’m also going to use the {glue} package later on, but that’s less central and I’ll load it when I need it.\n\nlibrary(tidyverse)\nlibrary(RcppRoll)\n\nNext, I’m going to set up a minimal tibble to use for calculations. This will have an day column and a val column. The val column is the one I’m going to be doing calculations on, and the day column is going to serve as an index for the rolling average.\n\nset.seed(0408)\n\ndf <- tibble(\n  day = c(1:250),\n  val = rnorm(250, mean = 5, sd = 1)\n)\n\ndf\n\n# A tibble: 250 × 2\n     day   val\n   <int> <dbl>\n 1     1  2.64\n 2     2  5.30\n 3     3  4.29\n 4     4  5.76\n 5     5  3.75\n 6     6  4.89\n 7     7  4.50\n 8     8  3.84\n 9     9  4.74\n10    10  6.41\n# … with 240 more rows\n\n\n\n\nStep 1: Testing Iteration\nSo, my process for building this function is going to be to create something very basic with few variables first and then gradually abstract this out to make a more responsive function. Eventually, I’ll get to a point where the rolling aggregation function will be general enough to allow for the specification of arbitrary aggregate functions and windows.\nThe first step, then, is just to test the logic of the calculation I need to create to calculate rolling averages. I’ll do this by assuming a 28 day window (we’ll be able to change the window later), create a “truth” to test against using RcppRoll’s roll_mean() function, and then iterate using map().\n\ntruth <- roll_mean(df$val, n = 28, align = \"right\")\n\ntest <- map_dbl(\n  c(28:length(df$val)), #this represents the days I want to calculate the average for. I'm starting on day 28 (because I want a 28-day rolling average, \n  #and the first time I'll have 28 days of data is on day 28) and going through the last day\n  function(a) {\n    mean(df$val[(a - 27):a], na.rm = FALSE) \n  } #this specifies what I'm doing -- taking the mean of the 'val' column for each 28 day window \n  #(day 1-28, day 2-29, etc). If I don't subtract 1 window value when I subset, \n  #I'll actually get 29 days.\n)\n\nall.equal(truth, test) #this tests to see that the vectors are equal.\n\n[1] TRUE\n\n\n\n\nStep 2: Building Out Functions\nGreat, so the logic of the calculation works. Now, let’s extend it a little bit to create a function where I can specify the variable I want to use as well as the window I want to take the rolling average over.\n\nee_roll_mean <- function(x, window) {\n\n  map_dbl(\n    c(window:length(x)),\n    function(a) {\n      mean(x[(a - window+1):a], na.rm = FALSE)\n    }\n\n  )\n}\n\ntest_2 <- ee_roll_mean(df$val, 28)\n\nall.equal(test_2, truth)\n\n[1] TRUE\n\n\nIt works when we set the window value to 28, but let’s also test that it works when we use a different window just to be safe.\n\ntruth_win8 <- roll_mean(df$val, n = 8, align = \"right\")\n\ntest_win8 <- ee_roll_mean(df$val, window = 8)\n\nall.equal(truth_win8, test_win8)\n\n[1] TRUE\n\n\nThis works well for taking the rolling average – we can specify the values we want to take the average over as well as the window for that average. But there are other functions we might be interested in getting rolling aggregates for as well. For instance, we might want to know the minimum or standard deviation of a value during some windows of time. Rather than write separate functions to do this, we can just extend our previous function to allow us to supply whichever aggregation function we want.\n\nee_roll_func <- function(x, window, fn = mean) {\n\n  map_dbl(\n    c(window:length(x)),\n    function(a) {\n      fn(x[(a - window+1):a], na.rm = FALSE)\n    }\n  ) \n}\ntest_3 <- ee_roll_func(df$val, window = 8, fn = sd)\n\n#testing against the RcppRoll function that does the same thing\n\ntruth_3 <- roll_sd(df$val, n = 8, align = \"right\")\n\nall.equal(test_3, truth_3)\n\n[1] TRUE\n\n\n\n\nStep 3: Pad the Output\nOne thing I’m noticing when looking at the output of each of these functions is that the length of the output vectors differ depending on the value we pass to the window argument.\n\nlength(test)\n\n[1] 223\n\nlength(test_win8)\n\n[1] 243\n\n\nI’m also noticing that these outputs are shorter than the length of the input vector (which is length 250). This makes sense because the function can’t take, for example, the 28 day average before the 28th day, and so the length of the output vector will be 27 elements shorter than the length of the input vector.\nThis isn’t so great if we want to add the results of this function back into our original df, though, because all of the vectors in a df need to be the same length. One solution is to “pad” our output vector with the appropriate amount of NA values so that it is the same length as the input vector and can therefore get added as a column in our df. So let’s do that.\n\nee_roll_func_padded <- function(x, window, fn = mean) {\n\n  map_dbl(\n    c(window:length(x)),\n    function(a) {\n      fn(x[(a - window+1):a], na.rm = FALSE)\n    }\n  ) %>%\n    append(rep(NA_real_, times = window-1), values = .)   #this will pad the front with a number of NAs equal\n  #to the window value minus 1\n\n}\ntest_pad1 <- ee_roll_func_padded(df$val, window = 8) #note that if we don't supply a function, it will use the mean\n\ntest_pad2 <- ee_roll_func_padded(df$val, window = 20)\n\ntest_pad1\n\n  [1]       NA       NA       NA       NA       NA       NA       NA 4.372225\n  [9] 4.634703 4.773530 4.751241 4.837210 4.835834 4.947405 5.023067 5.159392\n [17] 5.259393 4.897024 5.154236 4.748580 4.790054 4.403228 4.522648 4.519479\n [25] 4.480582 4.687750 4.701154 4.851093 4.652568 4.847791 4.811578 4.864686\n [33] 4.672642 4.530416 4.582749 4.682431 4.717240 4.746443 4.652665 4.466197\n [41] 4.611190 4.706513 4.568209 4.517622 4.872942 5.065789 5.186852 5.390533\n [49] 5.395041 5.507209 5.403271 5.174482 5.179670 5.038712 5.020135 4.838939\n [57] 4.875701 4.755078 4.865224 5.176775 5.202352 5.000563 4.797047 4.894503\n [65] 4.810376 5.004196 4.977340 4.848640 4.753013 4.961929 5.142875 5.096611\n [73] 5.248953 5.181127 4.941060 4.842180 4.693671 4.603321 4.722901 4.707204\n [81] 4.667018 4.490093 4.642128 4.688560 4.940980 5.010917 4.865457 5.077085\n [89] 4.943111 5.104771 5.225281 5.405689 5.459406 5.772019 5.873998 5.653444\n [97] 5.727537 5.800159 5.719428 5.649400 5.519840 5.130266 4.799206 5.049435\n[105] 4.941485 4.868625 4.976469 5.154863 5.039641 5.037770 5.202060 4.829763\n[113] 5.054458 5.091318 5.113392 5.056769 4.999436 5.110106 5.070160 5.305183\n[121] 5.148242 5.163269 5.116071 5.209866 5.295613 5.295760 5.642222 5.797642\n[129] 5.800138 5.454873 5.221126 5.037245 5.077385 5.216140 5.121762 4.768109\n[137] 4.833714 5.100003 5.221173 5.314504 5.166415 4.883192 4.762374 4.661057\n[145] 4.620171 4.638887 4.789642 4.625148 4.791990 5.013448 4.746997 5.084247\n[153] 4.989471 4.899552 4.728081 4.728852 4.656302 4.596832 4.789755 4.571342\n[161] 4.750549 4.828835 4.946644 4.904696 4.951820 4.962249 4.952014 5.015733\n[169] 4.920095 4.695109 4.624958 4.687815 5.038474 5.314062 5.471601 5.659262\n[177] 5.667469 5.904322 5.968823 6.073087 5.663232 5.407968 5.177870 5.237016\n[185] 5.445955 5.679831 5.614257 5.233444 5.227926 5.097925 5.119121 4.940067\n[193] 4.803742 4.593282 4.749424 5.008870 4.902099 5.014811 5.048332 5.111487\n[201] 5.059727 4.972699 4.866232 4.952064 4.924344 5.077133 5.166955 5.172722\n[209] 5.304330 5.370433 5.299762 5.238768 5.450415 5.399515 5.197358 5.101200\n[217] 5.005289 5.243733 5.194603 5.205039 5.192346 5.082026 5.030877 5.072784\n[225] 5.032299 4.637538 4.781121 4.812846 4.758887 4.541770 4.712547 4.636478\n[233] 4.876790 5.177345 4.831910 4.870811 5.106333 5.162062 4.990127 5.058875\n[241] 4.603333 4.441803 4.618171 4.585108 4.444892 4.505732 4.827083 4.840013\n[249] 5.098275 5.081742\n\n\nNotice that when we call test_pad1 we get a vector with several NA values appended to the front. And when we look at the length of each of these vectors, we can see that they’re length 250\n\nlength(test_pad1)\n\n[1] 250\n\nlength(test_pad2)\n\n[1] 250\n\n\n\n\nStep 4: Use Functions to Add Columns to Data\nNow that we have a function that reliably outputs a vector the same length as the columns in our dataframe, we can use it in conjunction with other tidyverse operations to add columns to our dataframe.\n\ndf %>%\n  mutate(roll_avg = ee_roll_func_padded(val, window = 8, fn = mean))\n\n# A tibble: 250 × 3\n     day   val roll_avg\n   <int> <dbl>    <dbl>\n 1     1  2.64    NA   \n 2     2  5.30    NA   \n 3     3  4.29    NA   \n 4     4  5.76    NA   \n 5     5  3.75    NA   \n 6     6  4.89    NA   \n 7     7  4.50    NA   \n 8     8  3.84     4.37\n 9     9  4.74     4.63\n10    10  6.41     4.77\n# … with 240 more rows\n\n\nFinally, what if we wanted to get the rolling mean, standard deviation, min, and max all as new columns in our dataframe using the function we created. Our function allows us to pass in whichever aggregation function we want to use (well, probably not any function), so we can use pmap() from {purrr} to iterate over multiple functions and, in combination with the {glue} package, also set meaningful names for the new variables.\nI’ll set up a dataframe called params that has the names of the new variables and the corresponding functions, then I’ll loop over these names and functions to create new columns in our original dataframe. I’m not going to go over all of the code here, but if you’re curious, it might be helpful to look at the documentation for {glue}, {purrr}, and possibly {rlang} (for the := operator).\n\nlibrary(glue)\n\nparams <- tibble(\n  names = c(\"roll_avg\", \"roll_sd\", \"roll_min\", \"roll_max\"),\n  fn = lst(mean, sd, min, max)\n)\n\nparams %>%\n  pmap_dfc(~df %>%\n             transmute(\"{.x}\" := ee_roll_func_padded(val, window = 8, fn = .y))) %>%\n  bind_cols(df, .)\n\n# A tibble: 250 × 6\n     day   val roll_avg roll_sd roll_min roll_max\n   <int> <dbl>    <dbl>   <dbl>    <dbl>    <dbl>\n 1     1  2.64    NA     NA        NA       NA   \n 2     2  5.30    NA     NA        NA       NA   \n 3     3  4.29    NA     NA        NA       NA   \n 4     4  5.76    NA     NA        NA       NA   \n 5     5  3.75    NA     NA        NA       NA   \n 6     6  4.89    NA     NA        NA       NA   \n 7     7  4.50    NA     NA        NA       NA   \n 8     8  3.84     4.37   0.982     2.64     5.76\n 9     9  4.74     4.63   0.691     3.75     5.76\n10    10  6.41     4.77   0.918     3.75     6.41\n# … with 240 more rows\n\n\nThis gives us, for each 8-day window (e.g. day 1-8, day 2-9, etc) an average, standard deviation, minimum, and maximum of the val column.\n\n\nWrapping Up\nAs sort of a final note, this activity was meant to be both an exercise for me in working through some programming using window functions as well as a walkthrough/tutorial for others interested in writing functions. That said, when I dive back into the Kaggle data I mentioned earlier, I’ll use the functions from the {RcppRoll} package rather than my own. These are optimized to run quickly because they use C++ code and they’re going to be more efficient than anything I just wrote. This doesn’t matter much when we use a little 250 observation dataframe for demonstration, but it will make a difference working with several thousand observations at once.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-nc/4.0/CitationBibTeX citation:@online{ekholm2020,\n  author = {Eric Ekholm},\n  title = {Writing {Window} {Functions}},\n  date = {2020-05-06},\n  url = {https://www.ericekholm.com/posts/writing-window-functions},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEric Ekholm. 2020. “Writing Window Functions.” May 6, 2020.\nhttps://www.ericekholm.com/posts/writing-window-functions."
  },
  {
    "objectID": "posts/your-moms-house-analysis/index.html",
    "href": "posts/your-moms-house-analysis/index.html",
    "title": "Your Mom’s House Analysis",
    "section": "",
    "text": "What’s going on, mommies? I’ve been a big fan of Your Mom’s House for a long time now, and find myself constantly refreshing my podcast feed on Wednesdays waiting for those sweet sweet updates. I’ve also been getting more and more into text analysis, and so I figured why not combine the two and analyze the transcripts from YMH.\nA few months ago, I wrote a blog post describing how to pull transcripts from each video in a Youtube playlist (6/22/22 update: this link is broken now – whoops!). I’m not going to go over that part again, so if you’re curious about how to get the data, check out that post.\nA couple notes before getting into the analysis:\nWith that out of the way, let’s move on and make sure to follow proto."
  },
  {
    "objectID": "posts/your-moms-house-analysis/index.html#setup",
    "href": "posts/your-moms-house-analysis/index.html#setup",
    "title": "Your Mom’s House Analysis",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(eemisc)\nlibrary(lubridate)\nlibrary(tidytext)\nlibrary(reactable)\nlibrary(tidylo)\nlibrary(igraph)\nlibrary(ggraph)\n\ntheme_set(theme_ee())\n\nherm <- harrypotter::hp(n = 1, option = \"HermioneGranger\")\n\nopts <- options(\n  ggplot2.discrete.fill = list(\n    harrypotter::hp(n = 3, option = \"HermioneGranger\"),\n    harrypotter::hp(n = 5, option = \"Always\")\n  )\n)\n\nymh <- read_csv(here::here(\"data/ymh_trans.csv\")) %>%\n  filter(ep_num > 370)\n\nymh_one_obs <- ymh %>%\n  group_by(title, ep_num) %>%\n  summarize(text = str_c(text, collapse = \" \") %>%\n           str_to_lower()) %>%\n  ungroup()"
  },
  {
    "objectID": "posts/your-moms-house-analysis/index.html#general-exploration",
    "href": "posts/your-moms-house-analysis/index.html#general-exploration",
    "title": "Your Mom’s House Analysis",
    "section": "General Exploration",
    "text": "General Exploration\nThere are a total of 169 episodes, beginning with 394 and ending with 567\nNext, let’s take a look at the length of the episodes here.\n\nep_lengths <- ymh %>%\n  group_by(ep_num) %>%\n  summarize(mins = (max(start) + max(duration))/60) %>%\n  ungroup()\n\nep_lengths %>%\n  ggplot(aes(x = ep_num, y = mins)) +\n  geom_line(color = herm) +\n  geom_point(color = herm) +\n  labs(\n    title = \"Length of YMH Episodes\",\n    x = \"Episode Number\",\n    y = \"Length (mins)\"\n  )\n\n\n\n\nIt looks like episodes have tended to get longer up until about episode ~510 or so, and then the length dropped a bit. This might have been due to COVID and the fact that there was a stretch there where they didn’t have guests (which made the episodes shorter).\nNext, let’s take a look at the most common guests on the show.\n\nymh %>%\n  distinct(title) %>%\n  mutate(guest = str_remove_all(title, \"^.* w/ \") %>%\n           str_remove_all(\" - Ep. \\\\d+| - REUPLOADED\")) %>%\n  filter(str_detect(guest, \"Your Mom\", negate = TRUE)) %>%\n  mutate(guest = str_replace_all(guest, \"\\\\&\", \",\")) %>%\n  separate_rows(guest, sep = \",\") %>%\n  mutate(guest = str_trim(guest)) %>%\n  count(guest, name = \"num_appearances\", sort = TRUE) %>% \n  reactable()\n\n\n\n\n\n\nNote that this only counts guests named in the title of the episode and won’t capture people like Josh Potter, Top Dog & Charo, etc. Dr. Drew is also probably under-counted here, since he makes a lot of cameos via phone call and whatnot (like when Tim and Christine called him up to ask about corn cob sanitation recently). The big takeaway here, though, is that there really aren’t that many repeat guests."
  },
  {
    "objectID": "posts/your-moms-house-analysis/index.html#most-common-words",
    "href": "posts/your-moms-house-analysis/index.html#most-common-words",
    "title": "Your Mom’s House Analysis",
    "section": "Most Common Words",
    "text": "Most Common Words\nCool. Let’s shift gears here, now, and start to dig into the actual transcripts. Once again, these don’t delineate who is speaking, but there is still a lot of data here. I’m going to start by looking at the most commonly-used words across all of the episodes. In doing this, I’m also going to filter out stop words – things like “the,” “at,” “I’m”, etc.\n\nymh_words <- ymh_one_obs %>%\n  unnest_tokens(word, text)\n\nymh_words <- ymh_words %>%\n  anti_join(stop_words) %>%\n  filter(!(word %in% c(\"yeah\", \"gonna\", \"uh\", \"hey\", \"cuz\")))\n\nymh_words %>%\n  count(word) %>%\n  slice_max(order_by = n, n = 20) %>%\n  ggplot(aes(x = n, y = fct_reorder(word, n))) +\n  geom_col(fill = herm) +\n  labs(\n    title = \"Most Used Words in YMH\",\n    y = NULL,\n    x = \"Count\"\n  )\n\n\n\n\nOk, so, there’s a lot we could unpack here. But what stands out to me is that “fucking” is the 3rd most commonly used word in the podcast, right after “people” and “guy.” This isn’t surprising to me, but I want to dig into it a little more on an episode-by-episode basis.\nI’m going to take a look at how many times the word “fucking” appears per episode. Actually, I’m going to look at “fucking”s per minute as a way to control for episode length.\n\nymh_words %>%\n  filter(word == \"fucking\") %>%\n  count(ep_num, word) %>%\n  left_join(ep_lengths, by = \"ep_num\") %>%\n  mutate(fpm = n/mins) %>%\n  ggplot(aes(x = ep_num, y = fpm)) +\n  geom_text(aes(size = fpm), label = \"fucking\", show.legend = FALSE, color = herm) +\n  annotate(\"text\", x =  510, y = 1.6, label = \"Ep 494 with Joey Diaz\", hjust = 0, size = 3.5) +\n  annotate(\"curve\", x = 494, xend = 508, y = 1.68, yend = 1.58, curvature = .4) +\n  labs(\n    x = \"Episode Number\",\n    y = \"'Fucking's Per Minute\",\n    title = \"Fucking's per Minute in YMH Episodes\"\n  )\n\n\n\n\nIt’s maybe no surprise that Episode 494 with Joey Diaz had far and away the highest rate of fucking’s-per-minute at 1.76, which is about .76 higher than the next highest (1.008 in Ep 433 with Bill Burr)."
  },
  {
    "objectID": "posts/your-moms-house-analysis/index.html#ymh-vocabulary-over-time",
    "href": "posts/your-moms-house-analysis/index.html#ymh-vocabulary-over-time",
    "title": "Your Mom’s House Analysis",
    "section": "YMH Vocabulary Over Time",
    "text": "YMH Vocabulary Over Time\nNext, I want to look at the evolution of some of the more popular mommy-isms over time – the memes and vocabulary that define the show. I’m going to start by tracking the use of the word “Julia” – as in “good morning, julia!” – over time, since that’s still my favorite clip from the show.\n\nymh_words %>%\n  filter(word == \"julia\") %>%\n  count(ep_num, word) %>%\n  left_join(x = tibble(ep_num = unique(ep_lengths$ep_num), word = \"julia\"),\n            y = ., by = c(\"ep_num\", \"word\")) %>%\n  mutate(n = replace_na(n, 0)) %>%\n  ggplot(aes(x = ep_num, y = n)) +\n  geom_line(color = herm, size = 1.25) +\n  labs(\n    y = \"Count\",\n    x = \"Episode Number\",\n    title = \"Good Morning, Julia!\",\n    subtitle = \"Counts of the word 'Julia' by episode\"\n  )\n\n\n\n\nSo, we see a pretty clear trend here – the popularity of “julia” starts up right around episode ~470ish, stays high for a few episodes, and then peaks at episode ~477 (give or take), which I assume is the episode where Tony and Catherine actually interview Julia.\nThere are tons of other words and phrases I could make this same graph for – “four strokes,” “let me eat ya,” etc. – but I’m going to pick a handful and write a little function that’ll extract those from the transcript and then make a faceted graph similar to the above.\n\nisolate_phrases <- function(phrase) {\n  nwords <- str_count(phrase, \" \") + 1\n  \n  token <- if (nwords == 1) {\n    \"words\"\n  } else {\n      \"ngrams\"\n  }\n  \n  tmp <- if (token == \"ngrams\") {\n    ymh_one_obs %>%\n      unnest_tokens(output = words, input = text, token = token, n = nwords)\n  } else {\n    ymh_one_obs %>%\n      unnest_tokens(output = words, input = text, token = token)\n  }\n  \n  tmp %>%\n    filter(words == phrase)\n}\n\nphrases <- c(\"julia\", \"charles\", \"robert paul\", \"retarded\", \"garth\", \"cool guy\", \"fed smoker\", \"feathering\", \"scrum\", \"mystic rick\", \"don't be stingy\", \"piss on me\")\n\nymh_phrase_list <- map(phrases, isolate_phrases)\n\nphrase_grid <- expand_grid(unique(ep_lengths$ep_num), phrases) %>%\n  rename(ep_num = 1)\n\nymh_phrase_df <- ymh_phrase_list %>%\n  bind_rows() %>%\n  count(ep_num, words) %>%\n  left_join(x = phrase_grid, y = ., by = c(\"ep_num\", \"phrases\" = \"words\")) %>%\n  mutate(n = replace_na(n, 0))\n\nymh_phrase_df %>%\n  mutate(phrases = str_to_title(phrases)) %>%\n  ggplot(aes(x = ep_num, y = n)) +\n  geom_line(color = herm) +\n  facet_wrap(~phrases, scales = \"free_y\") +\n  labs(\n    y = \"Count\",\n    x = \"Episode Number\",\n    title = \"Now Here Are Some Cool Phrases\",\n    subtitle = \"Counts of Key YMH Words/Phrases by Episode\"\n  ) +\n  ggthemes::theme_tufte() +\n  theme(\n    plot.title.position = \"plot\"\n  )\n\n\n\n\nOne thing to note here is that the y-axis scale is different for each facet, so it’s not really easy to make comparisons across plots. But if I keep them on the same scale, it makes it hard to see movement in anything other than the “Julia” plot. Overall, though, I think this does a pretty nice job at tracking the lifespan of some of the common memes/jokes that run through the show."
  },
  {
    "objectID": "posts/your-moms-house-analysis/index.html#popular-words-across-groups-of-episodes",
    "href": "posts/your-moms-house-analysis/index.html#popular-words-across-groups-of-episodes",
    "title": "Your Mom’s House Analysis",
    "section": "Popular Words Across Groups of Episodes",
    "text": "Popular Words Across Groups of Episodes\nOne limitation of the plots above is that I just chose 12 words/phrases that I think exemplify the show and tracked their usage over time. Like I said, there are tons of other phrases I could have chosen instead, and so the analysis is kinda limited by my personal choice.\nOne way to let the data drive the analysis is to look at “definitive” words over time. The way I’ll do this is – I’ll classify every 5 episodes as a group (yes this is arbitrary, but it seems reasonable enough). Then, I’ll calculate the weighted (log) odds of a specific word showing up in that group versus any other group. Then I’ll choose the words with the highest odds per group. These will be the “definitive” words for that group of episodes.\n\nymh_groups <- tibble(\n  ep_num = unique(ep_lengths$ep_num),\n  group = 1:length(unique(ep_lengths$ep_num)) %/% 5\n) %>%\n  group_by(group) %>%\n  mutate(max = max(ep_num),\n         min = min(ep_num),\n         group_name = glue::glue(\"Ep { min } - Ep { max }\")) %>%\n  ungroup() %>%\n  select(-c(group, min, max))\n\nset.seed(0408)\n\nymh_lo <- ymh_words %>%\n  left_join(x = .,\n            y = ymh_groups,\n            by = \"ep_num\") %>%\n  count(group_name, word) %>%\n  filter(n > 6 & (str_length(word) > 2) & !(word %in% c(\"xiy\", \"pql\", \"8ww\", \"hmh\", \"music\"))) %>%\n  bind_log_odds(set = group_name, feature = word, n = n) %>%\n  group_by(group_name) %>%\n  slice_max(order_by = log_odds_weighted, n = 3, with_ties = FALSE)\n\nymh_lo %>%\n  ungroup() %>%\n  mutate(header = rep(c(\"Most Distinctive\", \"2nd Most Distinctive\", \"3rd Most Distinctive\"),\n            length(unique(ymh_lo$group_name)))) %>%\n  select(-c(n, log_odds_weighted)) %>%\n  rename(`Ep Group` = group_name) %>%\n  pivot_wider(names_from = header, values_from = word) %>%\n  reactable()\n\n\n\n\n\n\nLet’s unpack this a little bit. I’ll start at the end. The three most definitive words of Eps 563-567 are “stingy” (as in “don’t be stingy, Mark”), “quarantine” (as in the thing everyone is talking about), and “fed” (as in “fed smoker” – presumably this is driven by the interview with fed smoker’s nemesis). Before that, in Eps 558-562, we have “jamie” (as in Jamie-Lynn Sigler), “jewish” (the dude with the cool mouse pads), and, again, “quarantine.” This analysis gives us a loose idea about what some of the key themes of these episodes are."
  },
  {
    "objectID": "posts/your-moms-house-analysis/index.html#connections-between-words",
    "href": "posts/your-moms-house-analysis/index.html#connections-between-words",
    "title": "Your Mom’s House Analysis",
    "section": "Connections between Words",
    "text": "Connections between Words\nThe last thing I’m going to do is look at connections between words. To do this, I’m going to make a network diagram. The basic idea here is that this analysis will visualize words that are commonly used together by drawing links between them. I’m also going to change the size of the word to indicate how often the word gets used (with bigger words being used more).\nOne other note – like in previous steps here, I’m filtering out stop words (e.g. “the”, “it”, etc). I also noticed when I first tried this that I got a lot of phrases from Saatva reads – lots of pairings with the word “mattress” and “delivery,” so I’m filtering those out as well.\n\nfilter_words <- c(\"music\", \"yeah\", \"uh\", \"huh\", \"hmm\", \"mattress\", \"delivery\")\n\nymh_bigrams <- ymh_one_obs %>%\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2) %>%\n  separate(bigram, c(\"word1\", \"word2\")) %>%\n  filter(!is.element(word1, stop_words$word) &\n           !is.element(word2, stop_words$word) &\n           !is.na(word1) &\n           !is.na(word2) &\n           !is.element(word1, filter_words) &\n           !is.element(word2, filter_words) &\n           word1 != word2) %>%\n  count(word1, word2, sort = TRUE)\n\nymh_bigrams_small <- ymh_bigrams %>%\n  slice_max(order_by = n, n = 100)\n\nymh_verts <- ymh_one_obs %>%\n  unnest_tokens(word, text) %>%\n  count(word) %>%\n  mutate(n = log(n)) %>%\n  filter(is.element(word, ymh_bigrams_small$word1) |\n           is.element(word, ymh_bigrams_small$word2))\n\nymh_net <- graph_from_data_frame(ymh_bigrams_small, vertices = ymh_verts)\n\nset.seed(0409)\n\nymh_net %>%\n  ggraph(layout = \"igraph\", algorithm = \"fr\") +\n  geom_edge_link(aes(size = n), color = herm) +\n  geom_node_point(color = herm) +\n  geom_node_text(aes(label = name, size = n), repel = TRUE, segment.color = herm, label.padding = .1, box.padding = .1) +\n  theme_void() +\n  labs(title = \"Most Commonly Connected Words in YMH\") +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = .5, size = 16))\n\n\n\n\nThe way to read this graph is to look for words that are connected (note that the direction doesn’t mean anything here – it’s just random). So, the words “tick” and “tock” (TikTok) show up a lot together, as do “toilet” and “paper” (from Tushy reads). Over on the left side, we see more of a wheel-shaped pattern with “gonna” in the middle, which is linked to a bunch of other words – “gonna die,” “gonna throw” (up), “gonna call,” etc. There’s also a little network toward the bottom left where we see connections between guy, guys, black, white, cool, and nice (black guys, cool guys, white guy, etc). A few other standouts to me are “water champ,” “mentall ill”, “robert paul,” and, of course “morning julia.”\nThat’s it for now – keep it high and tight, Jeans."
  },
  {
    "objectID": "pubs.html",
    "href": "pubs.html",
    "title": "Publications",
    "section": "",
    "text": "Peer-Reviewed Publications\nBack when I was more actively involved in academia, I published some papers, mostly related to student motivation, how people write, or statistical methodology. Publishing peer-reviewed work has been less of a priority for me lately, but I still dabble here and there.\n\nMarrs, S., Zumbrunn, S., & Ekholm, E. (2022). Measuring students’ perceptions of writing feedback. Current Research in Psychology and Behavioral Science, 3(4).\nZumbrunn, S., Ekholm, E., Broda, M., & Koenka, A. C. (2022). Trajectories of students’ writing feedback attitudes. Journal of Experimental Education. https://doi.org/10.1080/00220973.2022.2064413\nChow, J., Ekholm, E., & Bae, C. (2021). Relative contribution of verbal working memory and attention to child language. Assessment for Effective Intervention, 47(1), 3-13.\nBroda, M., Ekholm, E., & Zumbrunn, S. (2020). Assessing the predictive nature of teacher and student writing self-regulation discrepancy. Frontline Learning Research, 8(4), 52.\nGellock, J., Ekholm, E., Greenhalgh, G., LeCrom, C., Reina, C., & Kudesia, R. (2019). Women’s lacrosse players’ perceptions of teammate leadership: Examining athlete leadership behaviors, attributes, and interactions. Journal of Athlete Development and Experience, 1(2), 2. https://scholarworks.bgsu.edu/jade/vol1/iss2/2.\nZumbrunn, S., Marrs, S., Broda, M., Ekholm, E., Jackson, L., & DeBusk-Lane, M. (2019). Toward a more complete understanding of writing enjoyment: A mixed methods study of elementary students. AERA Open, 5(2), https://doi.org/10.1177/2332858419850792.\nBroda, M., Tucker, S., Ekholm, E., Johnson, T., & Liang, Q. (2019). Small fingers, big data: Preschoolers’ subitizing speed and accuracy during interactions with multi-touch technology. The Journal of Educational Research, 112(2), 211-222. https://doi.org/10.1080/00220671.2018.1486281.\nEkholm, E. & Chow. J. C. (2018). Addressing publication bias in educational psychology. Translational Issues in Psychological Science, 4(4), 425-439. http://dx.doi.org/10.1037/tps0000181.\nBroda, M., Ekholm, E., Schneider, B., & Hutton, A. (2018). Teachers’ social networks, college-going practices, and the diffusion of a school-based reform initiative. SAGE Open. https://doi.org/10.1177/2158244018817397.\nChow, J.C., Ekholm, E., & Coleman, H. (2018). Does oral language underpin the development of later behavior problems? A longitudinal meta-analysis. School Psychology Quarterly, 33(3), 337-349. http://dx.doi.org/10.1037/spq0000255.\nChow, J.C., & Ekholm, E. (2018). Do published studies yield larger effect sizes than unpublished studies in education and special education? A meta-review. Educational Psychology Review, 30(3), 727-744. https://doi.org/10.1007/s10648-018-9437-7.\nChow, J. C., & Ekholm, E. (2018). Language domains differentially predict mathematics performance in young children. Early Childhood Research Quarterly, 46, 179-186. https://doi.org/10.1016/j.ecresq.2018.02.011\nEkholm, E., Zumbrunn, S. & DeBusk-Lane, M. (2018). Clarifying an elusive construct: A systematic review of writing attitudes. Educational Psychology Review, 30(3), 827-856. https://doi.org/10.1007/s10648-017-9423-5.\nEkholm, E. (2017). Ethical concerns of using texts involving death in the English classroom. English Journal, 107(2), 25-30.\nZumbrunn, S., Ekholm, E., Stringer, J. K., McKnight, K., & DeBusk-Lane, M. (2017). Student experiences with writing: Taking the temperature of the classroom. The Reading Teacher, 70(6), 667-677. https://doi.org/10.1002/trtr.1574.\nEkholm, E., Zumbrunn, S., & Conklin, S. (2015). The relation of college student self-efficacy toward writing and writing self-regulation aptitude: writing feedback perceptions as a mediating variable. Teaching in Higher Education, 20(2), 197-207. https://doi.org/10.1080/13562517.2014.974026.\n\n\n\n\nOther Publications\n\nFiction and Humor\n\nEkholm, E. “If You Take My Survey, I’ll Let You Watch Me Cut My Finger Off.” McSweeney’s Internet Tendency (June 7, 2016).\nEkholm, E. “Call for Submissions for APA’s Annual Conference, Sponsored by Shark Week.” McSweeney’s Internet Tendency (August 11, 2014).\n\n\n\nBook Reviews\n\nEkholm, E. (September, 2016). A review of Kelly Gallagher’s In the Best Interests of Students.\n\n\n\nOther\n\nGuest blog post, AERA Motivation SIG Blog, Ekholm, E. (2018). “What Motivation Researchers Can Learn from Professional Sports Teams.” November 6, 2018.\n\n\n\n\n\nConference Presentations & Invited Talks\nOnly conference presentations since 2018 included\n\nEkholm, E. (May, 2022). Data Viz Tips & Tricks. Invited talk presented to the Educational Research & Evaluation Network (EREN).\nEkholme, E., & Fox, P. (October, 2021). Using machine learning to support student attendance practices. Session presented at the annual Metropolitan Education Research Consortium (MERC) conference, Richmond, VA. Video available here (~27:30 timestamp).\nEkholm, E., & Zumbrun, S. (April, 2020). Investigating relations between writers’ emotional experiences and attention regulation: A daily diary study. Paper presented at the American Educational Research Association (AERA) Annual Meeting, San Francisco, CA.\nEkholm, E., Zumbrunn, S., Broda, M., & Luther, T. C. (April, 2019). The development of student writing feedback attitudes in grades 3-7: A latent growth analysis. Poster presented at the American Educational Research Association (AERA) Annual Meeting, Toronto, Canada.\nZumbrunn, S., Broda, M., Marrs, S., & Ekholm, E. (April, 2019). The complexities and challenges of understanding individual differences and the development of multiple dimensions of writing self-efficacy. Poster presented at the American Educational Research Association (AERA) Annual Meeting, Toronto, Canada.\nBroda, M., Ekholm, E., Hutton, C., & Schneider, B. (April, 2019). Teachers’ social networks, college-going practices, and the diffusion of a school-based reform initiative. Paper presented at the American Educational Research Association (AERA) Annual Meeting, Toronto, Canada.\nZumbrunn, S., Broda, M., Ekholm, E., & Luther, T. C. (April, 2019). RoboCogger: Using mobile technology to assess and increase student writing metacognition, motivation, and performance. Paper presented at the American Educational Research Association (AERA) Annual Meeting, Toronto, Canada.\nBroda, M., Ekholm, E., & Zumbrunn, S. (October, 2018). Are we grading the “write” stuff? The relationship between teachers’ expectations, students’ self-regulation, and writing achievement. Paper presented at the CREATE Conference, Williamsburg, VA.\nZumbrunn, S., Marrs, S., Malmberg, L., Ekholm, E., Broda, M., & DeBusk-Lane, M. (August, 2018). Individual differences and the development of multiple dimensions of writing self-efficacy. Paper presented at the International Conference of the EARLI Special Interest Group on Writing, Antwerp, Belgium.\nEkholm, E., Zumbrunn, S., Hope, S., & Stim, H. (April, 2018). Teachers’ writing beliefs and instructional practices: A mixed-methods study. Poster presented at the American Educational Research Association (AERA) Annual Meeting, New York City, NY.\nMarrs, S., Ekholm, E., & Zumbrunn, S. (April, 2018). Exploring profiles of student perceptions of writing feedback. Roundtable presented at the American Educational Research Association (AERA) Annual Meeting, New York City, NY.\nTucker, S., Broda, M., Ekholm, E., Johnson, T., & Liang, Q. (April, 2018). Preschoolers’ subitizing speed and accuracy during interactions with multi-touch technology. Roundtable presented at the American Educational Research Association (AERA) Annual Meeting, New York City, NY.\nChow, J.C. & Ekholm, E. (February, 2018). Estimating the magnitude of the difference between published and unpublished studies in special education: A metareview. Poster presented at the Council for Exceptional Children Annual Convention, Tampa, FL."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Eric Ekholm’s Resume",
    "section": "",
    "text": "Eric Ekholm\n\n\n\n\n\n eric.ekholm@gmail.com\n github.com/ekholme\n 703-434-9689\n twitter.com/ekholm_e\n\n\n\n\n\nStatistical modeling\nData visualization and data dashboarding\nAdvanced proficiency in R\nProficiency in Go and Julia\nCloud computing via Google Cloud\n\n\n\n\nLast updated on 2022-08-31."
  },
  {
    "objectID": "resume.html#title",
    "href": "resume.html#title",
    "title": "Eric Ekholm’s Resume",
    "section": "Eric Ekholm",
    "text": "Eric Ekholm"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Eric Ekholm’s Resume",
    "section": "Education",
    "text": "Education\n\nPhD, Educational Psychology\nVirginia Commonwealth University\nRichmond, VA\n2019\nDissertation: Investigating Daily Writing Emotions, Attention Regulation, and Productivity: An Intensive Longitudinal Study\n\n\nMT, English Education\nVirginia Commonwealth University\nRichmond, VA\n2012\n\n\nBA, English\nVirginia Tech\nBlacksburg, VA\n2010"
  },
  {
    "objectID": "resume.html#professional-experience",
    "href": "resume.html#professional-experience",
    "title": "Eric Ekholm’s Resume",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nData Specialist\nChesterfield County Public Schools\nMidlothian, VA\n2021 - present\n\nProduce reports and visualizations that clearly communicate the results of statistical analyses to school division leadership and other nontechnical audiences.\nDesign and conduct program evaluations using rigorous statistical methods.\nDevelop and maintain internal R packages to streamline analytic workflows.\nCreate dashboards to help teachers, principals, and school division leadership extract actionable information from large datasets.\nDevelop predictive models and educate end-users on their appropriate use.\n\n\n\nAssociate Director of Early Childhood Data and Analytics\nVirginia Department of Education\nRichmond, VA\n2019 - 2021\n\nOversaw the development of LinkB5, Virginia’s early childhood data portal, including guiding UI/UX and business logic decisions to support users.\nFacilitated data sharing and research efforts with external partners, including public and private organizations.\nCreated individualized early childhood reports and visualizations for state legislators and other state officials.\nUsed data and published research to recommend equitable measurement criteria and thresholds for Virginia’s early childhood quality rating and improvement system.\n\n\n\nGraduate Research Assistant\nVirginia Commonwealth University\nRichmond, VA\n2016 - 2019\n\nPublished 11 peer-reviewed articles on a variety of topics, including statistical methodology, student motivation, student emotional experiences, and belief development.\nEmployed a variety of statistical methods to answer research questions, including multilevel modeling, structural equation modeling, psychometric analyses, meta-analysis, and more.\n\n\n\nEnglish Teacher\nChesterfield County Public Schools\nChesterfield, VA\n2012 - 2016\n\nTaught 8th grade English to a diverse student population\nDifferentiated lessons to meet individual student needs"
  },
  {
    "objectID": "resume.html#selected-blog-posts",
    "href": "resume.html#selected-blog-posts",
    "title": "Eric Ekholm’s Resume",
    "section": "Selected Blog Posts",
    "text": "Selected Blog Posts\n\nCombining pmap() and do.call() for flexible analysis workflows\nN/A\nN/A\n2022\n\n\nFitting a multiple regression with torch\nN/A\nN/A\n2021\n\n\nGrouping Dungeons and Dragons monsters using latent profile analysis\nN/A\nN/A\n2021\n\n\nPredicting the speaker of every line of dialogue from The Office\nN/A\nN/A\n2020"
  },
  {
    "objectID": "resume.html#selected-academic-publications",
    "href": "resume.html#selected-academic-publications",
    "title": "Eric Ekholm’s Resume",
    "section": "Selected Academic Publications",
    "text": "Selected Academic Publications\n\nTrajectories of students’ writing feedback attitudes\nN/A\nN/A\n2022\n\nZumbrunn, S., Ekholm, E., Broda, M., & Koenka, A.C. Journal of Experimental Education\n\n\n\nRelative Contribution of Verbal Working Memory and Attention to Child Language\nN/A\nN/A\n2021\n\nChow, J.C., Ekholm, E., & Bae, C.L. Assessment for Effective Intervention\n\n\n\nDo published studies yield larger effect sizes than unpublished studies in education and special education? A meta-review.\nN/A\nN/A\n2018\n\nChow, J.C., & Ekholm, E. Educational Psychology Review\n\n\n\nClarifying an elusive construct: A systematic review of writing attitudes\nN/A\nN/A\n2018\n\nEkholm, E., Zumbrunn, S., & DeBusk-Lane, M. Educational Psychology Review"
  },
  {
    "objectID": "resume.html#humor-and-other-publications",
    "href": "resume.html#humor-and-other-publications",
    "title": "Eric Ekholm’s Resume",
    "section": "Humor and Other Publications",
    "text": "Humor and Other Publications\n\nIf You Take My Survey, I’ll Let You Watch Me Cut My Finger Off\nN/A\nN/A\n2016\n\nMcSweeneys Internet Tendency\n\n\n\nCall for Submissions for APA’s Annual Conference, Sponsored by Shark Week\nN/A\nN/A\n2014\n\nMcSweeneys Internet Tendency"
  },
  {
    "objectID": "resume.html#selected-talks-and-presentations",
    "href": "resume.html#selected-talks-and-presentations",
    "title": "Eric Ekholm’s Resume",
    "section": "Selected Talks and Presentations",
    "text": "Selected Talks and Presentations\n\nWho we are and what we do: A conversation of the intersecting identities of early career educational psychologists and their engagement with community partners\nN/A\nN/A\n2022\n\nPanelist at the American Psychological Association (APA) Annual Meeting.\n\n\n\nData Viz Tips & Tricks\nN/A\nN/A\n2022\n\nEkholm, E. Invited talk presented to the Educational Research & Evaluation Network (EREN).\n\n\n\nUsing machine learning to support student attendance practices\nN/A\nN/A\n2021\n\nEkholme, E., & Fox, P. Session presented at the annual Metropolitan Education Research Consortium (MERC) conference, Richmond, VA.\n\n\n\nInvestigating relations between writers’ emotional experiences and attention regulation: A daily diary study\nN/A\nN/A\n2020\n\nEkholm, E., & Zumbrunn, S. Paper presented at the American Educational Research Association (AERA) Annual Meeting."
  },
  {
    "objectID": "posts/cor-generate-data/index.html",
    "href": "posts/cor-generate-data/index.html",
    "title": "Generating Data with a Given Correlation",
    "section": "",
    "text": "First we load our packages\n\nusing Statistics\nusing Distributions\nusing CairoMakie #for plotting\nusing Random #to set a seed\n\nRandom.seed!(0408)\n\nTaskLocalRNG()\n\n\nThe approach here is going to be to define a covariance (correlation) matrix and a vector of means, then define a multivariate normal distribution parameterized by these things. We’ll then use this distribution to generate our data.\nFirst we’ll define \\(\\Sigma\\), which is our covariance matrix. Since we’re generating a dataset with only 2 variables, this will be a 2x2 matrix, where the diagonals will be 1 and the off-diagonals will be .8, which is the correlation we want between X and Y.\n\n#define our covariance matrix\nΣ = [[1.0, .8] [.8, 1.0]]\n\n2×2 Matrix{Float64}:\n 1.0  0.8\n 0.8  1.0\n\n\nThen we’ll define a mean vector. This will be a 2-element vector (one for each variable), but we don’t actually care what the values are here, so let’s just make them 0.\n\n#define a mean vector\n#we don't actually care what these values are, though\nμ = zeros(2)\n\n2-element Vector{Float64}:\n 0.0\n 0.0\n\n\nNow we can define a distribution given \\(\\Sigma\\) and \\(\\mu\\)\n\nd = Distributions.MvNormal(μ, Σ)\n\nFullNormal(\ndim: 2\nμ: [0.0, 0.0]\nΣ: [1.0 0.8; 0.8 1.0]\n)\n\n\nAnd then we can draw a sample from this distribution\n\ns = rand(d, 200)\n\n2×200 Matrix{Float64}:\n -1.40556   0.469524  -1.19092  -0.40408   …  -0.244792  0.874835  -0.719764\n -0.595655  1.01141   -1.84189  -0.550097      0.250661  1.72269   -0.862095\n\n\nTo confirm this works like expected, we can plot the sample\n\nCairoMakie.scatter(s)\n\n\n\n\nIt looks like a .8 correlation to me. But to do a final check, we can get the correlation matrix of our sample.\n\n#we need to transpose the matrix from 2x200 to 200x2, hence s' instead of s\ncor(s')\n\n2×2 Matrix{Float64}:\n 1.0       0.769654\n 0.769654  1.0\n\n\nClose enough. Our correlation won’t be exactly equal to .8 using this approach since we’re sampling from a distribution, but there’s really no difference (imo) between a .77 correlation and a .80 correlation.\n\n\n\nReusehttps://creativecommons.org/licenses/by-nc/4.0/CitationBibTeX citation:@online{ekholm2022,\n  author = {Eric Ekholm and EE},\n  title = {Generating {Data} with a {Given} {Correlation}},\n  date = {2022/09/08},\n  url = {https://www.ericekholm.com/posts/cor-generate-data},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEric Ekholm, and EE. 2022–9AD. “Generating Data with a Given\nCorrelation.” 2022–9AD. https://www.ericekholm.com/posts/cor-generate-data."
  },
  {
    "objectID": "posts/mle-logreg-julia/index.html",
    "href": "posts/mle-logreg-julia/index.html",
    "title": "MLE Learning Out Loud 2: Logistic Regression",
    "section": "",
    "text": "As a reminder, the point of these “learning out loud” posts is to give myself a medium to work through concepts. Hopefully these metacognitive exercises will benefits others, too. The concepts I’m covering here are things that I’m either learning anew or brushing back up on after not using for a while. But either way, I’m not trying to portray myself as an expert. If you are an expert and you notice I’m doing something wrong, I’d love to hear from you!\n\nStating the Problem\nSo, what I want to do here is get point estimates for the coefficients in a logistic regression model “by hand” (or mostly by hand). I’m going to be doing this in Julia, because I’m also interested in getting better at Julia stuff, but obviously the concepts are the same across any programming language.\n\n\nSetup\nFirst, we’ll load the libraries we’re using here and set a seed:\n\nusing GLM #to check my work against\nusing Distributions #for the Bernoulli distribution\nusing Random #to set a seed\nusing Optim #to do the acutal optimizing\nusing Statistics #mean and std\nusing RDatasets #to get data\n\nRandom.seed!(0408)\n\nTaskLocalRNG()\n\n\n\n\nLoad and Preprocess Data\nNext, we’ll load in some data and do some light preprocessing. We’ll use the Default data from the RDatasets package, which presents features describing a given person as well as a binary indicator of whether they defaulted on a credit card payment.\nAfter loading the data, we’ll pull out the default variable, dummy code it, and then assign it to a vector called y. We’ll also select just the “balance” and “income” columns of the data and assign those to X. There are other columns we could use as predictors, but that’s not really the point here.\n\ndata = RDatasets.dataset(\"ISLR\", \"Default\")\n\ny = [r.Default == \"Yes\" ? 1 : 0 for r in eachrow(data)]\n\nX = data[:, [:Balance, :Income]]\n\n\n10,000 rows × 2 columnsBalanceIncomeFloat64Float641729.52644361.62817.1812106.131073.5531767.14529.25135704.55785.65638463.56919.5897491.567825.51324905.28808.66817600.591161.0637468.5100.029275.3110.021871.1121220.5813268.613237.04528251.714606.74244994.6151112.9723810.216286.23345042.4170.050265.318527.5417636.519485.93761566.1201095.0726464.621228.95350500.222954.26232457.5231055.9651317.924641.98430466.125773.21234353.326855.00925211.327643.041473.5281454.8632189.129615.70439376.4301119.5716556.1⋮⋮⋮\n\n\nNext, we’ll z_score the predictor variables, convert them to a matrix, and append a column vector of ones to the matrix (so we can estimate the intercept). The mapcols() function from DataFrames.jl will apply the z_score function to all of the columns in X, which is actually only 2 in this case.\nFirst we’ll define a z-score function\n\nfunction z_score(x)\n    u = mean(x)\n    s = std(x)\n\n    res = Float64[]\n    for i in 1:lastindex(x)\n        tmp = (x[i] - u) / s\n        push!(res, tmp)\n    end\n\n    return res\nend\n\nz_score (generic function with 1 method)\n\n\nAnd then we’ll actually apply it.\n\nXz = hcat(ones(length(y)), Matrix(mapcols(z_score, X)))\n\n10000×3 Matrix{Float64}:\n 1.0  -0.218824    0.813147\n 1.0  -0.037614   -1.60542\n 1.0   0.492386   -0.131206\n 1.0  -0.632861    0.164023\n 1.0  -0.102786    0.370897\n 1.0   0.174098   -1.95142\n 1.0  -0.0203871  -0.645722\n 1.0  -0.0552131  -1.19344\n 1.0   0.673295    0.296293\n 1.0  -1.727      -0.31805\n 1.0  -1.727      -0.873227\n 1.0   0.796355   -1.51825\n 1.0  -1.23695    -0.394799\n ⋮                \n 1.0  -1.727       0.616625\n 1.0   0.338849   -1.01252\n 1.0  -0.957166   -0.610505\n 1.0  -0.36504     1.59599\n 1.0   0.571147    0.897805\n 1.0   0.213889    1.73331\n 1.0  -1.37056    -1.39173\n 1.0  -0.255977    1.46029\n 1.0  -0.160036   -1.03896\n 1.0   0.02075     1.88347\n 1.0   1.51667     0.236351\n 1.0  -1.31163    -1.24874\n\n\n\n\nDefine a Logistic Function\nNext, we’ll write a logistic function that will implement the logistic transformation. This is built into the StatsFuns.jl package, but I want to write it out by hand to reinforce what it is. We’ll use this to predict y values with a given input (which will actually be X*\\(\\beta\\))\n\nmy_logistic(x) = exp(x) / (1 + exp(x))\n\nmy_logistic (generic function with 1 method)\n\n\n\n\nDefine a Maximum Likelihood Estimator\nNow that we have some data, we can write a function that uses maximum likelihood estimation to give us the best \\(\\beta\\) parameters for our given X and y. If you want to brush up on maximum likelihood, you can read my previous “learning out loud” post (INSERT LINK), or you can probably find materials written by someone who knows way more than I do. Either way, I’m not going to recap what MLE is here.\nLet’s define our function that we’ll use to estimate \\(\\beta\\). The important thing to keep in mind is that the return value of this function isn’t the \\(\\beta\\) values, but rather the negative log likelihood, since this is what we we want to optimize.\n\nfunction ml_logreg(x, y, b)\n\n    ŷ = my_logistic.(x*b)\n    res = Float64[]\n\n    for i in 1:lastindex(y)\n        push!(res, logpdf(Bernoulli(ŷ[i]), y[i]))\n    end\n\n    ret = -sum(res)\n\n    return ret\nend\n\nml_logreg (generic function with 1 method)\n\n\nSo what’s going on in this code?\n\nWe’re getting \\(ŷ\\) estimates for a given x and b by running them through the my_logistic() function. This will give us a 10000x1 vector\nWe’re instantiating an empty vector that will (eventually) contain Float64 values.\nFor each index in \\(ŷ\\) (i.e. 1 through 10000), we’re getting the log-likelihood of the true outcome (y[i]) given a Bernoulli distribution parameterized by success rate \\(ŷ\\)[i].\n\nI think this is the trickiest part of the whole problem, so I want to put it into words to make sure I understand it. In our problem, our y values are either 0 or 1. And the output of the my_logistic() function is going to be, for each y, a predicted probability that \\(y = 1\\), i.e. a predicted success rate. Since a Bernoulli distribution is parameterized by a given success rate and models the outcome of a single yes/no (1/0) trial, it makes sense to use this to generate the likelihoods we want to maximize.\nMore concretely, the likelihoods we get will be dependent on:\n\nthe provided success rate p, and\nthe actual outcome\n\nWhere values of p that are closer to the actual outcome will be larger:\n\nlogpdf(Bernoulli(.5), 1)\n\n-0.6931471805599453\n\n\n\n#will be larger than the previous\nlogpdf(Bernoulli(.8), 1)\n\n-0.2231435513142097\n\n\n\n#will be even larger\nlogpdf(Bernoulli(.99), 1)\n\n-0.01005033585350145\n\n\nAnd inversely, you can imagine that if the outcome were 0, we’d want our predicted success rate to be very low.\nReturning to our ml_logreg() function, what we’re doing then is applying this logic to all of our \\(ŷ\\) and corresponding y values (i.e. we’re getting the likelihood of y for a given \\(ŷ\\)), and then we’re creating a vector with all of these likelihoods – that’s what the push!(...) notation is doing – pushing these likelihoods to the empty float vector we created.\nFinally, we’re summing all of our likelihoods and then multiplying the result by negative one, since the optimizer we’re using actually wants to minimize a loss function rather than maximize a loss function.\nWe can run this function by providing any X, y, and \\(\\beta\\), and it’ll give us back a negative loglikelihood – the negative sum of all of the individual likelihoods.\n\n#just some arbitrary numbers to test the function with\nstart_vals = [.1, .1, .1]\n\nml_logreg(Xz, y, start_vals)\n\n7372.506385031871\n\n\n\n\nOptimize \\(\\beta\\)\nSo the above gives us the likelihood for a starting value of \\(\\beta\\), but we want to find the best values of \\(\\beta\\). To do that, we can optimize the function. Like I said in my previous post, the optimizers are written by people much smarter than I am, so I’m just going to use that package rather than futz around with doing any, like, calculus by hand – although maybe that’s a topic for a later learning out loud post.\n\nres = optimize(b -> ml_logreg(Xz, y, b), start_vals)\n\n * Status: success\n\n * Candidate solution\n    Final objective value:     7.894831e+02\n\n * Found with\n    Algorithm:     Nelder-Mead\n\n * Convergence measures\n    √(Σ(yᵢ-ȳ)²)/n ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   0  (vs limit Inf)\n    Iterations:    103\n    f(x) calls:    190\n\n\nAnd then we can get the \\(\\beta\\) coefficients that minimize the loss function (i.e. that maximize the likelihood)\n\nOptim.minimizer(res)\n\n3-element Vector{Float64}:\n -6.125561839584853\n  2.731586594783831\n  0.2775242967112382\n\n\nAnd just to confirm that we did this correctly, we can check our point estimates against what we’d get if we fit the model using the GLM package.\n\nlogreg_res = glm(Xz, y, Binomial())\n\nGeneralizedLinearModel{GLM.GlmResp{Vector{Float64}, Binomial{Float64}, LogitLink}, GLM.DensePredChol{Float64, LinearAlgebra.Cholesky{Float64, Matrix{Float64}}}}:\n\nCoefficients:\n─────────────────────────────────────────────────────────────────\n        Coef.  Std. Error       z  Pr(>|z|)  Lower 95%  Upper 95%\n─────────────────────────────────────────────────────────────────\nx1  -6.12557    0.187562   -32.66    <1e-99  -6.49318   -5.75795\nx2   2.73159    0.109984    24.84    <1e-99   2.51602    2.94715\nx3   0.277522   0.0664854    4.17    <1e-04   0.147213   0.407831\n─────────────────────────────────────────────────────────────────\n\n\nCool beans!\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-nc/4.0/CitationBibTeX citation:@online{ekholm2022,\n  author = {Eric Ekholm},\n  title = {MLE {Learning} {Out} {Loud} 2: {Logistic} {Regression}},\n  date = {2022-09-28},\n  url = {https://www.ericekholm.com/posts/mle-logreg-julia},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEric Ekholm. 2022. “MLE Learning Out Loud 2: Logistic\nRegression.” September 28, 2022. https://www.ericekholm.com/posts/mle-logreg-julia."
  }
]